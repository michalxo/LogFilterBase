Targets
    String 'LOG\.(trace|debug|info|warn|error|fatal)'
Found usages  (6,143 usages)
    Production  (3,859 usages)
        Unclassified usage  (3,859 usages)

hadoop-archives  (3 usages)
    org.apache.hadoop.tools  (3 usages)
        HadoopArchives.java  (3 usages)
            (566: 7) LOG.info("Unable to clean tmp directory " + jobDirectory);
            LOG.UNABLE_TO_DELETE(jobDirectory).tag("org.apache.hadoop.Archives").info();

            (898: 7) LOG.debug("Exception in archives  ", e);
            LOG.EXCEPTION(e).tag("org.apache.hadoop.Archives").debug();

hadoop-auth  (18 usages)
    org.apache.hadoop.security.authentication.client  (5 usages)
        KerberosAuthenticator.java  (5 usages)
            (162: 9) LOG.debug("JDK performed authentication on our behalf.");
            LOG.AUTHENTICATION("PERFORMED_BY_JDK").tag("org.apache.hadoop.KerberosClient").debug();

            (170: 9) LOG.debug("Performing our own SPNEGO sequence.");
            LOG.EXECUTED_SPNEGO_SEQUENCE().tag("org.apache.hadoop.KerberosClient").tag("AUTHENTICATION").debug();

            (173: 9) LOG.debug("Using fallback authenticator sequence.");
            LOG.USING_FALLBACK_SEQUENCE.tag("org.apache.hadoop.KerberosClient").tag("AUTHENTICATION").debug();

            (220: 9) LOG.debug("No subject in context, logging in");
            LOG.LOGGING_IN("NO_SUBJECT").tag("org.apache.hadoop.KerberosClient").debug();

            (228: 9) LOG.debug("Using subject: " + subject);
            LOG.LOGGING_IN(subject).tag("org.apache.hadoop.KerberosClient").debug();

    org.apache.hadoop.security.authentication.server  (12 usages)
        AuthenticationFilter.java  (5 usages)
            (158: 7) LOG.warn("'signature.secret' configuration not set, using a random value as secret");
            LOG.FIELD_NOT_SET("signature.secret").tag("org.apache.hadoop.KerberosServer").tag("AUTHENTICATION_CONFIG").warn();

            (341: 9) LOG.warn("AuthenticationToken ignored: " + ex.getMessage());
            LOG.TOKEN_IGNORED(ex.getMessage()).tag("org.apache.hadoop.KerberosServer").warn();

           (347: 13) LOG.debug("Request [{}] triggering authentication ", getRequestURL(httpRequest));
            LOG.REQUEST_TRIGGERED_AUTHENTICATION(getRequestURL(httpRequest)).tag("org.apache.hadoop.KerberosServer").debug();

            (359: 13) LOG.debug("Request [{}] user [{}] authenticated", getRequestURL(httpRequest), token.getUserName());
            LOG.AUTHENTICATED_USER(token.getUserName(), "request", getRequestURL(httpRequest)).tag("org.apache.hadoop.KerberosServer").debug();

            (391: 7) LOG.warn("Authentication exception: " + ex.getMessage(), ex);
            LOG.AUTHENTICATION_ERROR(ex.getMessage()).tag("org.apache.hadoop.KerberosServer").warn();

        KerberosAuthenticationHandler.java  (7 usages)
            (167: 7) LOG.info("Login using keytab "+keytab+", for principal "+principal);
            LOG.LOGIN_USING("KEYTAB", keytab, "PRINCIPAL", principal).tag("org.apache.hadoop.KerberosServer").info();

            (183: 7) LOG.info("Initialized, principal [{}] from keytab [{}]", principal, keytab);
            LOG.INITIALIZED_PRINCIPAL(principal, "FROM_KEYTAB", keytab).tag("org.apache.hadoop.KerberosServer").info();

            (202: 7) LOG.warn(ex.getMessage(), ex);
            LOG.LOGIN_ERROR(ex.getMessage()).tag("org.apache.hadoop.KerberosServer").warn();

            (280: 9) LOG.trace("SPNEGO starting");
            LOG.STARTING_SPNEGO().tag("org.apache.hadoop.KerberosServer").trace();

            (x:y) LOG.warn("'" + KerberosAuthenticator.AUTHORIZATION + "' does not start with '" + KerberosAuthenticator.NEGOTIATE + "' :  {}", authorization);
            LOG.WRONG_FORMAT(authorization).tag("org.apache.hadoop.KerberosServer").tag("AUTHENTICATION").warn();

            (307: 17) LOG.trace("SPNEGO in progress");
            LOG.SPNEGO_IN_PROGRESS().tag("org.apache.hadoop.KerberosServer").tag("AUTHENTICATION").trace();

            (314: 17) LOG.trace("SPNEGO completed for principal [{}]", clientPrincipal);
            LOG.SPNEGO_COMPLETED_FOR_PRINCIPAL(clientPrincipal).tag("org.apache.hadoop.KerberosServer").trace();

    org.apache.hadoop.security.authentication.util  (1 usage)
        KerberosName.java  (1 usage)
            (87: 9) LOG.debug("Kerberos krb5 configuration not found, setting default realm to empty");
            LOG.CONFIGURATION_NOT_FOUND("krb5", "EMPTY_REALM").tag("org.apache.hadoop.KerberosServer").tag("AUTHENTICATION").debug();

hadoop-auth-examples  (2 usages)
    org.apache.hadoop.security.authentication.examples  (2 usages)
        RequestLoggerFilter.java  (2 usages)
            (57: 9) LOG.debug(xRequest.getResquestInfo().toString());
            LOG.HTTP_REQUEST_FILTER_EXAMPLE(xRequest.getResquestInfo().toString()).tag("org.apache.hadoop.KerberosServer").tag("AUTHENTICATION").debug();

            (61: 9) LOG.debug(xResponse.getResponseInfo().toString());
            LOG.HTTP_RESPONSE_FILTER_EXAMPLE(xRequest.getResponseInfo().toString()).tag("org.apache.hadoop.KerberosServer").tag("AUTHENTICATION").debug();


hadoop-common  (672 usages)
    /home/michalxo/IdeaProjects/hadoop-common/hadoop-common-project/hadoop-common  (1 usage)
        CHANGES.txt  (1 usage)
            (2827: 58) HADOOP-6884. Add LOG.isDebugEnabled() guard for each LOG.debug(..).
    /home/michalxo/IdeaProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/aop/org/apache/hadoop/fi  (3 usages)
        ProbabilityModel.java  (3 usages)

            (61: 5) LOG.info(ALL_PROBABILITIES + "=" + conf.get(ALL_PROBABILITIES));
            LOG.ALL_PROBABILITIES(conf.get(ALL_PROBABILITIES)).tag("org.apache.hadoop.Common").info();

            (98: 7) LOG.debug("Request for " + newProbName + " returns=" + ret);
            LOG.REQUESTED_PROBABILITY_RETURNED(ret).tag("org.apache.hadoop.Common").debug();

            (102: 7) LOG.info("Probability level is incorrect. Default value is set");
            LOG.INCORRECT_PROBABILITY_LEVEL("Using default value").tag("org.apache.hadoop.Common").info();

    org.apache.hadoop.conf  (29 usages)
        Configuration.java  (23 usages)
            (463: 5) LOG.debug("Handling deprecation for all properties in config...");
            LOG.HANDLING_DEPRECATIONS().tag("org.apache.haddop.conf").tag("CONFIGURATION").debug();

            (467: 7) LOG.debug("Handling deprecation for " + (String)item);
            LOG.HANDLING_DEPRECATION_FOR((String)item).tag("org.apache.hadoop.conf").tag("CONFIGURATION").debug();

            (479: 7) LOG.warn("DEPRECATED: hadoop-site.xml found in the classpath. "
                    "Usage of hadoop-site.xml is deprecated. Instead use core-site.xml, "
                    + "mapred-site.xml and hdfs-site.xml to override properties of " +
                    "core-default.xml, mapred-default.xml and hdfs-default.xml " +
                    "respectively");

?                        LOG.DEPRECATED_FILE("hadoop-site.xml", "Usage of hadoop-site.xml is deprecated. Instead use core-site.xml, .warn();
                    + "mapred-site.xml and hdfs-site.xml to override properties of " +
                    "core-default.xml, mapred-default.xml and hdfs-default.xml " +
                    "respectively").tag("org.apache.hadoop.conf").tag("DEPRECATION");


            (688: 9) LOG.warn("Unexpected SecurityException in Configuration", se);
            LOG.UNEXPECTED_SECURITY_EXCEPTION().tag("org.apache.hadoop.conf").tag("CONFIGURATION").warn();


            (823: 7) LOG.warn(keyInfo.getWarningMessage(name));
            LOG.DEPRECATED_KEY(keyInfo.getWarningMessage(name)).tag("org.apache.hadoop.conf").tag("DEPRECATION").warn();

            (1150: 7) LOG.warn("Regular expression '" + valString + "' for property '" + name + "' not valid. Using default", pse);
            LOG.NOT_VALID_REGULAR_EXPRESSION(valString, "property", name, "using default", pse).tag("org.apache.hadoop.conf").warn();

            (1719: 5) LOG.warn("Could not make " + path + " in local directories from " + dirsProp);
            LOG.UNABLE_TO_CREATE_FILE(path, "in directory", dirsProp).tag("org.apache.hadoop.conf").warn();

            (1723: 7) LOG.warn(dirsProp + "[" + index + "]=" + dirs[index]);
?                        LOG.DIRECTORY_ON_INDEX(dirsProp + "[" + index + "]=" + dirs[index]).tag("org.apache.hadoop.conf").warn();

            (1775: 9) LOG.info(name + " not found");
            LOG.RESOURCE_NOT_FOUND(name).tag("org.apache.hadoop.conf").info();

            (1778: 9) LOG.info("found resource " + name + " at " + url);
            LOG.RESOURCE_FOUND(name, "location", url).tag("org.apache.hadoop.conf").tag("INPUTSTREAM").info();

            (1799: 9) LOG.info(name + " not found");
            LOG.RESOURCE_NOT_FOUND(name).tag("org.apache.hadoop.conf").info();

            (1802: 9) LOG.info("found resource " + name + " at " + url);
            LOG.RESOURCE_FOUND(name, "location", url).tag("org.apache.hadoop.conf").tag("READER").info();

            (1870: 7) LOG.info("parsing URL " + url);
            LOG.PARSING_URL(url).tag("org.apache.hadoop.conf").info();

            (1881: 7) LOG.info("parsing input stream " + is);
            LOG.PARSING_INPUT_STREAM(is).tag("org.apache.hadoop.conf").info();

            (1932: 9) LOG.error("Failed to set setXIncludeAware(true) for parser " + docBuilderFactory + ":" + e,e);
            LOG.FAILED_TO_SET_DOCBUILDERFACTORY("setXIncludeAware(true)", docBuilderFactory, e).tag("org.apache.hadoop.conf").error();

            (1954: 13) LOG.info("parsing File " + file);
            LOG.PARSING_FILE(file).tag("org.apache.hadoop.conf").info();

            (1982: 9) LOG.fatal("bad conf file: top-level element not <configuration>");
            LOG.WRONG_FILE_FORMAT("top-level element not <configuration>").tag("org.apache.hadoop.conf").fatal();

            (1994: 11) LOG.warn("bad conf file: element not <property>");
            LOG.WRONG_FILE_FORMAT("element not <property>").tag("org.apache.hadoop.conf").warn();

            (2043: 7) LOG.fatal("error parsing conf " + name, e);
            (2046: 7) LOG.fatal("error parsing conf " + name, e);
            (2049: 7) LOG.fatal("error parsing conf " + name, e);
            (2052: 7) LOG.fatal("error parsing conf " + name, e);
            LOG.ERROR_PARSING_CONF(name, e).tag("org.apache.hadoop.conf").fatal();

            (2070: 9) LOG.warn(name+":an attempt to override final parameter: "+attr+";  Ignoring.");
            LOG.IGNORING_PARAMETER(name, attr).tag("org.apache.hadoop.conf").warn();

        ReconfigurableBase.java  (1 usage)
            (65: 7) LOG.info("changing property " + property + " to " + newVal);
            LOG.CHANGING_PROPERTY(property, newVal).tag("org.apache.hadoop.conf").info();

        ReconfigurationServlet.java  (5 usages)
            (64: 5) LOG.info("servlet path: " + req.getServletPath());
            LOG.SERVLET_PATH(req.getServletPath()).tag("org.apache.hadoop.conf").info();

            (65: 5) LOG.info("getting attribute: " + CONF_SERVLET_RECONFIGURABLE_PREFIX + req.getServletPath());
            LOG.GETTING_ATTRIBUTE(CONF_SERVLET_RECONFIGURABLE_PREFIX + req.getServletPath()).tag("org.apache.hadoop.conf").info();

            (185: 15) LOG.info("property " + param + " unchanged");
            LOG.PROPERTY_PARAMETER_UNCHANGED(param).tag("org.apache.hadoop.conf").info();

            (203: 5) LOG.info("GET");
            LOG.GET().tag("org.apache.hadoop.conf").info();

            (217: 5) LOG.info("POST");
            LOG.POST().tag("org.apache.hadoop.conf").info();

    org.apache.hadoop.fs  (40 usages)
        ChecksumFileSystem.java  (1 usage)
            (158: 9) LOG.warn("Problem opening checksum file: "+ file + ".  Ignoring exception: " , e);
            LOG.ERROR_OPENING.CHECKSUM_FILE(file, "ignoring", e).tag("org.apache.hadoop.fs").warn();

        ChecksumFs.java  (1 usage)
            (145: 9) LOG.warn("Problem opening checksum file: "+ file +
            LOG.ERROR_OPENING.CHECKSUM_FILE(file, "ignoring", e).tag("org.apache.hadoop.fs").warn();

        DelegationTokenRenewer.java  (3 usages)
            (196: 11) LOG.error("Interrupted while canceling token for " + fs.getUri() + "filesystem");
            LOG.INTERRUPTED_WHILE_CANCELING_TOKEN(fs.getUri()).tag("org.apache.hadoop.fs").error();

            (199: 13) LOG.debug(ie.getStackTrace());
            LOG.STACKTRACE(ie.getStackTrace()).tag("org.apache.hadoop.fs").debug();

            (224: 29) action.weakFs.get().LOG.warn("Failed to renew token, action=" + action, ie);
            LOG.FAILED_TO_RENEW_TOKEN(action, ie).tag("org.apache.hadoop.fs").warn();

        DU.java  (1 usage)
            (96: 13) LOG.warn("Could not get disk usage information", e);
            LOG.ERROR_GETTING_DISK_USAGE_INFORMATION(e).tag("org.apache.hadoop.fs").warn();

        FileContext.java  (4 usages)
            (237: 7) LOG.error("Exception in getCurrentUser: ",e);
            LOG.ERROR_GETTING_CURRENT_USER(e).tag("org.apache.hadoop.fs").error();

            (288: 13) LOG.warn("Ignoring failure to deleteOnExit for path " + path);
            LOG.IGNORING_FAILURE_DELETEONEXIT(path).tag("org.apache.hadoop.fs").warn();

            (346: 7) LOG.error(ex);
            LOG.EXCEPTION(ex).tag("org.apache.hadoop.fs").tag("AbstractFileSystem").error();

            (457: 7) LOG.error(ex);
            LOG.EXCEPTION(ex).tag("org.apache.hadoop.fs").tag("FileContext").error();

        FileSystem.java  (4 usages)
            (270: 7) LOG.warn("\"local\" is a deprecated filesystem name." +" Use \"file:///\" instead.");
            LOG.DEPRECATED_FILESYSTEM_NAME("local", "use file:///").tag("org.apache.hadoop.fs").warn();

            (274: 7) LOG.warn("\""+name+"\" is a deprecated filesystem name."+" Use \"hdfs://"+name+"/\" instead.");
            LOG.DEPRECATED_FILESYSTEM_NAME("use hdfs://", name, "/").tag("org.apache.hadoop.fs").warn();

            (1333: 11) LOG.info("Ignoring failure to deleteOnExit for path " + path);
            LOG.IGNORING_FAILURE_DELETEONEXIT(path).tag("org.apache.hadoop.fs").info();

            (2385: 11) LOG.info("FileSystem.Cache.closeAll() threw an exception:\n" + e);
            LOG.ERROR_FILESYSTEM_CACHE_CLOSEALL(e).tag("org.apache.hadoop.fs").info();

        FileUtil.java  (3 usages)
            (140: 7) LOG.warn("null file argument.");
            LOG.NULL_FILE_ARGUMENT().tag("org.apache.hadoop.fs").warn();

            (149: 7) LOG.warn("Failed to delete file or dir ["+ f.getAbsolutePath() + "]: it still exists.");
            LOG.FAILED_TO_DELETE(f.getAbsolutePath(), "still exists").tag("org.apache.hadoop.fs").warn();

            (801: 9) LOG.debug("Error while changing permission : " + filename + " Exception: ", e);
            LOG.ERROR_CHANGING_PERMISSION(filename, e).tag("org.apache.hadoop.fs").debug();

        FSInputChecker.java  (1 usage)
            (284: 11) LOG.info("Found checksum error: b[" + off + ", " + (off+read) + "]=" + StringUtils.byteToHexString(b, off, off + read), ce);
            LOG.CHECKSUM_ERROR(ce).tag("org.apache.hadoop.fs").info();

        FsShell.java  (1 usage)
            (263: 9) LOG.debug("Error", e);
            LOG.ERROR(e).tag("org.apache.hadoop.fs").tag("FsShell.run()").debug();

        FsShellPermissions.java  (2 usages)
            (105: 11) LOG.debug("Error changing permissions of " + item, e);
            LOG.ERROR_CHANGING_PERMISSION(item, e).tag("org.apache.hadoop.fs").debug();

            (185: 11) LOG.debug("Error changing ownership of " + item, e);
            LOG.ERROR_CHANGING_OWNERSHIP(item, e).tag("org.apache.hadoop.fs").debug();

        LocalDirAllocator.java  (4 usages)
            (293: 17) LOG.warn( localDirs[i] + " is not writable\n", de);
            LOG.DIRECTORY_NOT_WRITABLE(localDirs[i], de).tag("org.apache.hadoop.fs").warn();

            (296: 15) LOG.warn( "Failed to create " + localDirs[i]);
            LOG.FAILED_TO_CREATE_DIRECTORY(localDirs[i]).tag("org.apache.hadoop.fs").warn();

            (299: 13) LOG.warn( "Failed to create " + localDirs[i] + ": " + ie.getMessage() + "\n", ie);
            LOG.FAILED_TO_CREATE_DIRECTORY(localDirs[i], ie.getMessage()).tag("org.apache.hadoop.fs").warn();

            (323: 11) LOG.warn("Disk Error Exception: ", d);
            LOG.DISK_ERROR_EXCEPTION(d).tag("org.apache.hadoop.fs").warn();

        LocalFileSystem.java  (4 usages)
            (125: 7) LOG.warn("Moving bad file " + f + " to " + badFile);
            LOG.MOVING_BAD_FILE(f, "to", badFile).tag("org.apache.hadoop.fs").warn();

            (129: 9) LOG.warn("Ignoring failure of renameTo");
            LOG.IGNORING_FAILURE_RENAMETO().tag("org.apache.hadoop.fs").("reportChecksumFailure").warn();

            (135: 11) LOG.warn("Ignoring failure of renameTo");
            LOG.IGNORING_FAILURE_RENAMETO().tag("org.apache.hadoop.fs").("reportChecksumFailure").warn();

            (138: 7) LOG.warn("Error moving bad file " + p + ": " + e);
            LOG.ERROR_MOVING_BAD_FILE(p, e).tag("org.apache.hadoop.fs").warn();

        TrashPolicyDefault.java  (11 usages)
            (134: 11) LOG.warn("Can't create(mkdir) trash directory: "+baseTrashPath);
            LOG.ERROR_CREATING_TRASH_DIRECTORY("mkdir", baseTrashPath).tag("org.apache.hadoop.fs").warn();

            (138: 9) LOG.warn("Can't create trash directory: "+baseTrashPath);
            LOG.ERROR_CREATING_TRASH_DIRECTORY(baseTrashPath).tag("org.apache.hadoop.fs").warn();

            (186: 5) LOG.info("Created trash checkpoint: "+checkpoint.toUri().getPath());
            LOG.CREATED_TRASH_CHECKPOINT(checkpoint.toUri().getPath()).tag("org.apache.hadoop.fs").info();

            (211: 9) LOG.warn("Unexpected item in trash: "+dir+". Ignoring.");
?                        LOG.UNEXPECTED_ITEM_IN_TRASH_DIRECTORY_IGNORING(dir).tag("org.apache.hadoop.fs").warn();
ignoring
            (217: 11) LOG.info("Deleted trash checkpoint: "+dir);
            LOG.DELETED_TRASH_CHECKPOINT(dir).tag("org.apache.hadoop.fs").info();

            (219: 11) LOG.warn("Couldn't delete checkpoint: "+dir+" Ignoring.");
?                        LOG.UNABLE_TO_DELETE_CHECKPOINT_IGNORING(dir).tag("org.apache.hadoop.fs").warn();

            (244: 9) LOG.info("The configured checkpoint interval is " + (emptierInterval / MSECS_PER_MINUTE) + " minutes." +
     " Using an interval of " + (deletionInterval / MSECS_PER_MINUTE) + " minutes that is used for deletion instead");
            LOG.USING_DELETION_CHECKPOINT_INTERVAL_INSTEAD_CONFIGURED("configured", (emptierInterval / MSECS_PER_MINUTE).info();
                "using", (deletionInterval / MSECS_PER_MINUTE))..tag("org.apache.hadoop.fs");

            (275: 15) LOG.warn("Trash can't list homes: "+e+" Sleeping.");
            LOG.TRASH_CANT_LIST_HOME_DIRECTORIES_SLEEPING(e).tag("org.apache.hadoop.fs").warn();

            (288: 17) LOG.warn("Trash caught: "+e+". Skipping "+home.getPath()+".");
            LOG.TRASH_EXCEPTION_SKIPPING_PATH(home.getPath(), "exception", e).tag("org.apache.hadoop.fs").warn();

            (293: 11) LOG.warn("RuntimeException during Trash.Emptier.run(): ", e);
            LOG.RUNTIME_EXCEPTION_DURING_TRASH_EMPTIER_RUN(e).tag("org.apache.hadoop.fs").tag("Trash.Emptier.run()").warn();

            (299: 9) LOG.warn("Trash cannot close FileSystem: ", e);
            LOG.TRASH_CANNON_CLOSE_FILESYSTEM(e).tag("org.apache.hadoop.fs").warn();

    org.apache.hadoop.fs.ftp  (1 usage)
        FTPFileSystem.java  (1 usage)
            (154: 9) LOG.warn("Logout failed while disconnecting, error code - "+ client.getReplyCode());
            LOG.LOGOUT_FAILED_WHILE_DISCONNECTING(client.getReplyCode()).tag("org.apache.hadoop.fs.ftp").warn();

    org.apache.hadoop.fs.permission  (2 usages)
        FsPermission.java  (2 usages)
            String error = "Unable to parse configuration " + UMASK_LABEL+ " with value " + confUmask + " as " + type + " umask.";
            (245: 9) LOG.warn(error);
            LOG.ERROR_PARSING_CONFIGURATION(UMASK_LABEL, "value", confUmask, type).tag("org.apache.hadoop.fs.permission").warn();

            (255: 11) LOG.warn(DEPRECATED_UMASK_LABEL + " configuration key is deprecated. " + "Convert to "
                + UMASK_LABEL + ", using octal or symbolic umask " + "specifications.");
            LOG.DEPRECATED_UMASK_LABEL_CONFIGURATION_KEY(DEPRECATED_UMASK_LABEL, UMASK_LABEL.warn();
                "convert using octal or symbolic umask specifications").tag("org.apache.hadoop.fs.permission");

    org.apache.hadoop.fs.s3  (4 usages)
        Jets3tFileSystemStore.java  (1 usage)
            (240: 11) LOG.warn("Ignoring failed delete");
        S3InputStream.java  (1 usage)
            (190: 9) LOG.warn("Ignoring failed delete");
        S3OutputStream.java  (2 usages)
            (190: 7) LOG.warn("Ignoring failed delete");
            (227: 7) LOG.warn("Ignoring failed delete");
            LOG.FAILED_TO_DELETE_FILE_IGNORING().tag("org.apache.hadoop.fs.s3").warn();

    org.apache.hadoop.fs.s3native  (30 usages)
        NativeS3FileSystem.java  (30 usages)
            (112: 9) LOG.info("Received IOException while reading '" + key + "', attempting to reopen.");
            LOG.ERROR_IOEXCEPTION_WHILE_READING(key, "reopening").tag("org.apache.hadoop.fs.s3native").info();

            (132: 9) LOG.info("Received IOException while reading '" + key + "', attempting to reopen.");
            LOG.ERROR_IOEXCEPTION_WHILE_READING(key, "reopening").tag("org.apache.hadoop.fs.s3native").info();

            (153: 7) LOG.info("Opening key '" + key + "' for reading at position '" + pos + "'");
            LOG.OPENING_KEY_FOR_READING(key, "position", pos).tag("org.apache.hadoop.fs.s3native").info();

            (182: 7) LOG.info("OutputStream for key '" + key + "' writing to tempfile '" + this.backupFile + "'");
            LOG.WRITING_TO_TEMPFILE_OUTPUTSTREAM_KEY(key, this.backupFile).tag("org.apache.hadoop.fs.s3native").info();

            (188: 9) LOG.warn("Cannot load MD5 digest algorithm," +"skipping message integrity check.", e);
            LOG.UNABLE_TO_LOAD_MD5_DIGEST_ALGORITHM_SKIPPING(e).tag("org.apache.hadoop.fs.s3native").warn();

            (217: 7) LOG.info("OutputStream for key '" + key + "' closed. Now beginning upload");
            LOG.UPLOADING_CLOSED_OUTPUTSTREAM_FOR_KEY(key).tag("org.apache.hadoop.fs.s3native").info();

            (224: 11) LOG.warn("Could not delete temporary s3n file: " + backupFile);
            LOG.UNABLE_TO_DELETE_S3N_TEMPORARY_FILE(backupFile).tag("org.apache.hadoop.fs.s3native").warn();

            (229: 7) LOG.info("OutputStream for key '" + key + "' upload complete");
            LOG.UPLOAD_FINISHED_FOR_OUTPUTSTREAM_KEY(key).tag("org.apache.hadoop.fs.s3native").info();

            (346: 7) LOG.debug("Creating new file '" + f + "' in S3");
            LOG.CREATING_NEW_S3_FILE(f).tag("org.apache.hadoop.fs.s3native").debug();

            (361: 9) LOG.debug("Delete called for '" + f +"' but file does not exist, so returning false");
            LOG.DELETE_NOT_EXISTING_FILE_RETURNING_FALSE(f).tag("org.apache.hadoop.fs.s3native").tag("FILE_DOES_NOT_EXIST").debug();

            (376: 9) LOG.debug("Deleting directory '" + f  + "'");
            LOG.DELETING_DIRECTORY(f).tag("org.apache.hadoop.fs.s3native").debug();

            (394: 9) LOG.debug("Deleting file '" + f + "'");
            LOG.DELETING_FILE(f).tag("org.apache.hadoop.fs.s3native").debug();

            (412: 7) LOG.debug("getFileStatus retrieving metadata for key '" + key + "'");
            LOG.RETRIEVING_METADATA_FOR_KEY(key).tag("org.apache.hadoop.fs.s3native").tag("getFileStatus()").debug();

            (417: 9) LOG.debug("getFileStatus returning 'file' for key '" + key + "'");
            LOG.RETURNING_FILE_FOR_KEY(key).tag("org.apache.hadoop.fs.s3native").tag("getFileStatus()").debug();

            (423: 9) LOG.debug("getFileStatus returning 'directory' for key '" + key +"' as '" + key + FOLDER_SUFFIX + "' exists");
!                        String folder = key + FOLDER_SUFFIX;
!                        LOG.RETURNING_DIRECTORY_FOR_KEY_EXISTS(key, "returning as", key + folder ).tag("org.apache.hadoop.fs.s3native").tag("getFileStatus()").debug();

            (430: 7) LOG.debug("getFileStatus listing key '" + key + "'");
            LOG.LISTING_KEY(key).tag("org.apache.hadoop.fs.s3native").tag("getFileStatus()").debug();

            (436: 9) LOG.debug("getFileStatus returning 'directory' for key '" + key + "' as it has contents");
            LOG.RETURNING_DIRECTORY_WITH_CONTENT_FOR_KEY(key).tag("org.apache.hadoop.fs.s3native").tag("getFileStatus()").debug();

            (443: 7) LOG.debug("getFileStatus could not find key '" + key + "'");
            LOG.UNABLE_TO_FIND_KEY(key).tag("org.apache.hadoop.fs.s3native").tag("getFileStatus()").debug();

            (547: 9) LOG.debug("Making dir '" + f + "' in S3");
            LOG.CREATING_DIR_IN_S3(f).tag("org.apache.hadoop.fs.s3native").debug();

            (561: 5) LOG.info("Opening '" + f + "' for reading");
            LOG.OPENING_FILE_FOR_READING(f).tag("org.apache.hadoop.fs.s3native").info();

!                    final String debugPreamble = "Renaming '" + src + "' to '" + dst + "' - ";
            (599: 11) LOG.debug(debugPreamble + "returning false as dst is an already existing file");
            LOG.DESTINATION_ALREADY_EXISTS(src, dst).tag("org.apache.hadoop.fs.s3native"),tag("RENAMING_FILE").debug();

            (605: 11) LOG.debug(debugPreamble + "using dst as output directory");
            LOG.USING_DESTINATION_AS_OUTPUT_DIRECTORY(src, dst).tag("org.apache.hadoop.fs.s3native").tag("RENAMING_FILE").debug();

            (611: 9) LOG.debug(debugPreamble + "using dst as output destination");
            LOG.USING_DESTINATION_AS_OUTPUT_FILE(src, dst).tag("org.apache.hadoop.fs.s3native").tag("RENAMING_FILE").debug();

            (617: 13) LOG.debug(debugPreamble + "returning false as dst parent exists and is a file");
            LOG.DESTINATION_PARENT_FILE_ALREADY_EXISTS(src, dst).tag("org.apache.hadoop.fs.s3native").tag("RENAMING_FILE").debug();

            (624: 11) LOG.debug(debugPreamble + "returning false as dst parent does not exist");
            LOG.DESTINATION_PARENT_DOES_NOT_EXIST(src, dst).tag("org.apache.hadoop.fs.s3native").tag("RENAMING_FILE").debug();

            (636: 9) LOG.debug(debugPreamble + "returning false as src does not exist");
            LOG.SOURCE_FILE_DOES_NOT_EXIST(src, dst).tag("org.apache.hadoop.fs.s3native").tag("RENAMING_FILE").debug();

            (642: 9) LOG.debug(debugPreamble + "src is file, so doing copy then delete in S3");
            LOG.STORING_COPY_OF_S3_FILE(src, dst).tag("org.apache.hadoop.fs.s3native").tag("RENAMING_FILE").debug();

            (649: 9) LOG.debug(debugPreamble + "src is directory, so copying contents");
            LOG.COPYING_SOURCE_CONTENT(src, dst).tag("org.apache.hadoop.fs.s3native").tag("RENAMING_FILE").debug();

            (665: 9) LOG.debug(debugPreamble + "all files in src copied, now removing src files");
            LOG.COPY_SUCCESSFUL_REMOVING_SOURCE_FILES(src, dst).tag("org.apache.hadoop.fs.s3native").tag("RENAMING_FILE").debug();

            (678: 9) LOG.debug(debugPreamble + "done");
            LOG.RENAMING_FILE_COMPLETED(src, dst).tag("org.apache.hadoop.fs.s3native").tag("RENAMING_FILE").debug();

    org.apache.hadoop.fs.shell  (1 usage)
        Command.java  (1 usage)
            (377: 7) LOG.debug(errorMessage);
            LOG.INTERNAL_ERROR(errorMessage).tag("org.apache.hadoop.fs.shell").debug();

    org.apache.hadoop.ha  (117 usages)
        ActiveStandbyElector.java  (40 usages)
            (253: 5) LOG.debug("Attempting active election for " + this);
            LOG.ATTEMPTING_ACTIVE_ELECTION_FOR(this).tag("org.apache.hadoop.ha").debug();

            (289: 7) LOG.debug("Ensuring existence of " + prefixPath);
            LOG.ENSURING_EXISTENCE_OF(prefixPath).tag("org.apache.hadoop.ha").debug();

            (302: 5) LOG.info("Successfully created " + znodeWorkingDir + " in ZK.");
            LOG.SUCCESFULLY_CREATED_ZNODE_WORKING_DIRECTORY(znodeWorkingDir).tag("org.apache.hadoop.ha").tag("ZOOKEEPER").info();

            (317: 7) LOG.info("Recursively deleting " + znodeWorkingDir + " from ZK...");
            LOG.RECURSIVELY_DELETING_ZNODE_WORKING_DIRECTORY(znodeWorkingDir).tag("org.apache.hadoop.ha").tag("ZOOKEEPER").info();

            (330: 5) LOG.info("Successfully deleted " + znodeWorkingDir + " from ZK.");
            LOG.SUCCESFULLY_DELETED_ZNODE(znodeWorkingDir).tag("org.apache.hadoop.ha").tag("ZOOKEEPER").info();

            (348: 5) LOG.info("Yielding from election");
            LOG.YIELDING_FROM_ELECTION().tag("org.apache.hadoop.ha").tag("quitElection()").info();

            (403: 5) LOG.debug("CreateNode result: " + rc + " for path: " + path + " connectionState: " + zkConnectionState +"  for " + this);
?                        LOG.CREATENODE_RESULT_FOR_PATH(rc, path, "connectionState", zkConnectionState, this).tag("org.apache.hadoop.ha").debug();

            String errorMessage = "Received create error from Zookeeper. code:" + code.toString() + " for path " + path;
            (434: 5) LOG.debug(errorMessage);
            LOG.CREATE_ERROR_CODE_FOR_PATH(code.toString, path).tag("org.apache.hadoop.ha").tag("ZOOKEEPER").debug();

            (438: 9) LOG.debug("Retrying createNode createRetryCount: " + createRetryCount);
            LOG.RETRYING_CREATE_NODE_RETRY_COUNT(createRetryCount).tag("org.apache.hadoop.ha").debug();

            (447: 7) LOG.warn("Lock acquisition failed because session was lost");
            LOG.LOCK_ACQUISITION_FAILED_SESSION_LOST().tag("org.apache.hadoop.ha").warn();

            (465: 5) LOG.debug("StatNode result: " + rc + " for path: " + path+ " connectionState: " + zkConnectionState + " for " + this);
            LOG.STATNODE_RESULT_FOR_PATH(rc, path, "connectionState", zkConnectionState, this).tag("org.apache.hadoop.ha").debug();

            String errorMessage = "Received stat error from Zookeeper. code:" + code.toString();
            (495: 5) LOG.debug(errorMessage);
            LOG.STAT_ERROR_CODE(code.toString()).tag("org.apache.hadoop.ha").tag("ZOOKEEPER").debug();

            (507: 7) LOG.warn("Lock monitoring failed because session was lost");
            LOG.SESSION_LOST_LOCK_MONITORING_FAILED().tag("org.apache.hadoop.ha").warn();

            (532: 5) LOG.debug("Watcher event type: " + eventType + " with state:" + event.getState() + " for path:" + event.getPath() + " connectionState: " + zkConnectionState + " for " + this);
            LOG.WATCHER_EVENT_TYPE_STATE_PATH(eventType, event.getState(), event.getPath(), "connectionState", zkConnectionState, path).tag("org.apache.hadoop.ha").debug();

            (541: 9) LOG.info("Session connected.");
            LOG.SESSION_CONNECTED().tag("org.apache.hadoop.ha").tag("processWatchEvent()").info();

            (552: 9) LOG.info("Session disconnected. Entering neutral mode...");
            LOG.SESSION_DISCONNECTED_ENTERING_NEUTRAL_MODE().tag("org.apache.hadoop.ha").tag("processWatchEvent()").info();

            (562: 9) LOG.info("Session expired. Entering neutral mode and rejoining...");
            LOG.SESSION_EXPIRED_ENTERING_NEUTRAL_MODE_AND_REJOINING().tag("org.apache.hadoop.ha").tag("processWatchEvent()").info();

            (591: 9) LOG.debug("Unexpected node event: " + eventType + " for path: " + path);
            LOG.UNEXPECTED_NODE_EVENT_FOR_PATH(eventType, path).tag("org.apache.hadoop.ha").debug();

            (633: 5) LOG.fatal(errorMessage);
            LOG.FATAL_ERROR(errorMessage).tag("org.apache.hadoop.ha").fatal();

            (640: 5) LOG.debug("Monitoring active leader for " + this);
            LOG.MONITORING_ACTIVE_LEADER(this).tag("org.apache.hadoop.ha").debug();

            (659: 5) LOG.info("Trying to re-establish ZK session");
            LOG.REESTABLISHING_ZOOKEEPER_SESSION().tag("org.apache.hadoop.ha").info();

            (723: 7) LOG.debug("Establishing zookeeper connection for " + this);
            LOG.ESTABLISHING_ZOOKEEPER_CONNECTION(this).tag("org.apache.hadoop.ha").debug();

            (728: 9) LOG.warn(e);
            LOG.IO_EXCEPTION(e).tag("org.apache.hadoop.ha").warn();

            (731: 9) LOG.warn(e);
            LOG.KEEPER_EXCEPTION(e).tag("org.apache.hadoop.ha").warn();

            (750: 5) LOG.debug("Created new connection for " + this);
            LOG.CREATED_NEW_CONNECTION(this).tag("org.apache.hadoop.ha").debug();

            (757: 5) LOG.debug("Terminating ZK connection for " + this);
            LOG.TERMINATING_CONNECTION(this).tag("org.apache.hadoop.ha").tag("ZOOKEEPER").debug();

            (763: 7) LOG.warn(e);
            LOG.INTERRUPTED_EXCEPTION(e).tag("org.apache.hadoop.ha").warn();

            (783: 7) LOG.debug("Becoming active for " + this);
            LOG.BECOMING_ACTIVE_FOR(this).tag("org.apache.hadoop.ha").debug();

            (788: 7) LOG.warn("Exception handling the winning of election", e);
            LOG.HANDLING_WINNING_OF_ELECTION_EXCEPTION(e).tag("org.apache.hadoop.ha").warn();

            (801: 5) LOG.info("Writing znode " + zkBreadCrumbPath + " to indicate that the local node is the most recent active...");
            LOG.ZNODE_TO_INDICATE_LOCAL_MOST_RECENT_ACTIVE_NODE(zkBreadCrumbPath).tag("org.apache.hadoop.ha").info();

            (821: 5) LOG.info("Deleting bread-crumb of active node...");
            LOG.DELETING_ACTIVE_NODE().tag("org.apache.hadoop.ha").info();

            (839: 7) LOG.warn("Unable to delete our own bread-crumb of being active at " + zkBreadCrumbPath + ": " + e.getLocalizedMessage() ". Expecting to be fenced by the next active.");
            LOG.UNABLE_TO_DELETE_ACTIVE_NODE(zkBreadCrumbPath, e.getLocalizedMessage(), "expecting to be fenced by the next active bread-crumb").tag("org.apache.hadoop.ha").warn();

            (854: 5) LOG.info("Checking for any old active which needs to be fenced...");
            LOG.CHECKING_OLD_ACTIVE_NODE_TO_BE_FENCED().tag("org.apache.hadoop.ha").info();

            (864: 9) LOG.info("No old node to fence");
            LOG.NO_OLD_NODE_TO_FENCE().tag("org.apache.hadoop.ha").info();

            (875: 5) LOG.info("Old node exists: " + StringUtils.byteToHexString(data));
            LOG.OLD_NODE_EXISTS(StringUtils.byteToHexString(data)).tag("org.apache.hadoop.ha").info();

            (877: 7) LOG.info("But old node has our own data, so don't need to fence it.");
            LOG.OLD_NODE_WITH_OWN_DATA_NOT_FENCING().tag("org.apache.hadoop.ha").info();

            (886: 7) LOG.debug("Becoming standby for " + this);
            LOG.BECOMING_STANDBY(this).tag("org.apache.hadoop.ha").debug();

            (894: 7) LOG.debug("Entering neutral mode for " + this);
            LOG.ENTERING_NEUTRAL_MODE(this).tag("org.apache.hadoop.ha").debug();

            (983: 7) LOG.warn("Ignoring stale result from old client with sessionId " + String.format("0x%08x", ((ZooKeeper)ctx).getSessionId()));
            LOG.IGNORING_RESULT_FROM_OLD_CLIENT_WITH_SESSIONID(String.format("0x%08x", ((ZooKeeper)ctx).getSessionId())).tag("org.apache.hadoop.ha").warn();

            (1028: 11) LOG.error("Connection timed out: couldn't connect to ZooKeeper in " + connectionTimeoutMs + " milliseconds");
            LOG.UNABLE_TO_CONNECT_ZOOKEEPER_TIMED_OUT_MS(connectionTimeoutMs).tag("org.apache.hadoop.ha").error();

        FailoverController.java  (7 usages)
        String msg = "Unable to get service state for " + target;
            (126: 7) LOG.error(msg + ": " + e.getLocalizedMessage());
            LOG.UNABLE_TO_GET_SERVICE_STATE(target).tag("org.apache.hadoop.ha").error();

            (142: 9) LOG.warn("Service is not ready to become active, but forcing: " +notReadyReason);
            LOG.SERVICE_NOT_READY_TO_BECOME_ACTIVE_FORCING(notReadyReason).tag("org.apache.hadoop.ha").warn();

            (175: 7) LOG.warn("Unable to gracefully make " + svc + " standby (" +sfe.getMessage() + ")");
            LOG.UNABLE_TO_PERFORM_MAKE_STANDBY(svc, sfe.getMessage()).tag("org.apache.hadoop.ha").warn();

            (178: 7) LOG.warn("Unable to gracefully make " + svc +" standby (unable to connect)", ioe);
            LOG.UNABLE_TO_PERFORM_MAKE_STANDBY(svc, ioe).tag("org.apache.hadoop.ha").tag("IO_EXCEPTION").warn();

            (230: 7) LOG.error("Unable to make " + toSvc + " active (" + sfe.getMessage() + "). Failing back.");
            LOG.UNABLE_TO_ACTIVATE_FAILING(toSvc, sfe.getMessage()).tag("org.apache.hadoop.ha").error();

            (235: 7) LOG.error("Unable to make " + toSvc + " active (unable to connect). Failing back.", ioe);
            LOG.UNABLE_TO_MAKE_ACTIVE_FAILING(toSvc, ioe).tag("org.apache.hadoop.ha").tag("IO_EXCEPTION").error();

            msg += ". Failback to " + fromSvc + " failed (" + ffe.getMessage() + ")";
            (255: 11) LOG.fatal(msg);
            LOG.FAILBACK_FAILED_FROM(fromSvc, ffe.getMessage()).tag("org.apache.hadoop.ha").fatal();

        HAAdmin.java  (3 usages)
            (176: 9) LOG.warn("Proceeding with manual HA state management even though\n" + "automatic failover is enabled for " + target);
            LOG.USING_MANUAL_HA_STATE_MANAGEMENT_AUTOMATIC_FAILOVER_ENABLED(target).tag("org.apache.hadoop.ha").warn();

            (327: 9) LOG.debug("Operation failed", ioe);
            LOG.OPERATION_FAILED(ioe).tag("org.apache.hadoop.ha").tag("run()").debug();

            (375: 9) LOG.fatal("Aborted");
            LOG.ABORTED().tag("org.apache.hadoop.ha").tag("runCmd()").fatal();

        HealthMonitor.java  (6 usages)
            (139: 5) LOG.info("Stopping HealthMonitor thread");
            LOG.STOPPING_HEALTH_MONITOR_THREAD().tag("org.apache.hadoop.ha").info();

            (171: 7) LOG.warn("Could not connect to local service at " + targetToMonitor + ": " + e.getMessage());
            LOG.UNABLE_TO_CONNECT_LOCAL_SERVICE(targetToMonitor, e.getMessage()).tag("org.apache.hadoop.ha").warn();

            (194: 9) LOG.warn("Service health check failed for " + targetToMonitor + ": " + e.getMessage());
            LOG.SERVICE_HEALTH_CHECK_FAILED(targetToMonitor, e.getMessage()).tag("org.apache.hadoop.ha").warn();

            (198: 9) LOG.warn("Transport-level exception trying to monitor health of " + targetToMonitor + ": " + t.getLocalizedMessage());
            LOG.TRANSPORT_EXCEPTION_WHILE_MONITORING_HEALTH(targetToMonitor, t.getLocalizedMessage()).tag("org.apache.hadoop.ha").warn();

            (224: 7) LOG.info("Entering state " + newState);
            LOG.ENTERING_STATE(newState).tag("org.apache.hadoop.ha").tag("enterState()").info();

            (261: 11) LOG.fatal("Health monitor failed", e);
            LOG.HEALTH_MONITOR_FAILED(e).tag("org.apache.hadoop.ha").tag("MonitorDaemon").fatal();

        NodeFencer.java  (7 usages)
            (91: 5) LOG.info("====== Beginning Service Fencing Process... ======");
            LOG.BEGINNING_SERVICE_FENCING_PROCESS().tag("org.apache.hadoop.ha").info();

            (94: 7) LOG.info("Trying method " + (++i) + "/" + methods.size() +": " + method);
            LOG.TRYING_METHOD(++i, methods.size(), method).tag("org.apache.hadoop.ha").info();

            (98: 11) LOG.info("====== Fencing successful by method " + method + " ======");
            LOG.FENCING_SUCCESSFUL_BY_METHOD(method).tag("org.apache.hadoop.ha").info();

            (102: 9) LOG.error("Fencing method " + method + " misconfigured", e);
            LOG.FENCING_METHOD_MISCONFIGURED(method, e).tag("org.apache.hadoop.ha").error();

            (105: 9) LOG.error("Fencing method " + method + " failed with an unexpected error.", t);
            LOG.FENCING_METHOD_FAILED_ERROR(method, t).tag("org.apache.hadoop.ha").error();

            (108: 7) LOG.warn("Fencing method " + method + " was unsuccessful.");
            LOG.FENCING_METHOD_UNSUCCESSFUL(method).tag("org.apache.hadoop.ha").warn();

            (111: 5) LOG.error("Unable to fence service by any configured method.");
            LOG.UNABLE_TO_FENCE_SERVICE_BY_PROVIDED_METHODS().tag("org.apache.hadoop.ha").error();

        ShellCommandFencer.java  (5 usages)
            (87: 7) LOG.warn("Unable to execute " + cmd, e);
            LOG.UNABLE_TO_EXECUTE(cmd, e).tag("org.apache.hadoop.ha").tag("tryFence()").warn();

            (92: 5) LOG.info("Launched fencing command '" + cmd + "' with " + ((pid != null) ? ("pid " + pid) : "unknown pid"));
????                        LOG.STARTED_FENCING_COMMAND(cmd, ((pid != null) ? pid : "unknown pid") ).tag("org.apache.hadoop.ha").info();

            (117: 7) LOG.warn("Interrupted while waiting for fencing command: " + cmd);
            LOG.INTERRUPTED_WHILE_WAITING_FOR_FENCING_COMMAND(cmd).tag("org.apache.hadoop.ha").warn();

            (158: 9) LOG.trace("Unable to determine pid for " + p + " since it is not a UNIXProcess");
            LOG.UNABLE_TO_DETERMINE_PID_NOT_UNIX_PROCESS(p).tag("org.apache.hadoop.ha").trace();

            (163: 7) LOG.trace("Unable to determine pid for " + p, t);
            LOG.UNABLE_TO_DETERMINE_PID(p, t).tag("org.apache.hadoop.ha").trace();

        SshFenceByTcpPort.java  (20 usages)
            (93: 7) LOG.warn("Unable to create SSH session", e);
            LOG.UNABLE_TO_CREATE_SSH_SESSION(e).tag("org.apache.hadoop.ha").warn();

            (97: 5) LOG.info("Connecting to " + host + "...");
            LOG.CONNECTING(host).tag("org.apache.hadoop.ha").info();

            (102: 7) LOG.warn("Unable to connect to " + host + " as user " + args.user, e);
            LOG.UNABLE_TO_CONNECT_HOST_AS_USER(host, args.user, e).tag("org.apache.hadoop.ha").info();

            (106: 5) LOG.info("Connected to " + host);
            LOG.CONNECTED_HOST(host).tag("org.apache.hadoop.ha").info();

            (111: 7) LOG.warn("Unable to achieve fencing on remote host", e);
            LOG.UNABLE_TO_ACHIEVE_FENCING_ON_REMOTE_HOST(e).tag("org.apache.hadoop.ha").warn();

            (135: 7) LOG.info("Looking for process running on port " + port);
            LOG.LOOKING_FOR_PROCESS_RUNNING_ON_PORT(port).tag("org.apache.hadoop.ha").info();

            (139: 9) LOG.info("Successfully killed process that was " + "listening on port " + port);
            LOG.KILLED_PROCESS_LISTENING_ON_PORT(port).tag("org.apache.hadoop.ha").info();

            (147: 9) LOG.info("Indeterminate response from trying to kill service. Verifying whether it is running using nc...");
            LOG.UNCLEAR_RESPONSE_FROM_KILL_SERVICE_VERIFYING_RUNNING_USING_NC().tag("org.apache.hadoop.ha").info();

            (154: 11) LOG.warn("Unable to fence - it is running but we cannot kill it");
            LOG.UNABLE_TO_FENCE_RUNNING_PROCESS_NOT_KILLABLE().tag("org.apache.hadoop.ha").warn();

            (157: 11) LOG.info("Verified that the service is down.");
            LOG.SERVICE_IS_DOWN_VERIFIED().tag("org.apache.hadoop.ha").info();

            (163: 7) LOG.info("rc: " + rc);
            LOG.INFO().RETURN_CODE(rc).tag("org.apache.hadoop.ha").tag("doFence()");

            (166: 7) LOG.warn("Interrupted while trying to fence via ssh", e);
            LOG.INTERRUPTED_WHILE_FENCING_VIA_SSH(e).tag("org.apache.hadoop.ha").warn();

            (169: 7) LOG.warn("Unknown failure while trying to fence via ssh", e);
            LOG.UNKNOWN_FAILURE_WHILE_FENCING_VIA_SSH(e).tag("org.apache.hadoop.ha").warn();

            (180: 5) LOG.debug("Running cmd: " + cmd);
            LOG.RUNNING_COMMAND(cmd).tag("org.apache.hadoop.ha").tag("execCommand()").debug();

            (211: 9) LOG.warn("Couldn't disconnect ssh channel", t);
            LOG.UNABLE_TO_DISCONNECT_SSH_CHANNEL(t).tag("org.apache.hadoop.ha").warn();

x                        CUSTOM LOGGERS
            (299: 9) LOG.debug(message);
            (302: 9) LOG.info(message);
            (305: 9) LOG.warn(message);
            (308: 9) LOG.error(message);
            (311: 9) LOG.fatal(message);
x
        StreamPumper.java  (1 usage)
?           (58: 30) ShellCommandFencer.LOG.warn(logPrefix + ": Unable to pump output from " + type, t);
?           LOG.UNABLE_TO_PUMP_OUTPUT_FROM(type, t, logPrefix).tag("org.apache.hadoop.ha").warn();

        ZKFailoverController.java  (28 usages)
            (154: 7) LOG.fatal("Automatic failover is not enabled for " + localTarget + ". Please ensure that automatic failover is enabled in the configuration before running the ZK failover controller.");
            LOG.AUTOMATIC_FAILOVER_NOT_ENABLED(localTarget, "Please ensure that automatic failover is enabled in the configuration before running the ZK failover controller.").tag("org.apache.hadoop.ha").fatal();

            (186: 7) LOG.fatal("Unable to start failover controller. Unable to connect to ZooKeeper quorum at " + zkQuorum + ". Please check the configured value for " + ZK_QUORUM_KEY + " and ensure that ZooKeeper is running.");
            LOG.UNABLE_TO_START_FAILOVER_CONTROLLER_CANT_CONNECT(zkQuorum, "check config value", ZK_QUORUM_KEY).tag("org.apache.hadoop.ha").fatal();

            (212: 7) LOG.fatal("Unable to start failover controller. "
            + "Parent znode does not exist.\n" + "Run with -formatZK flag to initialize ZooKeeper.");
            LOG.UNABLE_TO_START_FAILOVER_CONTROLLER_PARENT_ZNODE_NOT_EXISTS("Run with -formatZK flag to initialize ZooKeeper.").tag("org.apache.hadoop.ha").fatal();

            (221: 7) LOG.fatal("Fencing is not configured for " + localTarget + ".\n" + "You must configure a fencing method before using automatic " + "failover.", e);
            LOG.FENCING_NOT_CONFIGURED(localTarget, "Configure fencing method before using automatic failover.", e).tag("org.apache.hadoop.ha").fatal();

            (262: 9) LOG.error("Unable to clear zk parent znode", e);
            LOG.UNABLE_TO_CLEAR_ZK_PARENT_ZNODE(e).tag("org.apache.hadoop.ha").error();

            (284: 7) LOG.debug("Failed to confirm", e);
            LOG.FAILED_TO_CONFIRM(e).tag("org.apache.hadoop.ha").debug();

            (364: 5) LOG.fatal("Fatal error occurred:" + err);
            LOG.FATAL_ERROR(err).tag("org.apache.hadoop.ha").tag("fatalError()").fatal();

            (370: 5) LOG.info("Trying to make " + localTarget + " active...");
            LOG.TRYING_TO_ACTIVATE(localTarget).tag("org.apache.hadoop.ha").info();

            String msg = "Successfully transitioned " + localTarget +" to active state";
            (377: 7) LOG.info(msg);
            LOG.SUCCESSFULLY_TRANSITIONED_TO_ACTIVE_STATE(localTarget).tag("org.apache.hadoop.ha").info();

            String msg = "Couldn't make " + localTarget + " active";
            (382: 7) LOG.fatal(msg, t);
            LOG.UNABLE_TO_ACTIVATE(localTarget).tag("org.apache.hadoop.ha").fatal();

            (463: 5) LOG.warn(timeoutMillis + "ms timeout elapsed waiting for an attempt to become active");
            LOG.ACTIVATION_WAITING_TIMEOUT_ELAPSED(timeoutMillis).tag("org.apache.hadoop.ha").warn();

            (473: 5) LOG.info("ZK Election indicated that " + localTarget +" should become standby");
            LOG.BECOMING_STANDBY_BY_ELECTION(localTarget).tag("org.apache.hadoop.ha").info();

            (478: 7) LOG.info("Successfully transitioned " + localTarget +" to standby state");
            LOG.TRANSITIONED_TO_STANDBY_STATE(localTarget).tag("org.apache.hadoop.ha").info();

            (481: 7) LOG.error("Couldn't transition " + localTarget + " to standby state", e);
            LOG.UNABLE_TO_TRANSITION_TO_STANDBY_STATE(localTarget, e).tag("org.apache.hadoop.ha").error();

            (501: 5) LOG.info("Should fence: " + target);
            LOG.FENCING(target).tag("org.apache.hadoop.ha").tag("doFence()").info();

            (507: 7) LOG.info("Successfully transitioned " + target + " to standby state without fencing");
            LOG.TRANSITIONED_TO_STANDBY_STATE_WITHOUT_FENCING(target).tag("org.apache.hadoop.ha").info();

            (515: 7) LOG.error("Couldn't fence old active " + target, e);
            LOG.UNABLE_TO_FENCE_OLD_ACTIVE(target,e).tag("org.apache.hadoop.ha").error();

            (560: 9) LOG.info("Requested by " + UserGroupInformation.getCurrentUser() +" at " + Server.getRemoteAddress() + " to cede active role.");
            LOG.CEDING_ACTIVE_ROLE_BY_USER(UserGroupInformation.getCurrentUser(), Server.getRemoteAddress()).tag("org.apache.hadoop.ha").info();

            (565: 11) LOG.info("Successfully ensured local node is in standby mode");
            LOG.LOCAL_NODE_IN_STANDBY_MODE().tag("org.apache.hadoop.ha").info();

            (567: 11) LOG.warn("Unable to transition local node to standby: " + ioe.getLocalizedMessage());
            LOG.UNABLE_TO_TRANSITION_TO_STANDBY_LOCAL_NODE(ioe.getLocalizedMessage()).tag("org.apache.hadoop.ha").warn();

            (569: 11) LOG.warn("Quitting election but indicating that fencing is necessary");
            LOG.QUITTING_ELECTION_BUT_FENCING_NECESSARY().tag("org.apache.hadoop.ha").warn();

            (633: 7) LOG.info("Local node " + localTarget + " is already active. No need to failover. Returning success.");
            LOG.LOCAL_NODE_ALREADY_ACTIVE_NO_FAILOVER_NEEDED(localTarget).tag("org.apache.hadoop.ha").info();

            (639: 5) LOG.info("Asking " + oldActive + " to cede its active state for " +timeout + "ms");
            LOG.ASKING_OLDACTIVE_SERVICE_TO_CEDE_ACTIVE_STATE_FOR_MS(oldActive, timeout).tag("org.apache.hadoop.ha").info();

            (668: 7) LOG.info("Successfully became active. " + attempt.status);
            LOG.SUCCESSFULLY_BECAME_ACTIVE(attempt.status).tag("org.apache.hadoop.ha").info();

            (730: 13) LOG.info("Would have joined master election, but this node is prohibited from doing so for " +TimeUnit.NANOSECONDS.toMillis(remainingDelay) + " more ms");
            LOG.NODE_PROHIBITED_TO_JOIN_MASTER_ELECTION_FOR_MS(TimeUnit.NANOSECONDS.toMillis(remainingDelay)).tag("org.apache.hadoop.ha").info();

            (744: 11) LOG.info("Ensuring that " + localTarget + " does not participate in active master election");
?                        LOG.ENSURING_SERVICE_NO_OTHER_PARTICIPATION_IN_ACTIVE_MASTER_ELECTION(localTarget).tag("org.apache.hadoop.ha").info();

            (751: 11) LOG.info("Quitting master election for " + localTarget +" and marking that fencing is necessary");
            LOG.QUITTING_MASTER_ELECTION_BUT_FENCING_NECESSARY(localTarget).tag("org.apache.hadoop.ha").info();

            (796: 5) LOG.info("Local service " + localTarget +" entered state: " + newState);
            LOG.LOCAL_SERVICE_ENTERED_STATE(localTarget, newState).tag("org.apache.hadoop.ha").info();

    org.apache.hadoop.ha.protocolPB  (1 usage)
        HAServiceProtocolServerSideTranslatorPB.java  (1 usage)
            (95: 7) LOG.warn("Unknown request source: " + proto.getReqSource());
            LOG.UNKNOWN_REQUEST_SOURCE(proto.getReqSource()).tag("org.apache.hadoop.ha.protocolPB").warn();

    org.apache.hadoop.http  (13 usages)
        HttpServer.java  (13 usages)
            (280: 9) LOG.info("adding path spec: " + path);
            LOGADDING_PATH_SPECIFICATION(path).tag("org.apache.hadoop.http").info();

            (422: 5) LOG.info("addJerseyResourcePackage: packageName=" + packageName+ ", pathSpec=" + pathSpec);
            LOG.ADD_JERSEY_RESOURCE_PACKAGE_PATH_SPECIFICATION(packageName, pathSpec).tag("org.apache.hadoop.http").info();

            (480: 8) LOG.info("Adding Kerberos (SPNEGO) filter to " + name);
            LOG.ADDING_KERBEROS_SPNEGO_FILTER_TO(name).tag("org.apache.hadoop.http").info();

            (496: 5) LOG.info("Added filter " + name + " (class=" + classname+ ") to context " + webAppContext.getDisplayName());
            LOG.ADDED_FILTER_CLASS_TO_CONTEXT(name, classname, webAppContext.getDisplayName()).tag("org.apache.hadoop.http").tag("webAppContext").info();

            (503: 9) LOG.info("Added filter " + name + " (class=" + classname+ ") to context " + ctx.getDisplayName());
            LOG.ADDED_FILTER_CLASS_TO_CONTEXT(name, classname, ctx.getDisplayName()).tag("org.apache.hadoop.http").info();


            (518: 5) LOG.info("Added global filter '" + name + "' (class=" + classname + ")");
            LOG.ADDED_GLOBAL_FILTER(name, classname).tag("org.apache.hadoop.http").info();

            (676: 9) LOG.info("Jetty bound to port " + listener.getLocalPort());
            LOG.JETTY_BOUND_TO_PORT(listener.getLocalPort()).tag("org.apache.hadoop.http").info();

            (679: 9) LOG.info("HttpServer.start() threw a non Bind IOException", ex);
            LOG.HTTPSERVER_START_METHOD_THREW_NON_BIND_IOEXCEPTION(ex).tag("org.apache.hadoop.http").info();

            (682: 9) LOG.info("HttpServer.start() threw a MultiException", ex);
            LOG.HTTPSERVER_START_METHOD_THREW_MULTIEXCEPTION(ex).tag("org.apache.hadoop.http").info();

            (766: 7) LOG.error("Error while stopping listener for webapp" + webAppContext.getDisplayName(), e);
            LOG.ERROR_WHILE_STOPPING_LISTENER_FOR_WEBAPP(webAppContext.getDisplayName(), e).tag("org.apache.hadoop.http").error();

            (776: 7) LOG.error("Error while destroying the SSLFactory" + webAppContext.getDisplayName(), e);
            LOG.ERROR_WHILE_DESTROYING_SSLFACTORY(webAppContext.getDisplayName(), e).tag("org.apache.hadoop.http").error();

            (786: 7) LOG.error("Error while stopping web app context for webapp "+ webAppContext.getDisplayName(), e);
            LOG.ERROR_WHILE_STOPPING_WEBAPP_CONTEXT_FOR_WEBAPP(webAppContext.getDisplayName(), e).tag("org.apache.hadoop.http").error();

            (793: 7) LOG.error("Error while stopping web server for webapp "+ webAppContext.getDisplayName(), e);
            LOG.ERROR_WHILE_STOPPING_WEBSERVER_FOR_WEBAPP(webAppContext.getDisplayName(), e).tag("org.apache.hadoop.http").error();

    org.apache.hadoop.http.lib  (1 usage)
        StaticUserWebFilter.java  (1 usage)
            (141: 7) LOG.warn(DEPRECATED_UGI_KEY + " should not be used. Instead, use " + HADOOP_HTTP_STATIC_USER + ".");
            LOG.DO_NOT_USE_DEPRECATED_UGI_KEY_USE_INSTEAD_HADOOP_HTTP_STATIC_USER(DEPRECATED_UGI_KEY, HADOOP_HTTP_STATIC_USER).tag("org.apache.hadoop.http.lib").warn();

    org.apache.hadoop.io  (16 usages)
        BloomMapFile.java  (1 usage)
            (242: 9) LOG.warn("Can't open BloomFilter: " + ioe + " - fallback to MapFile.");
            LOG.UNABLE_TO_OPEN_BLOOMFILTER_FALLBACK_TO_MAPFILE(ioe).tag("org.apache.hadoop.io").warn();

        IOUtils.java  (1 usage)
            (269: 9) LOG.debug("Ignoring exception while closing socket", ignored);
            LOG.IGNORING_EXCEPTION_WHILE_CLOSING_SOCKET(ignored).tag("org.apache.hadoop.io").debug();

        MapFile.java  (1 usage)
            (504: 9) LOG.warn("Unexpected EOF reading " + index +" at entry #" + count + ".  Ignoring.");
            LOG.UNEXPECTED_EOF_READING_IGNORING(index, count).tag("org.apache.hadoop.io").warn();

        ReadaheadPool.java  (2 usages)
            (152: 7) LOG.trace("submit readahead: " + req);
            LOG.READAHEAD_SUBMIT(req).tag("org.apache.hadoop.io").trace();

            (214: 9) LOG.warn("Failed readahead on " + identifier, ioe);
            LOG.READAHEAD_FAILED_ON(identifier, ioe).tag("org.apache.hadoop.io").warn();

        SequenceFile.java  (9 usages)
            (2181: 11) LOG.info("available bytes: " + valIn.available());
            LOG.AVAILABLE_BYTES(valIn.available()).tag("org.apache.hadoop.io").info();

            (2196: 11) LOG.debug(val + " is a zero-length value");
            LOG.ZERO_LENGTH_VALUE(val).tag("org.apache.hadoop.io").debug();

            (2220: 11) LOG.info("available bytes: " + valIn.available());
            LOG.AVAILABLE_BYTES(valIn.available()).tag("org.apache.hadoop.io").info();

            (2235: 11) LOG.debug(val + " is a zero-length value");
            LOG.ZERO_LENGTH_VALUE(val).tag("org.apache.hadoop.io").debug();

            (2569: 9) LOG.warn("Bad checksum at "+getPosition()+". Skipping entries.");
            LOG.SKIPPING_ENTRIES_BAD_CHECKSUM(getPosition()).tag("org.apache.hadoop.io").warn();

            (2779: 9) LOG.debug("running sort pass");
            LOG.RUNNING_SORT_PASS().tag("org.apache.hadoop.io").debug();

            (2879: 13) LOG.debug("flushing segment " + segments);
            LOG.FLUSHING_SEGMENT(segmets).tag("org.apache.hadoop.io").debug();

            (3168: 9) LOG.debug("running merge pass");
            LOG.RUNNING_MERGE_PASS().tag("org.apache.hadoop.io").debug();

            (3406: 15) LOG.debug("writing intermediate results to " + outputFile);
            LOG.WRITING_INTERMEDIATE_RESULTS_TO(outputFile).tag("org.apache.hadoop.io").debug();

        UTF8.java  (2 usages)
            (88: 7) LOG.warn("truncating long string: " + string.length()+ " chars, starting with " + string.substring(0, 20));
            LOG.TRUNCATING_LONG_STRING_TO(string.length(), string.substring(0,20)).tag("org.apache.hadoop.io").warn();

            (326: 7) LOG.warn("truncating long string: " + s.length() + " chars, starting with " + s.substring(0, 20));
            LOG.TRUNCATING_LONG_STRING_TO(s.length(), s.substring(0,20)).tag("org.apache.hadoop.io").warn();

    org.apache.hadoop.io.compress  (5 usages)
        CodecPool.java  (4 usages)
            (107: 7) LOG.info("Got brand-new compressor ["+codec.getDefaultExtension()+"]");
            LOG.GOT_NEW_COMPRESSOR(codec.getDefaultExtension()).tag("org.apache.hadoop.io.compress").info();

            (111: 9) LOG.debug("Got recycled compressor");
            LOG.GOT_RECYCLED_COMPRESSOR().tag("org.apache.hadoop.io.compress").debug();

            (134: 7) LOG.info("Got brand-new decompressor ["+codec.getDefaultExtension()+"]");
            LOG.GOT_NEW_COMPRESSOR(codec.getDefaultExtension()).tag("org.apache.hadoop.io.compress").info();

            (137: 9) LOG.debug("Got recycled decompressor");
            LOG.GOT_RECYCLED_COMPRESSOR().tag("org.apache.hadoop.io.compress").debug();

        DefaultCodec.java  (1 usage)
            (57: 5) LOG.warn("DefaultCodec.createOutputStream() may leak memory. Create a compressor first.");
            LOG.DEFAULT_CODEC_MAY_LEAK_MEMORY_CREATE_COMPRESSOR().tag("org.apache.hadoop.io.compress").tag("DefaultCodec.createOutputStream()").warn();

    org.apache.hadoop.io.compress.lz4  (4 usages)
        Lz4Compressor.java  (2 usages)
            (63: 9) LOG.warn(t.toString());
            LOG.IGNORING_FAILURE_TO_INITIALIZE_LZ4(t).tag("org.apache.hadoop.io.compress").warn();

            (66: 7) LOG.error("Cannot load " + Lz4Compressor.class.getName() + " without native hadoop library!");
            LOG.UNABLE_TO_LOAD_LZ4_COMPRESSOR_WITHOUT_NATIVE_HADOOP_LIBRARY(Lz4Compressor.class.getName()).tag("org.apache.hadoop.io.compress").error();

        Lz4Decompressor.java  (2 usages)
            (58: 9) LOG.warn(t.toString());
            LOG.IGNORING_FAILURE_TO_INITIALIZE_LZ4(t).tag("org.apache.hadoop.io.compress").warn();

            (61: 7) LOG.error("Cannot load " + Lz4Compressor.class.getName() +" without native hadoop library!");
            LOG.UNABLE_TO_LOAD_LZ4_COMPRESSOR_WITHOUT_NATIVE_HADOOP_LIBRARY(Lz4Compressor.class.getName()).tag("org.apache.hadoop.io.compress").error();

    org.apache.hadoop.io.compress.snappy  (2 usages)
        SnappyCompressor.java  (1 usage)
            (64: 9) LOG.error("failed to load SnappyCompressor", t);
            LOG.FAILED_TO_LOAD_SNAPPY_COMPRESSOR(t).tag("org.apache.hadoop.io.compress").error();

        SnappyDecompressor.java  (1 usage)
            (60: 9) LOG.error("failed to load SnappyDecompressor", t);
            LOG.FAILED_TO_LOAD_SNAPPY_DECOMPRESSOR(t).tag("org.apache.hadoop.io.compress").error();

    org.apache.hadoop.io.compress.zlib  (5 usages)
        BuiltInZlibDeflater.java  (2 usages)
            (77: 7) LOG.warn(strategy + " not supported by BuiltInZlibDeflater.");
            LOG.STRATEGY_NOT_SUPPORTED_BY_BUILTINZLIBDEFLATER(strategy).tag("org.apache.hadoop.io.compress").warn();

            (81: 7) LOG.debug("Reinit compressor with new compression configuration");
            LOG.REINITIALIZING_COMPRESSOR_WITH_NEW_CONFIGURATION().tag("org.apache.hadoop.io.compress").debug();

        ZlibCompressor.java  (1 usage)
            (258: 7) LOG.debug("Reinit compressor with new compression configuration");
            LOG.REINITIALIZING_COMPRESSOR_WITH_NEW_CONFIGURATION().tag("org.apache.hadoop.io.compress").debug();

        ZlibFactory.java  (2 usages)
            (48: 9) LOG.info("Successfully loaded & initialized native-zlib library");
            LOG.SUCCESSFULLY_LOADED_AND_INITIALIZED_NATIVE_ZLIB_LIBRARY().tag("org.apache.hadoop.io.compress").info();

            (50: 9) LOG.warn("Failed to load/initialize native-zlib library");
            LOG.FAILED_TO_LOAD_OR_INITIALIZE_NATIVE_ZLIB_LIBRARY().tag("org.apache.hadoop.io.compress").warn();

    org.apache.hadoop.io.file.tfile  (7 usages)
        Compression.java  (7 usages)
            (90: 13) LOG.info("Trying to load Lzo codec class: " + clazz);
            LOG.TRYING_TO_LOAD_LZO_CODEC_CLASS(clazz).tag("org.apache.hadoop.io.file").info();

            (279: 13) LOG.warn("Compressor obtained from CodecPool already finished()");
            LOG.COMPRESSOR_OBTAINED_ALREADY_FINISHED_FLAG().tag("org.apache.hadoop.io.file").warn();

            (282: 15) LOG.debug("Got a compressor: " + compressor.hashCode());
            LOG.GOT_COMPRESSOR(compressor.hashCode()).tag("org.apache.hadoop.io.file").debug();

            (299: 11) LOG.debug("Return a compressor: " + compressor.hashCode());
            LOG.RETURN_COMPRESSOR(compressor.hashCode()).tag("org.apache.hadoop.io.file").debug();

            (313: 13) LOG.warn("Deompressor obtained from CodecPool already finished()");
            LOG.DECOMPRESSOR_OBTAINED_ALREADY_FINISHED_FLAG().tag("org.apache.hadoop.io.file").warn();

            (316: 15) LOG.debug("Got a decompressor: " + decompressor.hashCode());
            LOG.GOT_DECOMPRESSOR(decompressor.hashCode()).tag("org.apache.hadoop.io.file").debug();

            (334: 11) LOG.debug("Returned a decompressor: " + decompressor.hashCode());
            LOG.RETURN_DECOMPRESSOR(decompressor.hashCode()).tag("org.apache.hadoop.io.file").debug();

    org.apache.hadoop.io.nativeio  (3 usages)
        NativeIO.java  (3 usages)
            (114: 9) LOG.debug("Initialized cache for IDs to User/Group mapping with a cache timeout of " + cacheTimeout/1000 + " seconds.");
            LOG.INITIALIZED_CACHE_FOR_USER_GROUP_ID_MAPPING_WITH_CACHE_TIMEOUT_SECONDS(cacheTimeout/1000).tag("org.apache.hadoop.io.nativeio").debug();

            (121: 9) LOG.error("Unable to initialize NativeIO libraries", t);
            LOG.UNABLE_TO_INITIALIZED_NATIVEIO_LIBRARIES(t).tag("org.apache.hadoop.io.nativeio").error();

            (275: 9) LOG.debug("Got " + type + " " + name + " for ID " + id + " from the native implementation");
            LOG.GOT_FROM_NATIVE_IMPLEMENTATION_INFO(type, name, id).tag("org.apache.hadoop.io.nativeio").debug();

    org.apache.hadoop.io.retry  (12 usages)
        RetryInvocationHandler.java  (5 usages)
            (93: 13) LOG.warn("Exception while invoking " + currentProxy.getClass() + "." + method.getName() + ". Not retrying because " + action.reason, e);
            LOG.EXCEPTION_INVOKING_NOT_RETRYING_REASON(currentProxy.getClass(), method.getName(), action.reason, e).tag("org.apache.hadoop.io").warn();


HELP                        (114: 15) LOG.debug(msg, e);
HELP                        (116: 15) LOG.warn(msg);

            (120: 15) LOG.debug("Exception while invoking " + method.getName()+ " of class " + currentProxy.getClass().getSimpleName() + ". Retrying " + formatSleepMessage(action.delayMillis), e);
            LOG.ERROR_WHILE_INVOKING_RETRYING(method.getName(), currentProxy.getClass().getSimpleName(), formatSleepMessage(action.delayMillis), e).tag("org.apache.hadoop.io").debug();

            (139: 17) LOG.warn("A failover has occurred since the start of this method invocation attempt.");
            LOG.FAILOVER_OCCURED_SINCE-START_OF_METHOD_INVOCATION_ATTEMPT().tag("org.apache.hadoop.io").warn();

        RetryPolicies.java  (4 usages)
            (368: 9) LOG.warn("Illegal value: there is no element in \"" + s + "\".");
            LOG.ILLEGAL_VALUE_NO_ELEMENT_IN(s).tag("org.apache.hadoop.io").warn();

            (372: 9) LOG.warn("Illegal value: the number of elements in \"" + s + "\" is " + elements.length + " but an even number of elements is expected.");
?                        LOG.ERROR_ODD_NUMBER_OF_ELEMENTS_IN(s, elements.length).tag("org.apache.hadoop.io").warn();

            (410: 9) LOG.warn("Failed to parse \"" + s + "\", which is the index " + i + " element in \"" + originalString + "\"", nfe);
?                        LOG.FAILED_TO_PARSE(s, "index", i, "element in", originalString, nfe).tag("org.apache.hadoop.io").warn();

            (416: 9) LOG.warn("The value " + n + " <= 0: it is parsed from the string \"" + s + "\" which is the index " + i + " element in \"" + originalString + "\"");
?                        LOG.VALUE_LOWER_THEN_ZERO(n, "from", s, "index", i, "element", originalString).tag("org.apache.hadoop.io").warn();

        RetryUtils.java  (3 usages)
            (74: 7) LOG.debug("multipleLinearRandomRetry = " + multipleLinearRandomRetry);
            LOG.MULTIPLE_LINEAR_RANDOM_RETRY(multipleLinearRandomRetry).tag("org.apache.hadoop.io").debug();

            (106: 13) LOG.debug("RETRY " + retries + ") policy="+ p.getClass().getSimpleName() + ", exception=" + e);
            LOG.RETRY_POLICY_EXCEPTION(retries, policy, exception).tag("org.apache.hadoop.io").debug();

            (109: 11) LOG.info("RETRY " + retries + ") policy="+ p.getClass().getSimpleName() + ", exception=" + e);
            LOG.RETRY_POLICY_EXCEPTION(retries, policy, exception).tag("org.apache.hadoop.io").info();

    org.apache.hadoop.io.serializer  (2 usages)
        SerializationFactory.java  (2 usages)
            (59: 7) LOG.warn("Serialization for various data types may not be available. Please configure "
+ CommonConfigurationKeys.IO_SERIALIZATIONS_KEY + " properly to have serialization support (it is currently not set).");
            LOG.SERIALIZATION_MAY_NOT_BE_AVAILABLE_CONFIGURE_PROPERTY(CommonConfigurationKeys.IO_SERIALIZATIONS_KEY).tag("org.apache.hadoop.io").warn();

            (81: 7) LOG.warn("Serialization class not found: ", e);
            LOG.SERIALIZATION_CLASS_NOT_FOUND(e).tag("org.apache.hadoop.io").warn();

    org.apache.hadoop.ipc  (103 usages)
        Client.java  (23 usages)
            (300: 9) LOG.debug("The ping interval is " + this.pingInterval + " ms.");
            LOG.PING_INTERVAL_MS(this.pingInterval).tag("org.apache.hadoop.ipc").debug();

            (324: 13) LOG.debug("RPC Server's Kerberos principal name for protocol="+ protocol.getCanonicalName() + " is " + serverPrincipal);
            LOG.SERVER_KERBEROS_PRINCIPAL_NAME_FOR_PROTOCOL(serverPrincipal, protocol.getCanonicalName()).tag("org.apache.hadoop.ipc").debug();

            (341: 9) LOG.debug("Use " + authMethod + " authentication for protocol "+ protocol.getSimpleName());
            LOG.USE_AUTHENTICATION_METHOD_FOR_PROTOCOL(authMethod, protocol.getSimpleName()).tag("org.apache.hadoop.ipc").debug();

            (473: 9) LOG.warn("Address change detected. Old: " + server.toString() +" New: " + currentAddr.toString());
            LOG.DETECTED_ADDRESS_CHANGE_OLD_NEW(server.toString(), currentAddr.toString()).tag("org.apache.hadoop.ipc").warn();

            (554: 17) LOG.debug("Exception encountered while connecting to the server : " + ex);
            LOG.EXCEPTION_WHILE_CONNECTING_TO_SERVER(ex).tag("org.apache.hadoop.ipc").debug();

            String msg = "Couldn't setup connection for "+ UserGroupInformation.getLoginUser().getUserName() + " to "+ serverPrincipal;
            (573: 15) LOG.warn(msg);
            LOG.UNABLE_TO_SETUP_CONNECTION_FOR_TO(UserGroupInformation.getLoginUser().getUserName(), serverPrincipal).tag("org.apache.hadoop.ipc").warn();

            (577: 13) LOG.warn("Exception encountered while connecting to the server : " + ex);
            LOG.EXCEPTION_WHILE_CONNECTING_TO_SERVER(ex).tag("org.apache.hadoop.ipc").warn();

            (598: 11) LOG.debug("Connecting to "+server);
            LOG.CONNECTING_TO_SERVER(server).tag("org.apache.hadoop.ipc").debug();

            (677: 9) LOG.warn("Not able to close a socket", e);
            LOG.UNABLE_TO_CLOSE_SOCKET(e).tag("org.apache.hadoop.ipc").warn();

            (713: 7) LOG.info("Retrying connect to server: " + server + ". Already tried "+ curRetries + " time(s); maxRetries=" + maxRetries);
            LOG.RETRYING_CONNECT_SERVER(server, curRetries, maxRetries).tag("org.apache.hadoop.ipc").info();

            (729: 11) LOG.warn("Failed to connect to server: " + server + ": "+ action.reason, ioe);
            LOG.FAILED_TO_CONNECT_SERVER(server, action.reason, ioe).tag("org.apache.hadoop.ipc").warn();

            (741: 7) LOG.info("Retrying connect to server: " + server + ". Already tried "+ curRetries + " time(s); retry policy is " + connectionRetryPolicy);
            LOG.RETRYING_CONNECT_SERVER(server, curRetries, connectionRetryPolicy).tag("org.apache.hadoop.ipc").info();

            (840: 9) LOG.debug(getName() + ": starting, having connections " + connections.size());
            LOG.STARTING_THREAD_WITH-CONNECTIONS(getName(), connections.size()).tag("org.apache.hadoop.ipc").debug();

            (851: 9) LOG.warn("Unexpected error reading responses on connection " + this, t);
            LOG.ERROR_READING_RESPONSES_ON_CONNECTION(this, t).tag("org.apache.hadoop.ipc").warn();

            (858: 9) LOG.debug(getName() + ": stopped, remaining connections "+ connections.size());
            LOG.THREAD_STOPPED_REMAINING_CONECTIONS(getName(), connections.size()).tag("org.apache.hadoop.ipc").debug();

            (901: 19) LOG.debug(getName() + " sending #" + call.id);
            LOG.THREAD_SENDING_RPC_CALL(getName(), call.id).tag("org.apache.hadoop.ipc").debug();

            (956: 11) LOG.debug(getName() + " got value #" + callId);
            LOG.THREAD_GOT_RPC_CALL_VALUE(getName(), callId).tag("org.apache.hadoop.ipc").debug();

            (989: 9) LOG.error("The connection is not in the closed state");
            LOG.CONNECTION_IS_NOT_IN_CLOSED_STATE().tag("org.apache.hadoop.ipc").error();

            (1009: 11) LOG.warn("A connection is closed for no cause and calls are not empty");
            LOG.CLOSED_CONNECTION_AND_CALLS_NOT_EMPTY().tag("org.apache.hadoop.ipc").warn();

            (1019: 11) LOG.debug("closing ipc connection to " + server + ": " + closeException.getMessage(),closeException);
            LOG.CLOSING_IPC_CONNECTION_TO_SERVER(server, closeException.getMessage(), closeException).tag("org.apache.hadoop.ipc").debug();

            (1027: 9) LOG.debug(getName() + ": closed");
            LOG.THREAD_CLOSED(getName()).tag("org.apache.hadoop.ipc").debug();

            (1073: 7) LOG.debug("Stopping client");
            LOG.STOPPING_CLIENT().tag("org.apache.hadoop.ipc").debug();

            (1215: 7) LOG.warn("interrupted waiting to send rpc request to server", e);
            LOG.INTERRUPTED_WAITING_TO_SEND_RPC_REQUEST_TO_SERVER(e).tag("org.apache.hadoop.ipc").warn();

        ProtobufRpcEngine.java  (7 usages)
            (197: 9) LOG.trace(Thread.currentThread().getId() + ": Call -> " + remoteId + ": " + method.getName() + " {" + TextFormat.shortDebugString((Message) args[1]) + "}");
            LOG.CLIENT_RPC_METHOD_CALL_INVOKE(Thread.currentThread().getId(), remoteId, method.getName(), TextFormat.shortDebugString((Message) args[1])).tag("org.apache.hadoop.ipc").trace();

            (207: 11) LOG.trace(Thread.currentThread().getId() + ": Exception <- " + remoteId + ": " + method.getName() +" {" + e + "}");
            LOG.EXCEPTION_WHILE_INVOKING_CLIENT_RPC_METHOD(Thread.currentThread().getId(), remoteId, method.getName(), e).tag("org.apache.hadoop.ipc").trace();

            (217: 9) LOG.debug("Call: " + method.getName() + " took " + callTime + "ms");
            LOG.CLIENT_RPC_METHOD_CALL_TIME_MS(method.getName(), callTime).tag("org.apache.hadoop.ipc").debug();

            (232: 11) LOG.trace(Thread.currentThread().getId() + ": Response <- " + remoteId + ": " + method.getName() +" {" + TextFormat.shortDebugString(returnMessage) + "}");
            LOG.CLIENT_RPC_METHOD_CALL_RETURN_MESSAGE(Thread.currentThread().getId(), remoteId, method.getName(), TextFormat.shortDebugString(returnMessage)).tag("org.apache.hadoop.ipc").trace();

            (453: 11) LOG.info("Call: connectionProtocolName=" + connectionProtocolName + ", method=" + methodName);
            LOG.SERVER_RPC_METHOD_CALL(connectionProtocolName, methodName).tag("org.apache.hadoop.ipc").info();

            String msg = "Unknown method " + methodName + " called on " + connectionProtocolName + " protocol.";
            (464: 11) LOG.warn(msg);
            LOG.UNKNOWN_SERVER_METHOD_CALLED_ON_PROTOCOL(methodName, connectionProtocolName).tag("org.apache.hadoop.ipc").warn();

            (478: 13) LOG.info("Served: " + methodName + " queueTime= " + qTime +" procesingTime= " + processingTime);
            LOG.RPC_SERVER_SERVED_METHOD_QUEUE_PROCESSING_TIME(methodName, qTime, processingTime).tag("org.apache.hadoop.ipc").info();

        RPC.java  (9 usages)
            (125: 9) LOG.warn("Interface " + childInterface +" ignored because it does not extend VersionedProtocol");
            LOG.INTERFACE_DOES_NOT_EXTEND_VERSIONED_PROTOCOL_IGNORING(childInterface).tag("org.apache.hadoop.ipc").warn();

            (385: 9) LOG.info("Server at " + addr + " not available yet, Zzzzz...");
            LOG.SERVER_NOT_AVAILABLE(addr).tag("org.apache.hadoop.ipc").info();

            (388: 9) LOG.info("Problem connecting to server: " + addr);
            LOG.PROBLEM_CONNECTING_SERVER(addr).tag("org.apache.hadoop.ipc").info();

            (391: 9) LOG.info("No route to host for server: " + addr);
            LOG.NO_ROUTE_TO_HOST_FOR_SERVER(addr).tag("org.apache.hadoop.ipc").info();

            (620: 7) LOG.error("Closing proxy or invocation handler caused exception", e);
            LOG.EXCEPTION_CAUSED_BY_CLOSING_PROXY_OR_IVOCATION_HANDLER(e).tag("org.apache.hadoop.ipc").error();

            (622: 7) LOG.error("RPC.stopProxy called on non proxy.", e);
            LOG.RPC_STOPPROXY_METHOD_CALLED_ON_NON_PROXY_OBJECT(e).tag("org.apache.hadoop.ipc").error();

            (913: 8) LOG.warn("Protocol "  + protocolClass + " NOT registered as cannot get protocol version ");
            LOG.PROTOCOL_NOT_REGISTERED_UNABLE_TO_GET_PROTOCOL_VERSION(protocolClass).tag("org.apache.hadoop.ipc").warn();

            (921: 6) LOG.debug("RpcKind = " + rpcKind + " Protocol Name = " + protocolName +  " version=" + version +
            " ProtocolImpl=" + protocolImpl.getClass().getName() + " protocolClass=" + protocolClass.getName());
            LOG.RPC_SERVER_INFORMATION(rpcKind, protocolName, version, protocolImpl.getClass().getName(), protocolClass.getName()).tag("org.apache.hadoop.ipc").debug();

            (962: 8) LOG.debug("Size of protoMap for " + rpcKind + " ="+ getProtocolImplMap(rpcKind).size());
            LOG.RPC_SERVER_SIZE_OF_PROTOMAP_FOR_RPC_KIND(rpcKind, getProtocolImplMap(rpcKind).size()).tag("org.apache.hadoop.ipc").debug();

        Server.java  (60 usages)
            (245: 5) LOG.debug("rpcKind=" + rpcKind + ", rpcRequestWrapperClass=" + rpcRequestWrapperClass + ", rpcInvoker=" + rpcInvoker);
            LOG.RPC_REGISTER_KIND_AND_DESERIALIZE_CLASS(rpcKind, rpcRequestWrapperClass, rpcInvoker).tag("org.apache.hadoop.ipc").debug();

            (531: 9) LOG.info("Starting " + getName());
            LOG.STARTING_THREAD(getName()).tag("org.apache.hadoop.ipc").info();

            (538: 13) LOG.error("Error closing read selector in " + this.getName(), ioe);
            LOG.ERROR_CLOSING_READ_SELECTOR_IN_THREAD(this.getName(), ioe).tag("org.apache.hadoop.ipc").error();

            (565: 15) LOG.info(getName() + " unexpectedly interrupted", e);
            LOG.THREAD_UNEXPECTEDLY_INTERRUPTED(getName(), e).tag("org.apache.hadoop.ipc").info();

            (568: 13) LOG.error("Error in Reader", ex);
            LOG.RPC_SERVER_ERROR_IN_READER(ex).tag("org.apache.hadoop.ipc").error();

            (640: 15) LOG.debug(getName() + ": disconnecting client " + c.getHostAddress());
            LOG.RPC_SERVER_DISCONNECTING_CLIENT(getName(), c.getHostAddress).tag("org.apache.hadoop.ipc").debug();

            (655: 7) LOG.info(getName() + ": starting");
            LOG.RPC_STARTING_THREAD(getName()).tag("org.apache.hadoop.ipc").info();

            (678: 11) LOG.warn("Out of Memory in server select", e);
            LOG.OUT_OF_MEMORY_IN_SERVER_SELECT(e).tag("org.apache.hadoop.ipc").warn();

            (687: 7) LOG.info("Stopping " + this.getName());
            LOG.STOPPING_THREAD(this.getName()).tag("org.apache.hadoop.ipc").info();

            (710: 13) LOG.debug(getName() + ": disconnecting client " + c.getHostAddress());
            LOG.RPC_SERVER_DISCONNECTING_CLIENT(getName(), c.getHostAddress()).tag("org.apache.hadoop.ipc").debug();

            (741: 13) LOG.debug("Server connection from " + c.toString() +"; # active connections: " + numConnections +"; # queued calls: " + callQueue.size());
            LOG.SERVER_INFORMATION_CONNECTION_FROM_ACTIVE_CONNECTIONS_QUEUED_CALLS(c.toString(), numConnections, callQueue.size()).tag("org.apache.hadoop.ipc").debug();

            (761: 9) LOG.info(getName() + ": readAndProcess caught InterruptedException", ieo);
            LOG.INTERRUPTED_EXCEPTION_WHILE_READ_AND_PROCESS(ioe).tag("org.apache.hadoop.ipc").info();

            (764: 9) LOG.info(getName() + ": readAndProcess threw exception " + e +" from client " + c.getHostAddress() +". Count of bytes read: " + count, e);
            LOG.READ_AND_PROCESS_THREW_EXCEPTION_FROM_CLIENT(e, c.getHostAddress, "bytes read",count).tag("org.apache.hadoop.ipc").info();

-------------------------
            (771: 11) LOG.debug(getName() + ": disconnecting client " + c + ". Number of active connections: "+numConnections);
            String thread = getName();
            LOG.RPC_SERVER_DISCONNECTING_CLIENT(thread, c, "activeConnections", numConnections).tag("org.apache.hadoop.ipc").debug();

            (791: 11) LOG.info(getName() + ":Exception in closing listener socket. " + e);
            LOG.EXCEPTION_IN_CLOSING_LISTENER_SOCKET(getName(), e).tag("org.apache.hadoop.ipc").info();

            (824: 7) LOG.info(getName() + ": starting");
            LOG.RPC_RESPONDER_STARTING(getName()).tag("org.apache.hadoop.ipc").info();

            (829: 9) LOG.info("Stopping " + this.getName());
            LOG.RPC_RESPONDER_STOPPING(this.getName()).tag("org.apache.hadoop.ipc").info();

            (833: 11) LOG.error("Couldn't close write selector in " + this.getName(), ioe);
            LOG.UNABLE_TO_CLoSE_WRITE_SELECTOR_IN(this.getName(), ioe).tag("org.apache.hadoop.ipc").error();

            (854: 15) LOG.info(getName() + ": doAsyncWrite threw exception " + e);
            LOG.EXCEPTION_DOASYNCWRITE(getName(), e).tag("org.apache.hadoop.ipc").info();

            (867: 13) LOG.debug("Checking for old call responses.");
            LOG.CHECKING_OLD_CALL_RESPONSES().tag("org.apache.hadoop.ipc").debug();

            (888: 15) LOG.warn("Error in purging old calls " + e);
            LOG.ERROR_PURGING_OLD_CALLS(e).tag("org.apache.hadoop.ipc").warn();

            (897: 11) LOG.warn("Out of Memory in server select", e);
            LOG.OUT_OF_MEMORY_IN_SERVER_SELECT(e).tag("org.apache.hadoop.ipc").warn();

            (900: 11) LOG.warn("Exception in Responder", e);
            LOG.EXCEPTION_IN_RESPONDER(e).tag("org.apache.hadoop.ipc").warn();

            (924: 13) LOG.warn("Exception while changing ops : " + e);
            LOG.EXCEPTION_WHILE_CHANGING_OPERATION(e).tag("org.apache.hadoop.ipc").warn();

            (973: 13) LOG.debug(getName() + ": responding to #" + call.callId + " from " + call.connection);
            LOG.RESPONDING_TO_CALLID(getName(), call.callId, call.connction).tag("org.apache.hadoop.ipc").debug();

            (993: 15) LOG.debug(getName() + ": responding to #" + call.callId + " from " + call.connection + " Wrote " + numBytes + " bytes.");
            LOG.RESPONDING_TO_CALLID_WROTE_BYTES(getName(), call.callId, call.connction, numBytes).tag("org.apache.hadoop.ipc").debug();

            (1021: 15) LOG.debug(getName() + ": responding to #" + call.callId + " from " + call.connection + " Wrote partial " + numBytes + " bytes.");
            LOG.RESPONDING_TO_CALLID_WROTE_PARTIAL_BYTES(getName(), call.callId, call.connction, numBytes).tag("org.apache.hadoop.ipc").debug();

            (1030: 11) LOG.warn(getName()+", call " + call + ": output error");
            LOG.CALL_OUTPUT_ERROR(getName(), call).tag("org.apache.hadoop.ipc").warn();

            (1133: 11) LOG.warn("Connection: unable to set socket send buffer size to " +socketSendBufferSize);
            LOG.UNABLE_TO_SET_SOCKET_SEND_BUFFER_SIZE_TO(socketSendBufferSize).tag("org.apache.hadoop.ipc").warn();

            (1204: 13) LOG.debug("Have read input token of size " + saslToken.length+ " for processing by saslServer.evaluateResponse()");
            LOG.FOR_PROCESSING_BY_SASL_SERVER_READ_INPUT_TOKEN_OF_SIZE(saslToken.length).tag("org.apache.hadoop.ipc").debug();

++                        (1222: 16) AUDITLOG.warn(AUTH_FAILED_FOR + clientIP + ":" + attemptingUser);
            LOG.AUTHORIZATION_FAILED_FOR_CLIENT(clientIP, attemptingUser).tag("org.apache.hadoop.ipc").tag("AUDITLOG").warn();

            (1231: 13) LOG.debug("Will send token of size " + replyToken.length+ " from saslServer.");
            LOG.SENDING_FROM_SASL_SERVER_TOKEN_OF_SIZE(replyToken.length).tag("org.apache.hadoop.ipc").debug();

            (1238: 13) LOG.debug("SASL server context established. Negotiated QoP is "+ saslServer.getNegotiatedProperty(Sasl.QOP));
            LOG.SASL_SERVER_CONTEXT_ESTABLISHED_NEGOTIATED_QOP_IS(saslServer.getNegotiatedProperty(Sasl.QOP)).tag("org.apache.hadoop.ipc").debug();

            (1245: 13) LOG.debug("SASL server successfully authenticated client: " + user);
            LOG.SASL_SERVER_SUCCESSFULLY_AUTHENTICATED_CLIENT(user).tag("org.apache.hadoop.ipc").debug();

            (1248: 16) AUDITLOG.info(AUTH_SUCCESSFUL_FOR + user);
            LOG.AUTHORIZATION_SUCCESSFUL_FOR_CLIENT(user).tag("org.apache.hadoop.ipc").tag("AUDITLOG").info();

            (1253: 11) LOG.debug("Have read input token of size " + saslToken.length+ " for processing by saslServer.unwrap()");
            LOG.FOR_PROCESSING_BY_SASL_SERVER_READ_INPUT_TOKEN_OF_SIZE(saslToken.length).tag("org.apache.hadoop.ipc").debug();

            (1327: 13) LOG.warn("Incorrect header or version mismatch from " + hostAddress + ":" + remotePort +" got version " + version + " expected version " + CURRENT_VERSION);
            LOG.INCORRECT_HEADER_OR_VERSION_MISMATCH_FROM_HOST(hostAddress, remotePort, version, "expected version", CURRENT_VERSION).tag("org.apache.hadoop.ipc").warn();

            (1365: 13) LOG.warn("Unexpected data length " + dataLength + "!! from " + getHostAddress());
            LOG.UNEXPECTED_DATA_LENGTH_FROM_HOST(dataLength, getHostAddress).tag("org.apache.hadoop.ipc").warn();

            (1448: 13) LOG.debug("Kerberos principal name is " + fullName);
            LOG.KERBEROS_PRINCIPAL_NAME_IS(fullName).tag("org.apache.hadoop.ipc").debug();

            (1488: 9) LOG.debug("Created SASL server with mechanism = " + mechanism);
            LOG.DEBUG().CREATED_SASL_SERVER_WITH_MECHANISM(mechanism).tag("org.apache.hadoop.ipc");

            (1603: 15) LOG.debug("Received ping message");
            LOG.RECEIVED_PING_MESSAGE().tag("org.apache.hadoop.ipc").debug();

            (1644: 9) LOG.debug(" got #" + header.getCallId());
            LOG.SERVER_PROCESS_DATA_GOT_CALL_ID(header.getCallId()).tag("org.apache.hadoop.ipc").debug();

            (1662: 9) LOG.warn("Unknown rpc kind "  + header.getRpcKind() + " from client " + getHostAddress());
            LOG.UNKNOWN_RPC_KIND_FROM_CLIENT(header.getRpcKind(), getHostAddress).tag("org.apache.hadoop.ipc").warn();

            (1679: 9) LOG.warn("Unable to read call parameters for client " + getHostAddress() + "on connection protocol " +this.protocolName + " for rpcKind " + header.getRpcKind(),  t);
            LOG.UNABLE_TO_READ_CLIENT_CALL_PARAMETERS(getHostAddress(), this.protocolName, header.getRpcKind(), t).tag("org.apache.hadoop.ipc").warn();

            (1711: 11) LOG.debug("Successfully authorized " + connectionContext);
            LOG.SUCCESSFULLY_AUTHORIZED(connectionContext).tag("org.apache.hadoop.ipc").debug();

            (1731: 9) LOG.debug("Ignoring socket shutdown exception", e);
            LOG.IGNORING_SOCKET_SHUTDOWN_EXCEPTION(e).tag("org.apache.hadoop.ipc").debug();

            (1749: 7) LOG.debug(getName() + ": starting");
            LOG.IPC_STARTING(getName()).tag("org.apache.hadoop.ipc").debug();

            (1757: 13) LOG.debug(getName() + ": has Call#" + call.callId + "for RpcKind " + call.rpcKind + " from " + call.connection);
            LOG.IPC_SERVER_INFORMATION(getName(), call.callId, call.rpcKind, call.connection).tag("org.apache.hadoop.ipc").debug();

            (1791: 15) LOG.warn(logMsg, e);
            LOG.SERVER_ERROR(logMsg, e).tag("org.apache.hadoop.ipc").warn();

            (1795: 15) LOG.info(logMsg);
            LOG.SERVER_ERROR(logMsg).tag("org.apache.hadoop.ipc").info();

            (1797: 15) LOG.info(logMsg, e);
            LOG.SERVER_ERROR(logMsg, e).tag("org.apache.hadoop.ipc").info();

            (1820: 15) LOG.warn("Large response size " + buf.size() + " for call "+ call.toString());
            LOG.LARGE_RESPONSE_SIZE_FOR_CALL(buf.size(), call.toString()).tag("org.apache.hadoop.ipc").warn();

            (1828: 13) LOG.info(getName() + " unexpectedly interrupted", e);
            LOG.UNEXPECTED_INTERRUPT(getName(), e).tag("org.apache.hadoop.ipc").info();

            (1831: 11) LOG.info(getName() + " caught an exception", e);
            LOG.CAUGHT_EXCEPTION(getName(), e).tag("org.apache.hadoop.ipc").info();

            (1834: 7) LOG.debug(getName() + ": exiting");
            LOG.EXITING(getName()).tag("org.apache.hadoop.ipc").debug();

            (1954: 7) LOG.debug(AuthenticationMethod.TOKEN +" authentication enabled for secret manager");
            LOG.AUTHENTICATION_ENABLED_FOR_SECRET_MANAGER(AuthenticationToken.TOKEN).tag("org.apache.hadoop.ipc").debug();

            (1959: 5) LOG.debug("Server accepts auth methods:" + authMethods);
            LOG.SERVER_ACCEPTS_AUTH_METHODS(authMethods).tag("org.apache.hadoop.ipc").debug();

            (2002: 9) LOG.warn("Error serializing call response for call " + call, t);
            LOG.ERROR_SERIALIzING_CALL_RESPONSE_FOR_CALL(call, t).tag("org.apache.hadoop.ipc").warn();

            (2066: 9) LOG.debug("Adding saslServer wrapped token of size " + token.length+ " as call response.");
            LOG.TO_CALL_RESPONSE_ADDING_SASL_SERVER_WRAPPED_TOKEN_OF_SIZE(token.length).tag("org.apache.hadoop.ipc").debug();

            (2096: 5) LOG.info("Stopping server on " + port);
            LOG.STOPPING_SERVER_ON_PORT(port).tag("org.apache.hadoop.ipc").info();
        WritableRpcEngine.java  (4 usages)
            (235: 9) LOG.debug("Call: " + method.getName() + " " + callTime);
            LOG.RPC_METHOD_CALL_TIME(method.getName(), callTime).tag("org.apache.hadoop.ipc").debug();

??                        (411: 7) LOG.info(value);
            LOG.LOGGING_MESSAGE(value).tag("org.apache.hadoop.ipc").info();

            (486: 13) LOG.debug("Served: " + call.getMethodName() +" queueTime= " + qTime +" procesingTime= " + processingTime);
            LOG.SERVED_RPC_METHOD_QUEUE_PROCESSING_TIME(call.getMethodName(), qTime, processingTime).tag("org.apache.hadoop.ipc").debug();

            (509: 13) LOG.error("Unexpected throwable object ", e);
            LOG.UNEXPECTED_THROWABLE_OBJECT(e).tag("org.apache.hadoop.ipc").error();

    org.apache.hadoop.ipc.metrics  (2 usages)
        RpcDetailedMetrics.java  (1 usage)
            (47: 5) LOG.debug(registry.info());
            LOG.RPC_REGISTRY_METRICS(registry.info()).tag("org.apache.hadoop.ipc").debug();

        RpcMetrics.java  (1 usage)
            (50: 5) LOG.debug("Initialized "+ registry);
            LOG.INITIALZED_RPC_REGISTRY_METRICS(registry).tag("org.apache.hadoop.ipc").debug();
=======================
27.3.
LOG>ERR
    org.apache.hadoop.jmx  (15 usages)
        JMXJsonServlet.java  (15 usages)
            (215: 7) LOG.error("Caught an exception while processing JMX request", e);
            LOG.EXCEPTION_WHILE_PROCESSING_JMX_REQUEST(e).tag("org.apache.hadoop.jmx").tag("IOException").error();

            (218: 7) LOG.error("Caught an exception while processing JMX request", e);
            LOG.EXCEPTION_WHILE_PROCESSING_JMX_REQUEST(e).tag("org.apache.hadoop.jmx").tag("MalformedObjectNameException").error();

            (227: 5) LOG.debug("Listing beans for "+qry);
            LOG.LISTING_BEANS_FOR(qry).tag("org.apache.hadoop.jmx").debug();

            (254: 11) LOG.error("getting attribute " + prs + " of " + oname + " threw an exception", e);
            LOG.EXCEPTION_GETTING_ATTRIBUTE_OF_OBJECT(e, prs, oname).tag("org.apache.hadoop.jmx").tag("AttributeNotFoundException").error();

            (259: 11) LOG.error("getting attribute " + prs + " of " + oname + " threw an exception", e);
            LOG.EXCEPTION_GETTING_ATTRIBUTE_OF_OBJECT(e, prs, oname).tag("org.apache.hadoop.jmx").tag("MBeanException").error();

            (265: 11) LOG.error("getting attribute " + prs + " of " + oname + " threw an exception", e);
            LOG.EXCEPTION_GETTING_ATTRIBUTE_OF_OBJECT(e, prs, oname).tag("org.apache.hadoop.jmx").tag("RuntimeException").error();

            (271: 11) LOG.error("getting attribute " + prs + " of " + oname + " threw an exception", e);
            LOG.EXCEPTION_GETTING_ATTRIBUTE_OF_OBJECT(e, prs, oname).tag("org.apache.hadoop.jmx").tag("ReflectionException").error();

            (280: 9) LOG.error("Problem while trying to process JMX query: " + qry + " with MBean " + oname, e);
            LOG.PROBLEM_WHILE_TRYING_TO_PROCESS_JMX_QUERY_MBEAN(qry, oname, e).tag("org.apache.hadoop.jmx").error();

            (286: 9) LOG.error("Problem while trying to process JMX query: " + qry + " with MBean " + oname, e);
            LOG.PROBLEM_WHILE_TRYING_TO_PROCESS_JMX_QUERY_MBEAN(qry, oname, e).tag("org.apache.hadoop.jmx").error();

            (338: 9) LOG.debug("getting attribute "+attName+" of "+oname+" threw an exception", e);
            LOG.EXCEPTION_GETTING_ATTRIBUTE_OF_OBJECT(e, attName, oname).tag("org.apache.hadoop.jmx").tag("UnsupportedOperationException").debug();

            (340: 9) LOG.error("getting attribute "+attName+" of "+oname+" threw an exception", e);
            LOG.EXCEPTION_GETTING_ATTRIBUTE_OF_OBJECT(e, attName, oname).tag("org.apache.hadoop.jmx").debug();

            (346: 7) LOG.debug("getting attribute "+attName+" of "+oname+" threw an exception", e);
            LOG.EXCEPTION_GETTING_ATTRIBUTE_OF_OBJECT(e, attName, oname).tag("org.apache.hadoop.jmx").tag("RuntimeErrorException").debug();

            (356: 7) LOG.error("getting attribute "+attName+" of "+oname+" threw an exception", e);
            LOG.EXCEPTION_GETTING_ATTRIBUTE_OF_OBJECT(e, attName, oname).tag("org.apache.hadoop.jmx").tag("MBeanException").debug();

            (361: 7) LOG.error("getting attribute "+attName+" of "+oname+" threw an exception", e);
            LOG.EXCEPTION_GETTING_ATTRIBUTE_OF_OBJECT(e, attName, oname).tag("org.apache.hadoop.jmx").tag("RuntimeException").debug();

            (366: 7) LOG.error("getting attribute "+attName+" of "+oname+" threw an exception", e);
            LOG.EXCEPTION_GETTING_ATTRIBUTE_OF_OBJECT(e, attName, oname).tag("org.apache.hadoop.jmx").tag("ReflectionException").debug();

    org.apache.hadoop.metrics  (2 usages)
        MetricsUtil.java  (2 usages)
            (66: 7) LOG.error("Unable to create metrics context " + contextName, ex);
            LOG.UNABLE_TO_CREATE_METRICS_CONTEXT(contextName, ex).tag("org.apache.hadoop.metrics").error();

            (97: 7) LOG.info("Unable to obtain hostName", ex);
            LOG.UNABLE_TO_OBTAIN_HOSTNAME(ex).tag("org.apache.hadoop.metrics").info();

    org.apache.hadoop.metrics.ganglia  (8 usages)
        GangliaContext.java  (1 usage)
            (154: 9) LOG.warn("Unknown metrics type: " + metric.getClass());
            LOG.UNKNOWN_METRICS_TYPE(metric.getClass()).tag("org.apache.hadoop.metrics").warn();

        GangliaContext31.java  (7 usages)
            (50: 5) LOG.debug("Initializing the GangliaContext31 for Ganglia 3.1 metrics.");
            LOG.INITIALIZING_GANGLIACONTEXT31_FOR_GANGLIA_3_1_METRICS().tag("org.apache.hadoop.metrics").warn();

            (64: 9) LOG.error(uhe);
            LOG.UNKNOWN_HOST_EXCEPTION(uhe).tag("org.apache.hadoop.metrics").error();

            (75: 7) LOG.warn("Metric was emitted with no name.");
            LOG.EMMITED_METRIC_WITH_NO_NAME().tag("org.apache.hadoop.metrics").warn();

            (78: 7) LOG.warn("Metric name " + name +" was emitted with a null value.");
            LOG.EMMITED_METRIC_WITH_NULL_VALUE(name).tag("org.apache.hadoop.metrics").warn();

            (81: 7) LOG.warn("Metric name " + name + ", value " + value + " has no type.");
            LOG.METRIC_HAS_NO_TYPE(name, value).tag("org.apache.hadoop.metrics").warn();

            (85: 5) LOG.debug("Emitting metric " + name + ", type " + type + ", value " + value + " from hostname" + hostName);
            LOG.EMMITING_METRIC_FROM_HOSTNAME(hostname, name, type, value).tag("org.apache.hadoop.metrics").debug();

            (90: 7) LOG.warn("Metric name " + name + ", value " + value + " had 'null' units");
            LOG.METRIC_CONTAINED_NULL_UNITS(name, value).tag("org.apache.hadoop.metrics").warn();

    org.apache.hadoop.metrics.jvm (2 usages)
            (71: 13) log.info("Cannot initialize JVM Metrics with processName=" + processName + ", sessionId=" + sessionId + " - already initialized");
            LOG.UNABLE_TO_INITIALIZE_JVM_METRICS_FOR_PROCESS_ALREADY_INITIALIZED_FOR_SESSION(sessionId, processName).tag("org.apache.hadoop.metrics").info();

            (76: 13) log.info("Initializing JVM Metrics with processName=" + processName + ", sessionId=" + sessionId);
            LOG.INITIALIZING_JVM_METRICS_WITH_PROCESS_SESSION(processName, sessionId).tag("org.apache.hadoop.metrics").info();

    org.apache.hadoop.metrics.spi  (4 usages)
        CompositeContext.java  (4 usages)
            (60: 7) LOG.error("Unable to initialize composite metric " + contextName +": could not init arity", e);
            LOG.UNABLE_TO_INITIALIZE_ARITY_OF_COMPOSITE_METRIC(contextName, e).tag("org.apache.hadoop.metrics").error();

            (94: 9) LOG.warn("emitRecord failed: " + ctxt.getContextName(), e);
            LOG.ERROR_EMIT_RECORD_FAILED(ctxt.getContextName(), e).tag("org.apache.hadoop.metrics").warn();

            (106: 9) LOG.warn("flush failed: " + ctxt.getContextName(), e);
            LOG.ERROR_FLUSH_FAILED(ctxt.getContextname(), e).tag("org.apache.hadoop.metrics").warn();

            (118: 9) LOG.warn("startMonitoring failed: " + ctxt.getContextName(), e);
            LOG.ERROR_STARTMONITORING_FAILED(ctxt.getContextName(), e).tag("org.apache.hadoop.metrics").warn();

    org.apache.hadoop.metrics.util  (7 usages)
        MetricsDynamicMBeanBase.java  (3 usages)
            (115: 21) MetricsUtil.LOG.error("unknown metrics type: " + o.getClass().getName());
            LOG.UNKNOWN_METRICS_TYPE(o.getClass().getName()).tag("org.apache.hadoop.metrics").error();

            (163: 21) MetricsUtil.LOG.error("Unexpected attrubute suffix");
            LOG.UNEXPECTED_ATTRIBUTE_SUFFIX().tag("org.apache.hadoop.metrics").error();

            (167: 21) MetricsUtil.LOG.error("unknown metrics type: " + o.getClass().getName());
            LOG.UNKNOWN_METRICS_TYPE(o.getClass().getName()).tag("org.apache.hadoop.metrics").tag("AttributeNotFoundException").error();

        MetricsIntValue.java  (1 usage)
            (99: 9) LOG.info("pushMetric failed for " + getName() + "\n", e);
            LOG.ERROR_PUSHMETRIC_FAILED_FOR(getName(), e).tag("org.apache.hadoop.metrics").info();

        MetricsTimeVaryingInt.java  (1 usage)
            (108: 7) LOG.info("pushMetric failed for " + getName() + "\n" , e);
            LOG.ERROR_PUSHMETRIC_FAILED_FOR(getName(), e).tag("org.apache.hadoop.metrics").info();

        MetricsTimeVaryingLong.java  (1 usage)
            (104: 7) LOG.info("pushMetric failed for " + getName() + "\n" , e);
            LOG.ERROR_PUSHMETRIC_FAILED_FOR(getName(), e).tag("org.apache.hadoop.metrics").info();

        MetricsTimeVaryingRate.java  (1 usage)
            (152: 7) LOG.info("pushMetric failed for " + getName() + "\n" , e);
            LOG.ERROR_PUSHMETRIC_FAILED_FOR(getName(), e).tag("org.apache.hadoop.metrics").info();

    org.apache.hadoop.metrics2.impl  (63 usages)
        MBeanInfoBuilder.java  (1 usage)
            (109: 23) MetricsSystemImpl.LOG.debug(attrs);
            LOG.MBEAN_ATTRIBUTE_INFO_LIST(attrs).tag("org.apache.hadoop.metrics2").debug();

        MetricsConfig.java  (9 usages)
            (111: 9) LOG.info("loaded properties from "+ fname);
            LOG.PROPERTIES_LOADED_FROM(fname).tag("org.apache.hadoop.metrics2").info();

            (112: 9) LOG.debug(toString(cf));
            LOG.CONFIGURATION_FILE(toString(cf)).tag("org.apache.hadoop.metrics2").debug();

            (114: 9) LOG.debug(mc);
            LOG.METRICS_CONFIG(mc).tag("org.apache.hadoop.metrics2").debug();

            (123: 5) LOG.warn("Cannot locate configuration: tried "+ Joiner.on(",").join(fileNames));
            LOG.UNABLE_TO_LOCATE_CONFIGURATION_TRIED(Joiner.on(",").join(fileNames)).tag("org.apache.hadoop.metrics2").warn();

            (178: 9) LOG.debug("poking parent '"+ getParent().getClass().getSimpleName() + "' for key: "+ key);
???                        String parent = getParent().getClass().getSimpleName();
            LOG.POKING_PARENT_FOR_KEY(parent, key).tag("org.apache.hadoop.metrics2").debug();

            (185: 7) LOG.debug("returning '"+ value +"' for key: "+ key);
            LOG.RETURNING_VALUE_FOR_KEY(value, key).tag("org.apache.hadoop.metrics2").debug();

            (207: 5) LOG.debug(clsName);
            LOG.CLASS_NAME(clsName).tag("org.apache.hadoop.metrics2").debug();

            (226: 11) LOG.debug(jar);
?                        LOG.JAR_PLUGIN_LOADER(jar).tag("org.apache.hadoop.metrics2").debug();

            (233: 9) LOG.debug("using plugin jars: "+ Iterables.toString(jars));
            LOG.USING_PLUGIN_JARS(Iterables.toString(jars)).tag("org.apache.hadoop.metrics2").debug();
