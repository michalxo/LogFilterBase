Targets
    String 'LOG\.(trace|debug|info|warn|error|fatal)'
Found usages  (6,143 usages)
    Production  (3,859 usages)
        Unclassified usage  (3,859 usages)
            hadoop-archives  (3 usages)
                org.apache.hadoop.tools  (3 usages)
                    HadoopArchives.java  (3 usages)
                        (566: 7) LOG.info("Unable to clean tmp directory " + jobDirectory);
                        LOG.UNABLE_TO_DELETE(jobDirectory).tag("org.apache.hadoop.Archives").info();

                        (898: 7) LOG.debug("Exception in archives  ", e);
                        LOG.EXCEPTION(e).tag("org.apache.hadoop.Archives").debug();

            hadoop-auth  (18 usages)
                org.apache.hadoop.security.authentication.client  (5 usages)
                    KerberosAuthenticator.java  (5 usages)
                        (162: 9) LOG.debug("JDK performed authentication on our behalf.");
                        LOG.AUTHENTICATION("PERFORMED_BY_JDK").tag("org.apache.hadoop.KerberosClient").debug();

                        (170: 9) LOG.debug("Performing our own SPNEGO sequence.");
                        LOG.EXECUTED_SPNEGO_SEQUENCE().tag("org.apache.hadoop.KerberosClient").tag("AUTHENTICATION").debug();

                        (173: 9) LOG.debug("Using fallback authenticator sequence.");
                        LOG.USING_FALLBACK_SEQUENCE.tag("org.apache.hadoop.KerberosClient").tag("AUTHENTICATION").debug();

                        (220: 9) LOG.debug("No subject in context, logging in");
                        LOG.LOGGING_IN("NO_SUBJECT").tag("org.apache.hadoop.KerberosClient").debug();

                        (228: 9) LOG.debug("Using subject: " + subject);
                        LOG.LOGGING_IN(subject).tag("org.apache.hadoop.KerberosClient").debug();

                org.apache.hadoop.security.authentication.server  (12 usages)
                    AuthenticationFilter.java  (5 usages)
                        (158: 7) LOG.warn("'signature.secret' configuration not set, using a random value as secret");
                        LOG.FIELD_NOT_SET("signature.secret").tag("org.apache.hadoop.KerberosServer").tag("AUTHENTICATION_CONFIG").warn();

                        (341: 9) LOG.warn("AuthenticationToken ignored: " + ex.getMessage());
                        LOG.TOKEN_IGNORED(ex.getMessage()).tag("org.apache.hadoop.KerberosServer").warn();

                       (347: 13) LOG.debug("Request [{}] triggering authentication ", getRequestURL(httpRequest));
                        LOG.REQUEST_TRIGGERED_AUTHENTICATION(getRequestURL(httpRequest)).tag("org.apache.hadoop.KerberosServer").debug();

                        (359: 13) LOG.debug("Request [{}] user [{}] authenticated", getRequestURL(httpRequest), token.getUserName());
                        LOG.AUTHENTICATED_USER(token.getUserName(), "request", getRequestURL(httpRequest)).tag("org.apache.hadoop.KerberosServer").debug();

                        (391: 7) LOG.warn("Authentication exception: " + ex.getMessage(), ex);
                        LOG.AUTHENTICATION_ERROR(ex.getMessage()).tag("org.apache.hadoop.KerberosServer").warn();

                    KerberosAuthenticationHandler.java  (7 usages)
                        (167: 7) LOG.info("Login using keytab "+keytab+", for principal "+principal);
                        LOG.LOGIN_USING("KEYTAB", keytab, "PRINCIPAL", principal).tag("org.apache.hadoop.KerberosServer").info();

                        (183: 7) LOG.info("Initialized, principal [{}] from keytab [{}]", principal, keytab);
                        LOG.INITIALIZED_PRINCIPAL(principal, "FROM_KEYTAB", keytab).tag("org.apache.hadoop.KerberosServer").info();

                        (202: 7) LOG.warn(ex.getMessage(), ex);
                        LOG.LOGIN_ERROR(ex.getMessage()).tag("org.apache.hadoop.KerberosServer").warn();

                        (280: 9) LOG.trace("SPNEGO starting");
                        LOG.STARTING_SPNEGO().tag("org.apache.hadoop.KerberosServer").trace();

                        (282:9) LOG.warn("'" + KerberosAuthenticator.AUTHORIZATION + "' does not start with '" + KerberosAuthenticator.NEGOTIATE + "' :  {}", authorization);
                        LOG.WRONG_FORMAT(authorization).tag("org.apache.hadoop.KerberosServer").tag("AUTHENTICATION").warn();

                        (307: 17) LOG.trace("SPNEGO in progress");
                        LOG.SPNEGO_IN_PROGRESS().tag("org.apache.hadoop.KerberosServer").tag("AUTHENTICATION").trace();

                        (314: 17) LOG.trace("SPNEGO completed for principal [{}]", clientPrincipal);
                        LOG.SPNEGO_COMPLETED_FOR_PRINCIPAL(clientPrincipal).tag("org.apache.hadoop.KerberosServer").trace();

                org.apache.hadoop.security.authentication.util  (1 usage)
                    KerberosName.java  (1 usage)
                        (87: 9) LOG.debug("Kerberos krb5 configuration not found, setting default realm to empty");
                        LOG.CONFIGURATION_NOT_FOUND("krb5", "EMPTY_REALM").tag("org.apache.hadoop.KerberosServer").tag("AUTHENTICATION").debug();

            hadoop-auth-examples  (2 usages)
                org.apache.hadoop.security.authentication.examples  (2 usages)
                    RequestLoggerFilter.java  (2 usages)
                        (57: 9) LOG.debug(xRequest.getResquestInfo().toString());
                        LOG.HTTP_REQUEST_FILTER_EXAMPLE(xRequest.getResquestInfo().toString()).tag("org.apache.hadoop.KerberosServer").tag("AUTHENTICATION").debug();

                        (61: 9) LOG.debug(xResponse.getResponseInfo().toString());
                        LOG.HTTP_RESPONSE_FILTER_EXAMPLE(xRequest.getResponseInfo().toString()).tag("org.apache.hadoop.KerberosServer").tag("AUTHENTICATION").debug();


            hadoop-common  (672 usages)
                /home/michalxo/IdeaProjects/hadoop-common/hadoop-common-project/hadoop-common  (1 usage)
                    CHANGES.txt  (1 usage)
                        (2827: 58) HADOOP-6884. Add LOG.isDebugEnabled() guard for each LOG.debug(..).
                /home/michalxo/IdeaProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/aop/org/apache/hadoop/fi  (3 usages)
                    ProbabilityModel.java  (3 usages)

                        (61: 5) LOG.info(ALL_PROBABILITIES + "=" + conf.get(ALL_PROBABILITIES));
                        LOG.ALL_PROBABILITIES(conf.get(ALL_PROBABILITIES)).tag("org.apache.hadoop.Common").info();

                        (98: 7) LOG.debug("Request for " + newProbName + " returns=" + ret);
                        LOG.REQUESTED_PROBABILITY_RETURNED(ret).tag("org.apache.hadoop.Common").debug();

                        (102: 7) LOG.info("Probability level is incorrect. Default value is set");
                        LOG.INCORRECT_PROBABILITY_LEVEL("Using default value").tag("org.apache.hadoop.Common").info();

                org.apache.hadoop.conf  (29 usages)
                    Configuration.java  (23 usages)
                        (463: 5) LOG.debug("Handling deprecation for all properties in config...");
                        LOG.HANDLING_DEPRECATIONS().tag("org.apache.haddop.conf").tag("CONFIGURATION").debug();

                        (467: 7) LOG.debug("Handling deprecation for " + (String)item);
                        LOG.HANDLING_DEPRECATION_FOR((String)item).tag("org.apache.hadoop.conf").tag("CONFIGURATION").debug();

                        (479: 7) LOG.warn("DEPRECATED: hadoop-site.xml found in the classpath. "
                                "Usage of hadoop-site.xml is deprecated. Instead use core-site.xml, "
                                + "mapred-site.xml and hdfs-site.xml to override properties of " +
                                "core-default.xml, mapred-default.xml and hdfs-default.xml " +
                                "respectively");
                        LOG.DEPRECATED_FILE("hadoop-site.xml", "Usage of hadoop-site.xml is deprecated. Instead use core-site.xml, .warn();
                                + "mapred-site.xml and hdfs-site.xml to override properties of " +
                                "core-default.xml, mapred-default.xml and hdfs-default.xml " +
                                "respectively").tag("org.apache.hadoop.conf").tag("DEPRECATION");


                        (688: 9) LOG.warn("Unexpected SecurityException in Configuration", se);
                        LOG.UNEXPECTED_SECURITY_EXCEPTION().tag("org.apache.hadoop.conf").tag("CONFIGURATION").warn();


                        (823: 7) LOG.warn(keyInfo.getWarningMessage(name));
                        LOG.DEPRECATED_KEY(keyInfo.getWarningMessage(name)).tag("org.apache.hadoop.conf").tag("DEPRECATION").warn();

                        (1150: 7) LOG.warn("Regular expression '" + valString + "' for property '" + name + "' not valid. Using default", pse);
                        LOG.NOT_VALID_REGULAR_EXPRESSION(valString, "property", name, "using default", pse).tag("org.apache.hadoop.conf").warn();

                        (1719: 5) LOG.warn("Could not make " + path + " in local directories from " + dirsProp);
                        LOG.UNABLE_TO_CREATE_FILE(path, "in directory", dirsProp).tag("org.apache.hadoop.conf").warn();

                        (1723: 7) LOG.warn(dirsProp + "[" + index + "]=" + dirs[index]);
                        LOG.DIRECTORY_ON_INDEX(dirsProp + "[" + index + "]=" + dirs[index]).tag("org.apache.hadoop.conf").warn();

                        (1775: 9) LOG.info(name + " not found");
                        LOG.RESOURCE_NOT_FOUND(name).tag("org.apache.hadoop.conf").info();

                        (1778: 9) LOG.info("found resource " + name + " at " + url);
                        LOG.RESOURCE_FOUND(name, "location", url).tag("org.apache.hadoop.conf").tag("INPUTSTREAM").info();

                        (1799: 9) LOG.info(name + " not found");
                        LOG.RESOURCE_NOT_FOUND(name).tag("org.apache.hadoop.conf").info();

                        (1802: 9) LOG.info("found resource " + name + " at " + url);
                        LOG.RESOURCE_FOUND(name, "location", url).tag("org.apache.hadoop.conf").tag("READER").info();

                        (1870: 7) LOG.info("parsing URL " + url);
                        LOG.PARSING_URL(url).tag("org.apache.hadoop.conf").info();

                        (1881: 7) LOG.info("parsing input stream " + is);
                        LOG.PARSING_INPUT_STREAM(is).tag("org.apache.hadoop.conf").info();

                        (1932: 9) LOG.error("Failed to set setXIncludeAware(true) for parser " + docBuilderFactory + ":" + e,e);
                        LOG.FAILED_TO_SET_DOCBUILDERFACTORY("setXIncludeAware(true)", docBuilderFactory, e).tag("org.apache.hadoop.conf").error();

                        (1954: 13) LOG.info("parsing File " + file);
                        LOG.PARSING_FILE(file).tag("org.apache.hadoop.conf").info();

                        (1982: 9) LOG.fatal("bad conf file: top-level element not <configuration>");
                        LOG.WRONG_FILE_FORMAT("top-level element not <configuration>").tag("org.apache.hadoop.conf").fatal();

                        (1994: 11) LOG.warn("bad conf file: element not <property>");
                        LOG.WRONG_FILE_FORMAT("element not <property>").tag("org.apache.hadoop.conf").warn();

                        (2043: 7) LOG.fatal("error parsing conf " + name, e);
                        (2046: 7) LOG.fatal("error parsing conf " + name, e);
                        (2049: 7) LOG.fatal("error parsing conf " + name, e);
                        (2052: 7) LOG.fatal("error parsing conf " + name, e);
                        LOG.ERROR_PARSING_CONF(name, e).tag("org.apache.hadoop.conf").fatal();

                        (2070: 9) LOG.warn(name+":an attempt to override final parameter: "+attr+";  Ignoring.");
                        LOG.IGNORING_PARAMETER(name, attr).tag("org.apache.hadoop.conf").warn();

                    ReconfigurableBase.java  (1 usage)
                        (65: 7) LOG.info("changing property " + property + " to " + newVal);
                        LOG.CHANGING_PROPERTY(property, newVal).tag("org.apache.hadoop.conf").info();

                    ReconfigurationServlet.java  (5 usages)
                        (64: 5) LOG.info("servlet path: " + req.getServletPath());
                        LOG.SERVLET_PATH(req.getServletPath()).tag("org.apache.hadoop.conf").info();

                        (65: 5) LOG.info("getting attribute: " + CONF_SERVLET_RECONFIGURABLE_PREFIX + req.getServletPath());
                        LOG.GETTING_ATTRIBUTE(CONF_SERVLET_RECONFIGURABLE_PREFIX + req.getServletPath()).tag("org.apache.hadoop.conf").info();

                        (185: 15) LOG.info("property " + param + " unchanged");
                        LOG.PROPERTY_PARAMETER_UNCHANGED(param).tag("org.apache.hadoop.conf").info();

                        (203: 5) LOG.info("GET");
                        LOG.GET().tag("org.apache.hadoop.conf").info();

                        (217: 5) LOG.info("POST");
                        LOG.POST().tag("org.apache.hadoop.conf").info();

                org.apache.hadoop.fs  (40 usages)
                    ChecksumFileSystem.java  (1 usage)
                        (158: 9) LOG.warn("Problem opening checksum file: "+ file + ".  Ignoring exception: " , e);
                        LOG.ERROR_OPENING.CHECKSUM_FILE(file, "ignoring", e).tag("org.apache.hadoop.fs").warn();

                    ChecksumFs.java  (1 usage)
                        (145: 9) LOG.warn("Problem opening checksum file: "+ file +
                        LOG.ERROR_OPENING.CHECKSUM_FILE(file, "ignoring", e).tag("org.apache.hadoop.fs").warn();

                    DelegationTokenRenewer.java  (3 usages)
                        (196: 11) LOG.error("Interrupted while canceling token for " + fs.getUri() + "filesystem");
                        LOG.INTERRUPTED_WHILE_CANCELING_TOKEN(fs.getUri()).tag("org.apache.hadoop.fs").error();

                        (199: 13) LOG.debug(ie.getStackTrace());
                        LOG.STACKTRACE(ie.getStackTrace()).tag("org.apache.hadoop.fs").debug();

                        (224: 29) action.weakFs.get().LOG.warn("Failed to renew token, action=" + action, ie);
                        LOG.FAILED_TO_RENEW_TOKEN(action, ie).tag("org.apache.hadoop.fs").warn();

                    DU.java  (1 usage)
                        (96: 13) LOG.warn("Could not get disk usage information", e);
                        LOG.ERROR_GETTING_DISK_USAGE_INFORMATION(e).tag("org.apache.hadoop.fs").warn();

                    FileContext.java  (4 usages)
                        (237: 7) LOG.error("Exception in getCurrentUser: ",e);
                        LOG.ERROR_GETTING_CURRENT_USER(e).tag("org.apache.hadoop.fs").error();

                        (288: 13) LOG.warn("Ignoring failure to deleteOnExit for path " + path);
                        LOG.IGNORING_FAILURE_DELETEONEXIT(path).tag("org.apache.hadoop.fs").warn();

                        (346: 7) LOG.error(ex);
                        LOG.EXCEPTION(ex).tag("org.apache.hadoop.fs").tag("AbstractFileSystem").error();

                        (457: 7) LOG.error(ex);
                        LOG.EXCEPTION(ex).tag("org.apache.hadoop.fs").tag("FileContext").error();

                    FileSystem.java  (4 usages)
                        (270: 7) LOG.warn("\"local\" is a deprecated filesystem name." +" Use \"file:///\" instead.");
                        LOG.DEPRECATED_FILESYSTEM_NAME("local", "use file:///").tag("org.apache.hadoop.fs").warn();

                        (274: 7) LOG.warn("\""+name+"\" is a deprecated filesystem name."+" Use \"hdfs://"+name+"/\" instead.");
                        LOG.DEPRECATED_FILESYSTEM_NAME("use hdfs://", name, "/").tag("org.apache.hadoop.fs").warn();

                        (1333: 11) LOG.info("Ignoring failure to deleteOnExit for path " + path);
                        LOG.IGNORING_FAILURE_DELETEONEXIT(path).tag("org.apache.hadoop.fs").info();

                        (2385: 11) LOG.info("FileSystem.Cache.closeAll() threw an exception:\n" + e);
                        LOG.ERROR_FILESYSTEM_CACHE_CLOSEALL(e).tag("org.apache.hadoop.fs").info();

                    FileUtil.java  (3 usages)
                        (140: 7) LOG.warn("null file argument.");
                        LOG.NULL_FILE_ARGUMENT().tag("org.apache.hadoop.fs").warn();

                        (149: 7) LOG.warn("Failed to delete file or dir ["+ f.getAbsolutePath() + "]: it still exists.");
                        LOG.FAILED_TO_DELETE(f.getAbsolutePath(), "still exists").tag("org.apache.hadoop.fs").warn();

                        (801: 9) LOG.debug("Error while changing permission : " + filename + " Exception: ", e);
                        LOG.ERROR_CHANGING_PERMISSION(filename, e).tag("org.apache.hadoop.fs").debug();

                    FSInputChecker.java  (1 usage)
                        (284: 11) LOG.info("Found checksum error: b[" + off + ", " + (off+read) + "]=" + StringUtils.byteToHexString(b, off, off + read), ce);
                        LOG.CHECKSUM_ERROR(ce).tag("org.apache.hadoop.fs").info();

                    FsShell.java  (1 usage)
                        (263: 9) LOG.debug("Error", e);
                        LOG.ERROR(e).tag("org.apache.hadoop.fs").tag("FsShell.run()").debug();

                    FsShellPermissions.java  (2 usages)
                        (105: 11) LOG.debug("Error changing permissions of " + item, e);
                        LOG.ERROR_CHANGING_PERMISSION(item, e).tag("org.apache.hadoop.fs").debug();

                        (185: 11) LOG.debug("Error changing ownership of " + item, e);
                        LOG.ERROR_CHANGING_OWNERSHIP(item, e).tag("org.apache.hadoop.fs").debug();

                    LocalDirAllocator.java  (4 usages)
                        (293: 17) LOG.warn( localDirs[i] + " is not writable\n", de);
                        LOG.DIRECTORY_NOT_WRITABLE(localDirs[i], de).tag("org.apache.hadoop.fs").warn();

                        (296: 15) LOG.warn( "Failed to create " + localDirs[i]);
                        LOG.FAILED_TO_CREATE_DIRECTORY(localDirs[i]).tag("org.apache.hadoop.fs").warn();

                        (299: 13) LOG.warn( "Failed to create " + localDirs[i] + ": " + ie.getMessage() + "\n", ie);
                        LOG.FAILED_TO_CREATE_DIRECTORY(localDirs[i], ie.getMessage()).tag("org.apache.hadoop.fs").warn();

                        (323: 11) LOG.warn("Disk Error Exception: ", d);
                        LOG.DISK_ERROR_EXCEPTION(d).tag("org.apache.hadoop.fs").warn();

                    LocalFileSystem.java  (4 usages)
                        (125: 7) LOG.warn("Moving bad file " + f + " to " + badFile);
                        LOG.MOVING_BAD_FILE(f, "to", badFile).tag("org.apache.hadoop.fs").warn();

                        (129: 9) LOG.warn("Ignoring failure of renameTo");
                        LOG.IGNORING_FAILURE_RENAMETO().tag("org.apache.hadoop.fs").("reportChecksumFailure").warn();

                        (135: 11) LOG.warn("Ignoring failure of renameTo");
                        LOG.IGNORING_FAILURE_RENAMETO().tag("org.apache.hadoop.fs").("reportChecksumFailure").warn();

                        (138: 7) LOG.warn("Error moving bad file " + p + ": " + e);
                        LOG.ERROR_MOVING_BAD_FILE(p, e).tag("org.apache.hadoop.fs").warn();

                    TrashPolicyDefault.java  (11 usages)
                        (134: 11) LOG.warn("Can't create(mkdir) trash directory: "+baseTrashPath);
                        LOG.ERROR_CREATING_TRASH_DIRECTORY("mkdir", baseTrashPath).tag("org.apache.hadoop.fs").warn();

                        (138: 9) LOG.warn("Can't create trash directory: "+baseTrashPath);
                        LOG.ERROR_CREATING_TRASH_DIRECTORY(baseTrashPath).tag("org.apache.hadoop.fs").warn();

                        (186: 5) LOG.info("Created trash checkpoint: "+checkpoint.toUri().getPath());
                        LOG.CREATED_TRASH_CHECKPOINT(checkpoint.toUri().getPath()).tag("org.apache.hadoop.fs").info();

                        (211: 9) LOG.warn("Unexpected item in trash: "+dir+". Ignoring.");
                        LOG.UNEXPECTED_ITEM_IN_TRASH_DIRECTORY_IGNORING(dir).tag("org.apache.hadoop.fs").warn();

                        (217: 11) LOG.info("Deleted trash checkpoint: "+dir);
                        LOG.DELETED_TRASH_CHECKPOINT(dir).tag("org.apache.hadoop.fs").info();

                        (219: 11) LOG.warn("Couldn't delete checkpoint: "+dir+" Ignoring.");
                        LOG.UNABLE_TO_DELETE_CHECKPOINT_IGNORING(dir).tag("org.apache.hadoop.fs").warn();

                        (244: 9) LOG.info("The configured checkpoint interval is " + (emptierInterval / MSECS_PER_MINUTE) + " minutes." +
                 " Using an interval of " + (deletionInterval / MSECS_PER_MINUTE) + " minutes that is used for deletion instead");
                        LOG.USING_DELETION_CHECKPOINT_INTERVAL_INSTEAD_CONFIGURED("configured", (emptierInterval / MSECS_PER_MINUTE).info();
                            "using", (deletionInterval / MSECS_PER_MINUTE))..tag("org.apache.hadoop.fs");

                        (275: 15) LOG.warn("Trash can't list homes: "+e+" Sleeping.");
                        LOG.TRASH_CANT_LIST_HOME_DIRECTORIES_SLEEPING(e).tag("org.apache.hadoop.fs").warn();

                        (288: 17) LOG.warn("Trash caught: "+e+". Skipping "+home.getPath()+".");
                        LOG.TRASH_EXCEPTION_SKIPPING_PATH(home.getPath(), "exception", e).tag("org.apache.hadoop.fs").warn();

                        (293: 11) LOG.warn("RuntimeException during Trash.Emptier.run(): ", e);
                        LOG.RUNTIME_EXCEPTION_DURING_TRASH_EMPTIER_RUN(e).tag("org.apache.hadoop.fs").tag("Trash.Emptier.run()").warn();

                        (299: 9) LOG.warn("Trash cannot close FileSystem: ", e);
                        LOG.TRASH_CANNON_CLOSE_FILESYSTEM(e).tag("org.apache.hadoop.fs").warn();

                org.apache.hadoop.fs.ftp  (1 usage)
                    FTPFileSystem.java  (1 usage)
                        (154: 9) LOG.warn("Logout failed while disconnecting, error code - "+ client.getReplyCode());
                        LOG.LOGOUT_FAILED_WHILE_DISCONNECTING(client.getReplyCode()).tag("org.apache.hadoop.fs.ftp").warn();

                org.apache.hadoop.fs.permission  (2 usages)
                    FsPermission.java  (2 usages)
                        String error = "Unable to parse configuration " + UMASK_LABEL+ " with value " + confUmask + " as " + type + " umask.";
                        (245: 9) LOG.warn(error);
                        LOG.ERROR_PARSING_CONFIGURATION(UMASK_LABEL, "value", confUmask, type).tag("org.apache.hadoop.fs.permission").warn();

                        (255: 11) LOG.warn(DEPRECATED_UMASK_LABEL + " configuration key is deprecated. " + "Convert to "
                            + UMASK_LABEL + ", using octal or symbolic umask " + "specifications.");
                        LOG.DEPRECATED_UMASK_LABEL_CONFIGURATION_KEY(DEPRECATED_UMASK_LABEL, UMASK_LABEL.warn();
                            "convert using octal or symbolic umask specifications").tag("org.apache.hadoop.fs.permission");

                org.apache.hadoop.fs.s3  (4 usages)
                    Jets3tFileSystemStore.java  (1 usage)
                        (240: 11) LOG.warn("Ignoring failed delete");
                    S3InputStream.java  (1 usage)
                        (190: 9) LOG.warn("Ignoring failed delete");
                    S3OutputStream.java  (2 usages)
                        (190: 7) LOG.warn("Ignoring failed delete");
                        (227: 7) LOG.warn("Ignoring failed delete");
                        LOG.FAILED_TO_DELETE_FILE_IGNORING().tag("org.apache.hadoop.fs.s3").warn();

                org.apache.hadoop.fs.s3native  (30 usages)
                    NativeS3FileSystem.java  (30 usages)
                        (112: 9) LOG.info("Received IOException while reading '" + key + "', attempting to reopen.");
                        LOG.ERROR_IOEXCEPTION_WHILE_READING(key, "reopening").tag("org.apache.hadoop.fs.s3native").info();

                        (132: 9) LOG.info("Received IOException while reading '" + key + "', attempting to reopen.");
                        LOG.ERROR_IOEXCEPTION_WHILE_READING(key, "reopening").tag("org.apache.hadoop.fs.s3native").info();

                        (153: 7) LOG.info("Opening key '" + key + "' for reading at position '" + pos + "'");
                        LOG.OPENING_KEY_FOR_READING(key, "position", pos).tag("org.apache.hadoop.fs.s3native").info();

                        (182: 7) LOG.info("OutputStream for key '" + key + "' writing to tempfile '" + this.backupFile + "'");
                        LOG.WRITING_TO_TEMPFILE_OUTPUTSTREAM_KEY(key, this.backupFile).tag("org.apache.hadoop.fs.s3native").info();

                        (188: 9) LOG.warn("Cannot load MD5 digest algorithm," +"skipping message integrity check.", e);
                        LOG.UNABLE_TO_LOAD_MD5_DIGEST_ALGORITHM_SKIPPING(e).tag("org.apache.hadoop.fs.s3native").warn();

                        (217: 7) LOG.info("OutputStream for key '" + key + "' closed. Now beginning upload");
                        LOG.UPLOADING_CLOSED_OUTPUTSTREAM_FOR_KEY(key).tag("org.apache.hadoop.fs.s3native").info();

                        (224: 11) LOG.warn("Could not delete temporary s3n file: " + backupFile);
                        LOG.UNABLE_TO_DELETE_S3N_TEMPORARY_FILE(backupFile).tag("org.apache.hadoop.fs.s3native").warn();

                        (229: 7) LOG.info("OutputStream for key '" + key + "' upload complete");
                        LOG.UPLOAD_FINISHED_FOR_OUTPUTSTREAM_KEY(key).tag("org.apache.hadoop.fs.s3native").info();

                        (346: 7) LOG.debug("Creating new file '" + f + "' in S3");
                        LOG.CREATING_NEW_S3_FILE(f).tag("org.apache.hadoop.fs.s3native").debug();

                        (361: 9) LOG.debug("Delete called for '" + f +"' but file does not exist, so returning false");
                        LOG.DELETE_NOT_EXISTING_FILE_RETURNING_FALSE(f).tag("org.apache.hadoop.fs.s3native").tag("FILE_DOES_NOT_EXIST").debug();

                        (376: 9) LOG.debug("Deleting directory '" + f  + "'");
                        LOG.DELETING_DIRECTORY(f).tag("org.apache.hadoop.fs.s3native").debug();

                        (394: 9) LOG.debug("Deleting file '" + f + "'");
                        LOG.DELETING_FILE(f).tag("org.apache.hadoop.fs.s3native").debug();

                        (412: 7) LOG.debug("getFileStatus retrieving metadata for key '" + key + "'");
                        LOG.RETRIEVING_METADATA_FOR_KEY(key).tag("org.apache.hadoop.fs.s3native").tag("getFileStatus()").debug();

                        (417: 9) LOG.debug("getFileStatus returning 'file' for key '" + key + "'");
                        LOG.RETURNING_FILE_FOR_KEY(key).tag("org.apache.hadoop.fs.s3native").tag("getFileStatus()").debug();

                        (423: 9) LOG.debug("getFileStatus returning 'directory' for key '" + key +"' as '" + key + FOLDER_SUFFIX + "' exists");
!                        String folder = key + FOLDER_SUFFIX;
!                        LOG.RETURNING_DIRECTORY_FOR_KEY_EXISTS(key, "returning as", key + folder ).tag("org.apache.hadoop.fs.s3native").tag("getFileStatus()").debug();

                        (430: 7) LOG.debug("getFileStatus listing key '" + key + "'");
                        LOG.LISTING_KEY(key).tag("org.apache.hadoop.fs.s3native").tag("getFileStatus()").debug();

                        (436: 9) LOG.debug("getFileStatus returning 'directory' for key '" + key + "' as it has contents");
                        LOG.RETURNING_DIRECTORY_WITH_CONTENT_FOR_KEY(key).tag("org.apache.hadoop.fs.s3native").tag("getFileStatus()").debug();

                        (443: 7) LOG.debug("getFileStatus could not find key '" + key + "'");
                        LOG.UNABLE_TO_FIND_KEY(key).tag("org.apache.hadoop.fs.s3native").tag("getFileStatus()").debug();

                        (547: 9) LOG.debug("Making dir '" + f + "' in S3");
                        LOG.CREATING_DIR_IN_S3(f).tag("org.apache.hadoop.fs.s3native").debug();

                        (561: 5) LOG.info("Opening '" + f + "' for reading");
                        LOG.OPENING_FILE_FOR_READING(f).tag("org.apache.hadoop.fs.s3native").info();

!                    final String debugPreamble = "Renaming '" + src + "' to '" + dst + "' - ";
                        (599: 11) LOG.debug(debugPreamble + "returning false as dst is an already existing file");
                        LOG.DESTINATION_ALREADY_EXISTS(src, dst).tag("org.apache.hadoop.fs.s3native"),tag("RENAMING_FILE").debug();

                        (605: 11) LOG.debug(debugPreamble + "using dst as output directory");
                        LOG.USING_DESTINATION_AS_OUTPUT_DIRECTORY(src, dst).tag("org.apache.hadoop.fs.s3native").tag("RENAMING_FILE").debug();

                        (611: 9) LOG.debug(debugPreamble + "using dst as output destination");
                        LOG.USING_DESTINATION_AS_OUTPUT_FILE(src, dst).tag("org.apache.hadoop.fs.s3native").tag("RENAMING_FILE").debug();

                        (617: 13) LOG.debug(debugPreamble + "returning false as dst parent exists and is a file");
                        LOG.DESTINATION_PARENT_FILE_ALREADY_EXISTS(src, dst).tag("org.apache.hadoop.fs.s3native").tag("RENAMING_FILE").debug();

                        (624: 11) LOG.debug(debugPreamble + "returning false as dst parent does not exist");
                        LOG.DESTINATION_PARENT_DOES_NOT_EXIST(src, dst).tag("org.apache.hadoop.fs.s3native").tag("RENAMING_FILE").debug();

                        (636: 9) LOG.debug(debugPreamble + "returning false as src does not exist");
                        LOG.SOURCE_FILE_DOES_NOT_EXIST(src, dst).tag("org.apache.hadoop.fs.s3native").tag("RENAMING_FILE").debug();

                        (642: 9) LOG.debug(debugPreamble + "src is file, so doing copy then delete in S3");
                        LOG.STORING_COPY_OF_S3_FILE(src, dst).tag("org.apache.hadoop.fs.s3native").tag("RENAMING_FILE").debug();

                        (649: 9) LOG.debug(debugPreamble + "src is directory, so copying contents");
                        LOG.COPYING_SOURCE_CONTENT(src, dst).tag("org.apache.hadoop.fs.s3native").tag("RENAMING_FILE").debug();

                        (665: 9) LOG.debug(debugPreamble + "all files in src copied, now removing src files");
                        LOG.COPY_SUCCESSFUL_REMOVING_SOURCE_FILES(src, dst).tag("org.apache.hadoop.fs.s3native").tag("RENAMING_FILE").debug();

                        (678: 9) LOG.debug(debugPreamble + "done");
                        LOG.RENAMING_FILE_COMPLETED(src, dst).tag("org.apache.hadoop.fs.s3native").tag("RENAMING_FILE").debug();

                org.apache.hadoop.fs.shell  (1 usage)
                    Command.java  (1 usage)
                        (377: 7) LOG.debug(errorMessage);
                        LOG.INTERNAL_ERROR(errorMessage).tag("org.apache.hadoop.fs.shell").debug();

                org.apache.hadoop.ha  (117 usages)
                    ActiveStandbyElector.java  (40 usages)
                        (253: 5) LOG.debug("Attempting active election for " + this);
                        LOG.ATTEMPTING_ACTIVE_ELECTION_FOR(this).tag("org.apache.hadoop.ha").debug();

                        (289: 7) LOG.debug("Ensuring existence of " + prefixPath);
                        LOG.ENSURING_EXISTENCE_OF(prefixPath).tag("org.apache.hadoop.ha").debug();

                        (302: 5) LOG.info("Successfully created " + znodeWorkingDir + " in ZK.");
                        LOG.SUCCESFULLY_CREATED_ZNODE_WORKING_DIRECTORY(znodeWorkingDir).tag("org.apache.hadoop.ha").tag("ZOOKEEPER").info();

                        (317: 7) LOG.info("Recursively deleting " + znodeWorkingDir + " from ZK...");
                        LOG.RECURSIVELY_DELETING_ZNODE_WORKING_DIRECTORY(znodeWorkingDir).tag("org.apache.hadoop.ha").tag("ZOOKEEPER").info();

                        (330: 5) LOG.info("Successfully deleted " + znodeWorkingDir + " from ZK.");
                        LOG.SUCCESFULLY_DELETED_ZNODE(znodeWorkingDir).tag("org.apache.hadoop.ha").tag("ZOOKEEPER").info();

                        (348: 5) LOG.info("Yielding from election");
                        LOG.YIELDING_FROM_ELECTION().tag("org.apache.hadoop.ha").tag("quitElection()").info();

                        (403: 5) LOG.debug("CreateNode result: " + rc + " for path: " + path + " connectionState: " + zkConnectionState +"  for " + this);
                        LOG.CREATENODE_RESULT_FOR_PATH(rc, path, "connectionState", zkConnectionState, this).tag("org.apache.hadoop.ha").debug();

                        String errorMessage = "Received create error from Zookeeper. code:" + code.toString() + " for path " + path;
                        (434: 5) LOG.debug(errorMessage);
                        LOG.CREATE_ERROR_CODE_FOR_PATH(code.toString, path).tag("org.apache.hadoop.ha").tag("ZOOKEEPER").debug();

                        (438: 9) LOG.debug("Retrying createNode createRetryCount: " + createRetryCount);
                        LOG.RETRYING_CREATE_NODE_RETRY_COUNT(createRetryCount).tag("org.apache.hadoop.ha").debug();

                        (447: 7) LOG.warn("Lock acquisition failed because session was lost");
                        LOG.LOCK_ACQUISITION_FAILED_SESSION_LOST().tag("org.apache.hadoop.ha").warn();

                        (465: 5) LOG.debug("StatNode result: " + rc + " for path: " + path+ " connectionState: " + zkConnectionState + " for " + this);
                        LOG.STATNODE_RESULT_FOR_PATH(rc, path, "connectionState", zkConnectionState, this).tag("org.apache.hadoop.ha").debug();

                        String errorMessage = "Received stat error from Zookeeper. code:" + code.toString();
                        (495: 5) LOG.debug(errorMessage);
                        LOG.STAT_ERROR_CODE(code.toString()).tag("org.apache.hadoop.ha").tag("ZOOKEEPER").debug();

                        (507: 7) LOG.warn("Lock monitoring failed because session was lost");
                        LOG.SESSION_LOST_LOCK_MONITORING_FAILED().tag("org.apache.hadoop.ha").warn();

                        (532: 5) LOG.debug("Watcher event type: " + eventType + " with state:" + event.getState() + " for path:" + event.getPath()
                            + " connectionState: " + zkConnectionState + " for " + this);
                        LOG.WATCHER_EVENT_TYPE_STATE_PATH(eventType, event.getState(), event.getPath(), "connectionState", zkConnectionState, path).tag("org.apache.hadoop.ha").debug();

                        (541: 9) LOG.info("Session connected.");
                        LOG.SESSION_CONNECTED().tag("org.apache.hadoop.ha").tag("processWatchEvent()").info();

                        (552: 9) LOG.info("Session disconnected. Entering neutral mode...");
                        LOG.SESSION_DISCONNECTED_ENTERING_NEUTRAL_MODE().tag("org.apache.hadoop.ha").tag("processWatchEvent()").info();

                        (562: 9) LOG.info("Session expired. Entering neutral mode and rejoining...");
                        LOG.SESSION_EXPIRED_ENTERING_NEUTRAL_MODE_AND_REJOINING().tag("org.apache.hadoop.ha").tag("processWatchEvent()").info();

                        (591: 9) LOG.debug("Unexpected node event: " + eventType + " for path: " + path);
                        LOG.UNEXPECTED_NODE_EVENT_FOR_PATH(eventType, path).tag("org.apache.hadoop.ha").debug();

                        (633: 5) LOG.fatal(errorMessage);
                        LOG.FATAL_ERROR(errorMessage).tag("org.apache.hadoop.ha").fatal();

                        (640: 5) LOG.debug("Monitoring active leader for " + this);
                        LOG.MONITORING_ACTIVE_LEADER(this).tag("org.apache.hadoop.ha").debug();

                        (659: 5) LOG.info("Trying to re-establish ZK session");
                        LOG.REESTABLISHING_ZOOKEEPER_SESSION().tag("org.apache.hadoop.ha").info();

                        (723: 7) LOG.debug("Establishing zookeeper connection for " + this);
                        LOG.ESTABLISHING_ZOOKEEPER_CONNECTION(this).tag("org.apache.hadoop.ha").debug();

                        (728: 9) LOG.warn(e);
                        LOG.IO_EXCEPTION(e).tag("org.apache.hadoop.ha").warn();

                        (731: 9) LOG.warn(e);
                        LOG.KEEPER_EXCEPTION(e).tag("org.apache.hadoop.ha").warn();

                        (750: 5) LOG.debug("Created new connection for " + this);
                        LOG.CREATED_NEW_CONNECTION(this).tag("org.apache.hadoop.ha").debug();

                        (757: 5) LOG.debug("Terminating ZK connection for " + this);
                        LOG.TERMINATING_CONNECTION(this).tag("org.apache.hadoop.ha").tag("ZOOKEEPER").debug();

                        (763: 7) LOG.warn(e);
                        LOG.INTERRUPTED_EXCEPTION(e).tag("org.apache.hadoop.ha").warn();

                        (783: 7) LOG.debug("Becoming active for " + this);
                        LOG.BECOMING_ACTIVE_FOR(this).tag("org.apache.hadoop.ha").debug();

                        (788: 7) LOG.warn("Exception handling the winning of election", e);
                        LOG.HANDLING_WINNING_OF_ELECTION_EXCEPTION(e).tag("org.apache.hadoop.ha").warn();

                        (801: 5) LOG.info("Writing znode " + zkBreadCrumbPath + " to indicate that the local node is the most recent active...");
                        LOG.ZNODE_TO_INDICATE_LOCAL_MOST_RECENT_ACTIVE_NODE(zkBreadCrumbPath).tag("org.apache.hadoop.ha").info();

                        (821: 5) LOG.info("Deleting bread-crumb of active node...");
                        LOG.DELETING_ACTIVE_NODE().tag("org.apache.hadoop.ha").info();

                        (839: 7) LOG.warn("Unable to delete our own bread-crumb of being active at " + zkBreadCrumbPath + ": " + e.getLocalizedMessage()
                            + ". " +"Expecting to be fenced by the next active.");
                        LOG.UNABLE_TO_DELETE_ACTIVE_NODE(zkBreadCrumbPath, e.getLocalizedMessage(), "expecting to be fenced by the next active bread-crumb").tag("org.apache.hadoop.ha").warn();

                        (854: 5) LOG.info("Checking for any old active which needs to be fenced...");
                        LOG.CHECKING_OLD_ACTIVE_NODE_TO_BE_FENCED().tag("org.apache.hadoop.ha").info();

                        (864: 9) LOG.info("No old node to fence");
                        LOG.NO_OLD_NODE_TO_FENCE().tag("org.apache.hadoop.ha").info();

                        (875: 5) LOG.info("Old node exists: " + StringUtils.byteToHexString(data));
                        LOG.OLD_NODE_EXISTS(StringUtils.byteToHexString(data)).tag("org.apache.hadoop.ha").info();

                        (877: 7) LOG.info("But old node has our own data, so don't need to fence it.");
                        LOG.OLD_NODE_WITH_OWN_DATA_NOT_FENCING().tag("org.apache.hadoop.ha").info();

                        (886: 7) LOG.debug("Becoming standby for " + this);
                        LOG.BECOMING_STANDBY(this).tag("org.apache.hadoop.ha").debug();

                        (894: 7) LOG.debug("Entering neutral mode for " + this);
                        LOG.ENTERING_NEUTRAL_MODE(this).tag("org.apache.hadoop.ha").debug();

                        (983: 7) LOG.warn("Ignoring stale result from old client with sessionId " + String.format("0x%08x", ((ZooKeeper)ctx).getSessionId()));
                        LOG.IGNORING_RESULT_FROM_OLD_CLIENT_WITH_SESSIONID(String.format("0x%08x", ((ZooKeeper)ctx).getSessionId())).tag("org.apache.hadoop.ha").warn();

                        (1028: 11) LOG.error("Connection timed out: couldn't connect to ZooKeeper in " + connectionTimeoutMs + " milliseconds");
                        LOG.UNABLE_TO_CONNECT_ZOOKEEPER_TIMED_OUT_MS(connectionTimeoutMs).tag("org.apache.hadoop.ha").error();
-----
19.3.
                    FailoverController.java  (7 usages)
                    String msg = "Unable to get service state for " + target;
                        (126: 7) LOG.error(msg + ": " + e.getLocalizedMessage());
                        LOG.UNABLE_TO_GET_SERVICE_STATE(target).tag("org.apache.hadoop.ha").error();

                        (142: 9) LOG.warn("Service is not ready to become active, but forcing: " +notReadyReason);
                        LOG.SERVICE_NOT_READY_TO_BECOME_ACTIVE_FORCING(notReadyReason).tag("org.apache.hadoop.ha").warn();

                        (175: 7) LOG.warn("Unable to gracefully make " + svc + " standby (" +sfe.getMessage() + ")");
                        LOG.UNABLE_TO_PERFORM_MAKE_STANDBY(svc, sfe.getMessage()).tag("org.apache.hadoop.ha").warn();

                        (178: 7) LOG.warn("Unable to gracefully make " + svc +" standby (unable to connect)", ioe);
                        LOG.UNABLE_TO_PERFORM_MAKE_STANDBY(svc, ioe).tag("org.apache.hadoop.ha").tag("IO_EXCEPTION").warn();

                        (230: 7) LOG.error("Unable to make " + toSvc + " active (" + sfe.getMessage() + "). Failing back.");
                        LOG.UNABLE_TO_ACTIVATE_FAILING(toSvc, sfe.getMessage()).tag("org.apache.hadoop.ha").error();

                        (235: 7) LOG.error("Unable to make " + toSvc + " active (unable to connect). Failing back.", ioe);
                        LOG.UNABLE_TO_MAKE_ACTIVE_FAILING(toSvc, ioe).tag("org.apache.hadoop.ha").tag("IO_EXCEPTION").error();

                        msg += ". Failback to " + fromSvc + " failed (" + ffe.getMessage() + ")";
                        (255: 11) LOG.fatal(msg);
                        LOG.FAILBACK_FAILED_FROM(fromSvc, ffe.getMessage()).tag("org.apache.hadoop.ha").fatal();

                    HAAdmin.java  (3 usages)
                        (176: 9) LOG.warn("Proceeding with manual HA state management even though\n" + "automatic failover is enabled for " + target);
                        LOG.USING_MANUAL_HA_STATE_MANAGEMENT_AUTOMATIC_FAILOVER_ENABLED(target).tag("org.apache.hadoop.ha").warn();

                        (327: 9) LOG.debug("Operation failed", ioe);
                        LOG.OPERATION_FAILED(ioe).tag("org.apache.hadoop.ha").tag("run()").debug();

                        (375: 9) LOG.fatal("Aborted");
                        LOG.ABORTED().tag("org.apache.hadoop.ha").tag("runCmd()").fatal();

                    HealthMonitor.java  (6 usages)
                        (139: 5) LOG.info("Stopping HealthMonitor thread");
                        LOG.STOPPING_HEALTH_MONITOR_THREAD().tag("org.apache.hadoop.ha").info();

                        (171: 7) LOG.warn("Could not connect to local service at " + targetToMonitor + ": " + e.getMessage());
                        LOG.UNABLE_TO_CONNECT_LOCAL_SERVICE(targetToMonitor, e.getMessage()).tag("org.apache.hadoop.ha").warn();

                        (194: 9) LOG.warn("Service health check failed for " + targetToMonitor + ": " + e.getMessage());
                        LOG.SERVICE_HEALTH_CHECK_FAILED(targetToMonitor, e.getMessage()).tag("org.apache.hadoop.ha").warn();

                        (198: 9) LOG.warn("Transport-level exception trying to monitor health of " + targetToMonitor + ": " + t.getLocalizedMessage());
                        LOG.TRANSPORT_EXCEPTION_WHILE_MONITORING_HEALTH(targetToMonitor, t.getLocalizedMessage()).tag("org.apache.hadoop.ha").warn();

                        (224: 7) LOG.info("Entering state " + newState);
                        LOG.ENTERING_STATE(newState).tag("org.apache.hadoop.ha").tag("enterState()").info();

                        (261: 11) LOG.fatal("Health monitor failed", e);
                        LOG.HEALTH_MONITOR_FAILED(e).tag("org.apache.hadoop.ha").tag("MonitorDaemon").fatal();

                    NodeFencer.java  (7 usages)
                        (91: 5) LOG.info("====== Beginning Service Fencing Process... ======");
                        LOG.BEGINNING_SERVICE_FENCING_PROCESS().tag("org.apache.hadoop.ha").info();

                        (94: 7) LOG.info("Trying method " + (++i) + "/" + methods.size() +": " + method);
                        LOG.TRYING_METHOD(++i, methods.size(), method).tag("org.apache.hadoop.ha").info();

                        (98: 11) LOG.info("====== Fencing successful by method " + method + " ======");
                        LOG.FENCING_SUCCESSFUL_BY_METHOD(method).tag("org.apache.hadoop.ha").info();

                        (102: 9) LOG.error("Fencing method " + method + " misconfigured", e);
                        LOG.FENCING_METHOD_MISCONFIGURED(method, e).tag("org.apache.hadoop.ha").error();

                        (105: 9) LOG.error("Fencing method " + method + " failed with an unexpected error.", t);
                        LOG.FENCING_METHOD_FAILED_ERROR(method, t).tag("org.apache.hadoop.ha").error();

                        (108: 7) LOG.warn("Fencing method " + method + " was unsuccessful.");
                        LOG.FENCING_METHOD_UNSUCCESSFUL(method).tag("org.apache.hadoop.ha").warn();

                        (111: 5) LOG.error("Unable to fence service by any configured method.");
                        LOG.UNABLE_TO_FENCE_SERVICE_BY_PROVIDED_METHODS().tag("org.apache.hadoop.ha").error();

                    ShellCommandFencer.java  (5 usages)
                        (87: 7) LOG.warn("Unable to execute " + cmd, e);
                        LOG.UNABLE_TO_EXECUTE(cmd, e).tag("org.apache.hadoop.ha").tag("tryFence()").warn();

                        (92: 5) LOG.info("Launched fencing command '" + cmd + "' with " + ((pid != null) ? ("pid " + pid) : "unknown pid"));
                        LOG.STARTED_FENCING_COMMAND(cmd, ((pid != null) ? pid : "unknown pid") ).tag("org.apache.hadoop.ha").info();

                        (117: 7) LOG.warn("Interrupted while waiting for fencing command: " + cmd);
                        LOG.INTERRUPTED_WHILE_WAITING_FOR_FENCING_COMMAND(cmd).tag("org.apache.hadoop.ha").warn();

                        (158: 9) LOG.trace("Unable to determine pid for " + p + " since it is not a UNIXProcess");
                        LOG.UNABLE_TO_DETERMINE_PID_NOT_UNIX_PROCESS(p).tag("org.apache.hadoop.ha").trace();

                        (163: 7) LOG.trace("Unable to determine pid for " + p, t);
                        LOG.UNABLE_TO_DETERMINE_PID(p, t).tag("org.apache.hadoop.ha").trace();

                    SshFenceByTcpPort.java  (20 usages)
                        (93: 7) LOG.warn("Unable to create SSH session", e);
                        LOG.UNABLE_TO_CREATE_SSH_SESSION(e).tag("org.apache.hadoop.ha").warn();

                        (97: 5) LOG.info("Connecting to " + host + "...");
                        LOG.CONNECTING(host).tag("org.apache.hadoop.ha").info();

                        (102: 7) LOG.warn("Unable to connect to " + host + " as user " + args.user, e);
                        LOG.UNABLE_TO_CONNECT_HOST_AS_USER(host, args.user, e).tag("org.apache.hadoop.ha").info();

                        (106: 5) LOG.info("Connected to " + host);
                        LOG.CONNECTED_HOST(host).tag("org.apache.hadoop.ha").info();

                        (111: 7) LOG.warn("Unable to achieve fencing on remote host", e);
                        LOG.UNABLE_TO_ACHIEVE_FENCING_ON_REMOTE_HOST(e).tag("org.apache.hadoop.ha").warn();

                        (135: 7) LOG.info("Looking for process running on port " + port);
                        LOG.LOOKING_FOR_PROCESS_RUNNING_ON_PORT(port).tag("org.apache.hadoop.ha").info();

                        (139: 9) LOG.info("Successfully killed process that was " + "listening on port " + port);
                        LOG.KILLED_PROCESS_LISTENING_ON_PORT(port).tag("org.apache.hadoop.ha").info();

                        (147: 9) LOG.info("Indeterminate response from trying to kill service. Verifying whether it is running using nc...");
                        LOG.UNCLEAR_RESPONSE_FROM_KILL_SERVICE_VERIFYING_RUNNING_USING_NC().tag("org.apache.hadoop.ha").info();

                        (154: 11) LOG.warn("Unable to fence - it is running but we cannot kill it");
                        LOG.UNABLE_TO_FENCE_RUNNING_PROCESS_NOT_KILLABLE().tag("org.apache.hadoop.ha").warn();

                        (157: 11) LOG.info("Verified that the service is down.");
                        LOG.SERVICE_IS_DOWN_VERIFIED().tag("org.apache.hadoop.ha").info();

                        (163: 7) LOG.info("rc: " + rc);
                        LOG.INFO().RETURN_CODE(rc).tag("org.apache.hadoop.ha").tag("doFence()");

                        (166: 7) LOG.warn("Interrupted while trying to fence via ssh", e);
                        LOG.INTERRUPTED_WHILE_FENCING_VIA_SSH(e).tag("org.apache.hadoop.ha").warn();

                        (169: 7) LOG.warn("Unknown failure while trying to fence via ssh", e);
                        LOG.UNKNOWN_FAILURE_WHILE_FENCING_VIA_SSH(e).tag("org.apache.hadoop.ha").warn();

                        (180: 5) LOG.debug("Running cmd: " + cmd);
                        LOG.RUNNING_COMMAND(cmd).tag("org.apache.hadoop.ha").tag("execCommand()").debug();

                        (211: 9) LOG.warn("Couldn't disconnect ssh channel", t);
                        LOG.UNABLE_TO_DISCONNECT_SSH_CHANNEL(t).tag("org.apache.hadoop.ha").warn();

x                        CUSTOM LOGGERS
x                        (299: 9) LOG.debug(message);
x                        (302: 9) LOG.info(message);
x                        (305: 9) LOG.warn(message);
x                        (308: 9) LOG.error(message);
x                        (311: 9) LOG.fatal(message);
x
                    StreamPumper.java  (1 usage)
                        (58: 30) ShellCommandFencer.LOG.warn(logPrefix + ": Unable to pump output from " + type, t);
                        LOG.UNABLE_TO_PUMP_OUTPUT_FROM(type, t, logPrefix).tag("org.apache.hadoop.ha").warn();

                    ZKFailoverController.java  (28 usages)
                        (154: 7) LOG.fatal("Automatic failover is not enabled for " + localTarget + "." +
                        " Please ensure that automatic failover is enabled in the configuration before running the ZK failover controller.");
                        LOG.AUTOMATIC_FAILOVER_NOT_ENABLED(localTarget, "Please ensure that automatic failover is enabled in the configuration before running the ZK failover controller.").tag("org.apache.hadoop.ha").fatal();

                        (186: 7) LOG.fatal("Unable to start failover controller. Unable to connect "
                          + "to ZooKeeper quorum at " + zkQuorum + ". Please check the configured value for " + ZK_QUORUM_KEY + " and ensure that "
                          + "ZooKeeper is running.");
                        LOG.UNABLE_TO_START_FAILOVER_CONTROLLER_CANT_CONNECT(zkQuorum, "check config value", ZK_QUORUM_KEY).tag("org.apache.hadoop.ha").fatal();

                        (212: 7) LOG.fatal("Unable to start failover controller. "
                        + "Parent znode does not exist.\n" + "Run with -formatZK flag to initialize ZooKeeper.");
                        LOG.UNABLE_TO_START_FAILOVER_CONTROLLER_PARENT_ZNODE_NOT_EXISTS("Run with -formatZK flag to initialize ZooKeeper.").tag("org.apache.hadoop.ha").fatal();

                        (221: 7) LOG.fatal("Fencing is not configured for " + localTarget + ".\n" + "You must configure a fencing method before using automatic " + "failover.", e);
                        LOG.FENCING_NOT_CONFIGURED(localTarget, "Configure fencing method before using automatic failover.", e).tag("org.apache.hadoop.ha").fatal();

                        (262: 9) LOG.error("Unable to clear zk parent znode", e);
                        LOG.UNABLE_TO_CLEAR_ZK_PARENT_ZNODE(e).tag("org.apache.hadoop.ha").error();

                        (284: 7) LOG.debug("Failed to confirm", e);
                        LOG.FAILED_TO_CONFIRM(e).tag("org.apache.hadoop.ha").debug();

                        (364: 5) LOG.fatal("Fatal error occurred:" + err);
                        LOG.FATAL_ERROR(err).tag("org.apache.hadoop.ha").tag("fatalError()").fatal();

                        (370: 5) LOG.info("Trying to make " + localTarget + " active...");
                        LOG.TRYING_TO_ACTIVATE(localTarget).tag("org.apache.hadoop.ha").info();

                        String msg = "Successfully transitioned " + localTarget +" to active state";
                        (377: 7) LOG.info(msg);
                        LOG.SUCCESSFULLY_TRANSITIONED_TO_ACTIVE_STATE(localTarget).tag("org.apache.hadoop.ha").info();

                        String msg = "Couldn't make " + localTarget + " active";
                        (382: 7) LOG.fatal(msg, t);
                        LOG.UNABLE_TO_ACTIVATE(localTarget).tag("org.apache.hadoop.ha").fatal();

                        (463: 5) LOG.warn(timeoutMillis + "ms timeout elapsed waiting for an attempt to become active");
                        LOG.ACTIVATION_WAITING_TIMEOUT_ELAPSED(timeoutMillis).tag("org.apache.hadoop.ha").warn();

                        (473: 5) LOG.info("ZK Election indicated that " + localTarget +" should become standby");
                        LOG.BECOMING_STANDBY_BY_ELECTION(localTarget).tag("org.apache.hadoop.ha").info();

                        (478: 7) LOG.info("Successfully transitioned " + localTarget +" to standby state");
                        LOG.TRANSITIONED_TO_STANDBY_STATE(localTarget).tag("org.apache.hadoop.ha").info();

                        (481: 7) LOG.error("Couldn't transition " + localTarget + " to standby state", e);
                        LOG.UNABLE_TO_TRANSITION_TO_STANDBY_STATE(localTarget, e).tag("org.apache.hadoop.ha").error();

                        (501: 5) LOG.info("Should fence: " + target);
                        LOG.FENCING(target).tag("org.apache.hadoop.ha").tag("doFence()").info();

                        (507: 7) LOG.info("Successfully transitioned " + target + " to standby state without fencing");
                        LOG.TRANSITIONED_TO_STANDBY_STATE_WITHOUT_FENCING(target).tag("org.apache.hadoop.ha").info();

                        (515: 7) LOG.error("Couldn't fence old active " + target, e);
                        LOG.UNABLE_TO_FENCE_OLD_ACTIVE(target,e).tag("org.apache.hadoop.ha").error();

                        (560: 9) LOG.info("Requested by " + UserGroupInformation.getCurrentUser() +" at " + Server.getRemoteAddress() + " to cede active role.");
                        LOG.CEDING_ACTIVE_ROLE_BY_USER(UserGroupInformation.getCurrentUser(), Server.getRemoteAddress()).tag("org.apache.hadoop.ha").info();

                        (565: 11) LOG.info("Successfully ensured local node is in standby mode");
                        LOG.LOCAL_NODE_IN_STANDBY_MODE().tag("org.apache.hadoop.ha").info();

                        (567: 11) LOG.warn("Unable to transition local node to standby: " + ioe.getLocalizedMessage());
                        LOG.UNABLE_TO_TRANSITION_TO_STANDBY_LOCAL_NODE(ioe.getLocalizedMessage()).tag("org.apache.hadoop.ha").warn();

                        (569: 11) LOG.warn("Quitting election but indicating that fencing is necessary");
                        LOG.QUITTING_ELECTION_BUT_FENCING_NECESSARY().tag("org.apache.hadoop.ha").warn();

                        (633: 7) LOG.info("Local node " + localTarget + " is already active. No need to failover. Returning success.");
                        LOG.LOCAL_NODE_ALREADY_ACTIVE_NO_FAILOVER_NEEDED(localTarget).tag("org.apache.hadoop.ha").info();

                        (639: 5) LOG.info("Asking " + oldActive + " to cede its active state for " +timeout + "ms");
                        LOG.ASKING_OLDACTIVE_SERVICE_TO_CEDE_ACTIVE_STATE_FOR_MS(oldActive, timeout).tag("org.apache.hadoop.ha").info();

                        (668: 7) LOG.info("Successfully became active. " + attempt.status);
                        LOG.SUCCESSFULLY_BECAME_ACTIVE(attempt.status).tag("org.apache.hadoop.ha").info();

                        (730: 13) LOG.info("Would have joined master election, but this node is " +
                "prohibited from doing so for " +TimeUnit.NANOSECONDS.toMillis(remainingDelay) + " more ms");
                        LOG.NODE_PROHIBITED_TO_JOIN_MASTER_ELECTION_FOR_MS(TimeUnit.NANOSECONDS.toMillis(remainingDelay)).tag("org.apache.hadoop.ha").info();

                        (744: 11) LOG.info("Ensuring that " + localTarget + " does not participate in active master election");
                        LOG.ENSURING_SERVICE_NO_OTHER_PARTICIPATION_IN_ACTIVE_MASTER_ELECTION(localTarget).tag("org.apache.hadoop.ha").info();

                        (751: 11) LOG.info("Quitting master election for " + localTarget +" and marking that fencing is necessary");
                        LOG.QUITTING_MASTER_ELECTION_BUT_FENCING_NECESSARY(localTarget).tag("org.apache.hadoop.ha").info();

                        (796: 5) LOG.info("Local service " + localTarget +" entered state: " + newState);
                        LOG.LOCAL_SERVICE_ENTERED_STATE(localTarget, newState).tag("org.apache.hadoop.ha").info();

                org.apache.hadoop.ha.protocolPB  (1 usage)
                    HAServiceProtocolServerSideTranslatorPB.java  (1 usage)
                        (95: 7) LOG.warn("Unknown request source: " + proto.getReqSource());
                        LOG.UNKNOWN_REQUEST_SOURCE(proto.getReqSource()).tag("org.apache.hadoop.ha.protocolPB").warn();

                org.apache.hadoop.http  (13 usages)
                    HttpServer.java  (13 usages)
                        (280: 9) LOG.info("adding path spec: " + path);
                        LOGADDING_PATH_SPECIFICATION(path).tag("org.apache.hadoop.http").info();

                        (422: 5) LOG.info("addJerseyResourcePackage: packageName=" + packageName+ ", pathSpec=" + pathSpec);
                        LOG.ADD_JERSEY_RESOURCE_PACKAGE_PATH_SPECIFICATION(packageName, pathSpec).tag("org.apache.hadoop.http").info();

                        (480: 8) LOG.info("Adding Kerberos (SPNEGO) filter to " + name);
                        LOG.ADDING_KERBEROS_SPNEGO_FILTER_TO(name).tag("org.apache.hadoop.http").info();

                        (496: 5) LOG.info("Added filter " + name + " (class=" + classname+ ") to context " + webAppContext.getDisplayName());
                        LOG.ADDED_FILTER_CLASS_TO_CONTEXT(name, classname, webAppContext.getDisplayName()).tag("org.apache.hadoop.http").tag("webAppContext").info();

                        (503: 9) LOG.info("Added filter " + name + " (class=" + classname+ ") to context " + ctx.getDisplayName());
                        LOG.ADDED_FILTER_CLASS_TO_CONTEXT(name, classname, ctx.getDisplayName()).tag("org.apache.hadoop.http").info();


                        (518: 5) LOG.info("Added global filter '" + name + "' (class=" + classname + ")");
                        LOG.ADDED_GLOBAL_FILTER(name, classname).tag("org.apache.hadoop.http").info();

                        (676: 9) LOG.info("Jetty bound to port " + listener.getLocalPort());
                        LOG.JETTY_BOUND_TO_PORT(listener.getLocalPort()).tag("org.apache.hadoop.http").info();

                        (679: 9) LOG.info("HttpServer.start() threw a non Bind IOException", ex);
                        LOG.HTTPSERVER_START_METHOD_THREW_NON_BIND_IOEXCEPTION(ex).tag("org.apache.hadoop.http").info();

                        (682: 9) LOG.info("HttpServer.start() threw a MultiException", ex);
                        LOG.HTTPSERVER_START_METHOD_THREW_MULTIEXCEPTION(ex).tag("org.apache.hadoop.http").info();

                        (766: 7) LOG.error("Error while stopping listener for webapp" + webAppContext.getDisplayName(), e);
                        LOG.ERROR_WHILE_STOPPING_LISTENER_FOR_WEBAPP(webAppContext.getDisplayName(), e).tag("org.apache.hadoop.http").error();

                        (776: 7) LOG.error("Error while destroying the SSLFactory" + webAppContext.getDisplayName(), e);
                        LOG.ERROR_WHILE_DESTROYING_SSLFACTORY(webAppContext.getDisplayName(), e).tag("org.apache.hadoop.http").error();

                        (786: 7) LOG.error("Error while stopping web app context for webapp "+ webAppContext.getDisplayName(), e);
                        LOG.ERROR_WHILE_STOPPING_WEBAPP_CONTEXT_FOR_WEBAPP(webAppContext.getDisplayName(), e).tag("org.apache.hadoop.http").error();

                        (793: 7) LOG.error("Error while stopping web server for webapp "+ webAppContext.getDisplayName(), e);
                        LOG.ERROR_WHILE_STOPPING_WEBSERVER_FOR_WEBAPP(webAppContext.getDisplayName(), e).tag("org.apache.hadoop.http").error();

                org.apache.hadoop.http.lib  (1 usage)
                    StaticUserWebFilter.java  (1 usage)
                        (141: 7) LOG.warn(DEPRECATED_UGI_KEY + " should not be used. Instead, use " + HADOOP_HTTP_STATIC_USER + ".");
                        LOG.DO_NOT_USE_DEPRECATED_UGI_KEY_USE_INSTEAD_HADOOP_HTTP_STATIC_USER(DEPRECATED_UGI_KEY, HADOOP_HTTP_STATIC_USER).tag("org.apache.hadoop.http.lib").warn();

                org.apache.hadoop.io  (16 usages)
                    BloomMapFile.java  (1 usage)
                        (242: 9) LOG.warn("Can't open BloomFilter: " + ioe + " - fallback to MapFile.");
                        LOG.UNABLE_TO_OPEN_BLOOMFILTER_FALLBACK_TO_MAPFILE(ioe).tag("org.apache.hadoop.io").warn();

                    IOUtils.java  (1 usage)
                        (269: 9) LOG.debug("Ignoring exception while closing socket", ignored);
                        LOG.IGNORING_EXCEPTION_WHILE_CLOSING_SOCKET(ignored).tag("org.apache.hadoop.io").debug();

                    MapFile.java  (1 usage)
                        (504: 9) LOG.warn("Unexpected EOF reading " + index +" at entry #" + count + ".  Ignoring.");
                        LOG.UNEXPECTED_EOF_READING_IGNORING(index, count).tag("org.apache.hadoop.io").warn();

                    ReadaheadPool.java  (2 usages)
                        (152: 7) LOG.trace("submit readahead: " + req);
                        LOG.READAHEAD_SUBMIT(req).tag("org.apache.hadoop.io").trace();

                        (214: 9) LOG.warn("Failed readahead on " + identifier, ioe);
                        LOG.READAHEAD_FAILED_ON(identifier, ioe).tag("org.apache.hadoop.io").warn();

                    SequenceFile.java  (9 usages)
                        (2181: 11) LOG.info("available bytes: " + valIn.available());
                        LOG.AVAILABLE_BYTES(valIn.available()).tag("org.apache.hadoop.io").info();

                        (2196: 11) LOG.debug(val + " is a zero-length value");
                        LOG.ZERO_LENGTH_VALUE(val).tag("org.apache.hadoop.io").debug();

                        (2220: 11) LOG.info("available bytes: " + valIn.available());
                        LOG.AVAILABLE_BYTES(valIn.available()).tag("org.apache.hadoop.io").info();

                        (2235: 11) LOG.debug(val + " is a zero-length value");
                        LOG.ZERO_LENGTH_VALUE(val).tag("org.apache.hadoop.io").debug();

                        (2569: 9) LOG.warn("Bad checksum at "+getPosition()+". Skipping entries.");
                        LOG.SKIPPING_ENTRIES_BAD_CHECKSUM(getPosition()).tag("org.apache.hadoop.io").warn();

                        (2779: 9) LOG.debug("running sort pass");
                        LOG.RUNNING_SORT_PASS().tag("org.apache.hadoop.io").debug();

                        (2879: 13) LOG.debug("flushing segment " + segments);
                        LOG.FLUSHING_SEGMENT(segmets).tag("org.apache.hadoop.io").debug();

                        (3168: 9) LOG.debug("running merge pass");
                        LOG.RUNNING_MERGE_PASS().tag("org.apache.hadoop.io").debug();

                        (3406: 15) LOG.debug("writing intermediate results to " + outputFile);
                        LOG.WRITING_INTERMEDIATE_RESULTS_TO(outputFile).tag("org.apache.hadoop.io").debug();

                    UTF8.java  (2 usages)
                        (88: 7) LOG.warn("truncating long string: " + string.length()+ " chars, starting with " + string.substring(0, 20));
                        LOG.TRUNCATING_LONG_STRING_TO(string.length(), string.substring(0,20)).tag("org.apache.hadoop.io").warn();

                        (326: 7) LOG.warn("truncating long string: " + s.length() + " chars, starting with " + s.substring(0, 20));
                        LOG.TRUNCATING_LONG_STRING_TO(s.length(), s.substring(0,20)).tag("org.apache.hadoop.io").warn();

-----------------------------
26.3.
                org.apache.hadoop.io.compress  (5 usages)
                    CodecPool.java  (4 usages)
                        (107: 7) LOG.info("Got brand-new compressor ["+codec.getDefaultExtension()+"]");
                        LOG.GOT_NEW_COMPRESSOR(codec.getDefaultExtension()).tag("org.apache.hadoop.io.compress").info();

                        (111: 9) LOG.debug("Got recycled compressor");
                        LOG.GOT_RECYCLED_COMPRESSOR().tag("org.apache.hadoop.io.compress").debug();

                        (134: 7) LOG.info("Got brand-new decompressor ["+codec.getDefaultExtension()+"]");
                        LOG.GOT_NEW_COMPRESSOR(codec.getDefaultExtension()).tag("org.apache.hadoop.io.compress").info();

                        (137: 9) LOG.debug("Got recycled decompressor");
                        LOG.GOT_RECYCLED_COMPRESSOR().tag("org.apache.hadoop.io.compress").debug();

                    DefaultCodec.java  (1 usage)
                        (57: 5) LOG.warn("DefaultCodec.createOutputStream() may leak memory. Create a compressor first.");
                        LOG.DEFAULT_CODEC_MAY_LEAK_MEMORY_CREATE_COMPRESSOR().tag("org.apache.hadoop.io.compress").tag("DefaultCodec.createOutputStream()").warn();

                org.apache.hadoop.io.compress.lz4  (4 usages)
                    Lz4Compressor.java  (2 usages)
                        (63: 9) LOG.warn(t.toString());
                        LOG.IGNORING_FAILURE_TO_INITIALIZE_LZ4(t).tag("org.apache.hadoop.io.compress").warn();

                        (66: 7) LOG.error("Cannot load " + Lz4Compressor.class.getName() + " without native hadoop library!");
                        LOG.UNABLE_TO_LOAD_LZ4_COMPRESSOR_WITHOUT_NATIVE_HADOOP_LIBRARY(Lz4Compressor.class.getName()).tag("org.apache.hadoop.io.compress").error();

                    Lz4Decompressor.java  (2 usages)
                        (58: 9) LOG.warn(t.toString());
                        LOG.IGNORING_FAILURE_TO_INITIALIZE_LZ4(t).tag("org.apache.hadoop.io.compress").warn();

                        (61: 7) LOG.error("Cannot load " + Lz4Compressor.class.getName() +" without native hadoop library!");
                        LOG.UNABLE_TO_LOAD_LZ4_COMPRESSOR_WITHOUT_NATIVE_HADOOP_LIBRARY(Lz4Compressor.class.getName()).tag("org.apache.hadoop.io.compress").error();

                org.apache.hadoop.io.compress.snappy  (2 usages)
                    SnappyCompressor.java  (1 usage)
                        (64: 9) LOG.error("failed to load SnappyCompressor", t);
                        LOG.FAILED_TO_LOAD_SNAPPY_COMPRESSOR(t).tag("org.apache.hadoop.io.compress").error();

                    SnappyDecompressor.java  (1 usage)
                        (60: 9) LOG.error("failed to load SnappyDecompressor", t);
                        LOG.FAILED_TO_LOAD_SNAPPY_DECOMPRESSOR(t).tag("org.apache.hadoop.io.compress").error();

                org.apache.hadoop.io.compress.zlib  (5 usages)
                    BuiltInZlibDeflater.java  (2 usages)
                        (77: 7) LOG.warn(strategy + " not supported by BuiltInZlibDeflater.");
                        LOG.STRATEGY_NOT_SUPPORTED_BY_BUILTINZLIBDEFLATER(strategy).tag("org.apache.hadoop.io.compress").warn();

                        (81: 7) LOG.debug("Reinit compressor with new compression configuration");
                        LOG.REINITIALIZING_COMPRESSOR_WITH_NEW_CONFIGURATION().tag("org.apache.hadoop.io.compress").debug();

                    ZlibCompressor.java  (1 usage)
                        (258: 7) LOG.debug("Reinit compressor with new compression configuration");
                        LOG.REINITIALIZING_COMPRESSOR_WITH_NEW_CONFIGURATION().tag("org.apache.hadoop.io.compress").debug();

                    ZlibFactory.java  (2 usages)
                        (48: 9) LOG.info("Successfully loaded & initialized native-zlib library");
                        LOG.SUCCESSFULLY_LOADED_AND_INITIALIZED_NATIVE_ZLIB_LIBRARY().tag("org.apache.hadoop.io.compress").info();

                        (50: 9) LOG.warn("Failed to load/initialize native-zlib library");
                        LOG.FAILED_TO_LOAD_OR_INITIALIZE_NATIVE_ZLIB_LIBRARY().tag("org.apache.hadoop.io.compress").warn();

                org.apache.hadoop.io.file.tfile  (7 usages)
                    Compression.java  (7 usages)
                        (90: 13) LOG.info("Trying to load Lzo codec class: " + clazz);
                        LOG.TRYING_TO_LOAD_LZO_CODEC_CLASS(clazz).tag("org.apache.hadoop.io.file").info();

                        (279: 13) LOG.warn("Compressor obtained from CodecPool already finished()");
                        LOG.COMPRESSOR_OBTAINED_ALREADY_FINISHED_FLAG().tag("org.apache.hadoop.io.file").warn();

                        (282: 15) LOG.debug("Got a compressor: " + compressor.hashCode());
                        LOG.GOT_COMPRESSOR(compressor.hashCode()).tag("org.apache.hadoop.io.file").debug();

                        (299: 11) LOG.debug("Return a compressor: " + compressor.hashCode());
                        LOG.RETURN_COMPRESSOR(compressor.hashCode()).tag("org.apache.hadoop.io.file").debug();

                        (313: 13) LOG.warn("Deompressor obtained from CodecPool already finished()");
                        LOG.DECOMPRESSOR_OBTAINED_ALREADY_FINISHED_FLAG().tag("org.apache.hadoop.io.file").warn();

                        (316: 15) LOG.debug("Got a decompressor: " + decompressor.hashCode());
                        LOG.GOT_DECOMPRESSOR(decompressor.hashCode()).tag("org.apache.hadoop.io.file").debug();

                        (334: 11) LOG.debug("Returned a decompressor: " + decompressor.hashCode());
                        LOG.RETURN_DECOMPRESSOR(decompressor.hashCode()).tag("org.apache.hadoop.io.file").debug();

                org.apache.hadoop.io.nativeio  (3 usages)
                    NativeIO.java  (3 usages)
                        (114: 9) LOG.debug("Initialized cache for IDs to User/Group mapping with a cache timeout of " + cacheTimeout/1000 + " seconds.");
                        LOG.INITIALIZED_CACHE_FOR_USER_GROUP_ID_MAPPING_WITH_CACHE_TIMEOUT_SECONDS(cacheTimeout/1000).tag("org.apache.hadoop.io.nativeio").debug();

                        (121: 9) LOG.error("Unable to initialize NativeIO libraries", t);
                        LOG.UNABLE_TO_INITIALIZED_NATIVEIO_LIBRARIES(t).tag("org.apache.hadoop.io.nativeio").error();

                        (275: 9) LOG.debug("Got " + type + " " + name + " for ID " + id + " from the native implementation");
                        LOG.GOT_FROM_NATIVE_IMPLEMENTATION_INFO(type, name, id).tag("org.apache.hadoop.io.nativeio").debug();

                org.apache.hadoop.io.retry  (12 usages)
                    RetryInvocationHandler.java  (5 usages)
                        (93: 13) LOG.warn("Exception while invoking " + currentProxy.getClass() + "." + method.getName() + ". Not retrying because " + action.reason, e);
                        LOG.EXCEPTION_INVOKING_NOT_RETRYING_REASON(currentProxy.getClass(), method.getName(), action.reason, e).tag("org.apache.hadoop.io").warn();


HELP                        (114: 15) LOG.debug(msg, e);
HELP                        (116: 15) LOG.warn(msg);

                        (120: 15) LOG.debug("Exception while invoking " + method.getName()+ " of class " + currentProxy.getClass().getSimpleName() + ". Retrying " + formatSleepMessage(action.delayMillis), e);
                        LOG.ERROR_WHILE_INVOKING_RETRYING(method.getName(), currentProxy.getClass().getSimpleName(), formatSleepMessage(action.delayMillis), e).tag("org.apache.hadoop.io").debug();

                        (139: 17) LOG.warn("A failover has occurred since the start of this method invocation attempt.");
                        LOG.FAILOVER_OCCURED_SINCE-START_OF_METHOD_INVOCATION_ATTEMPT().tag("org.apache.hadoop.io").warn();

                    RetryPolicies.java  (4 usages)
                        (368: 9) LOG.warn("Illegal value: there is no element in \"" + s + "\".");
                        LOG.ILLEGAL_VALUE_NO_ELEMENT_IN(s).tag("org.apache.hadoop.io").warn();

                        (372: 9) LOG.warn("Illegal value: the number of elements in \"" + s + "\" is " + elements.length + " but an even number of elements is expected.");
                        LOG.ERROR_ODD_NUMBER_OF_ELEMENTS_IN(s, elements.length).tag("org.apache.hadoop.io").warn();

                        (410: 9) LOG.warn("Failed to parse \"" + s + "\", which is the index " + i + " element in \"" + originalString + "\"", nfe);
                        LOG.FAILED_TO_PARSE(s, "index", i, "element in", originalString, nfe).tag("org.apache.hadoop.io").warn();

                        (416: 9) LOG.warn("The value " + n + " <= 0: it is parsed from the string \"" + s + "\" which is the index " + i + " element in \"" + originalString + "\"");
                        LOG.VALUE_LOWER_THEN_ZERO(n, "from", s, "index", i, "element", originalString).tag("org.apache.hadoop.io").warn();

                    RetryUtils.java  (3 usages)
                        (74: 7) LOG.debug("multipleLinearRandomRetry = " + multipleLinearRandomRetry);
                        LOG.MULTIPLE_LINEAR_RANDOM_RETRY(multipleLinearRandomRetry).tag("org.apache.hadoop.io").debug();

                        (106: 13) LOG.debug("RETRY " + retries + ") policy="+ p.getClass().getSimpleName() + ", exception=" + e);
                        LOG.RETRY_POLICY_EXCEPTION(retries, policy, exception).tag("org.apache.hadoop.io").debug();

                        (109: 11) LOG.info("RETRY " + retries + ") policy="+ p.getClass().getSimpleName() + ", exception=" + e);
                        LOG.RETRY_POLICY_EXCEPTION(retries, policy, exception).tag("org.apache.hadoop.io").info();

                org.apache.hadoop.io.serializer  (2 usages)
                    SerializationFactory.java  (2 usages)
                        (59: 7) LOG.warn("Serialization for various data types may not be available. Please configure " + CommonConfigurationKeys.IO_SERIALIZATIONS_KEY + " properly to have serialization support (it is currently not set).");
                        LOG.SERIALIZATION_MAY_NOT_BE_AVAILABLE_CONFIGURE_PROPERTY(CommonConfigurationKeys.IO_SERIALIZATIONS_KEY).tag("org.apache.hadoop.io").warn();

                        (81: 7) LOG.warn("Serialization class not found: ", e);
                        LOG.SERIALIZATION_CLASS_NOT_FOUND(e).tag("org.apache.hadoop.io").warn();

                org.apache.hadoop.ipc  (103 usages)
                    Client.java  (23 usages)
                        (300: 9) LOG.debug("The ping interval is " + this.pingInterval + " ms.");
                        LOG.PING_INTERVAL_MS(this.pingInterval).tag("org.apache.hadoop.ipc").debug();

                        (324: 13) LOG.debug("RPC Server's Kerberos principal name for protocol="+ protocol.getCanonicalName() + " is " + serverPrincipal);
                        LOG.SERVER_KERBEROS_PRINCIPAL_NAME_FOR_PROTOCOL(serverPrincipal, protocol.getCanonicalName()).tag("org.apache.hadoop.ipc").debug();

                        (341: 9) LOG.debug("Use " + authMethod + " authentication for protocol "+ protocol.getSimpleName());
                        LOG.USE_AUTHENTICATION_METHOD_FOR_PROTOCOL(authMethod, protocol.getSimpleName()).tag("org.apache.hadoop.ipc").debug();

                        (473: 9) LOG.warn("Address change detected. Old: " + server.toString() +" New: " + currentAddr.toString());
                        LOG.DETECTED_ADDRESS_CHANGE_OLD_NEW(server.toString(), currentAddr.toString()).tag("org.apache.hadoop.ipc").warn();

                        (554: 17) LOG.debug("Exception encountered while connecting to the server : " + ex);
                        LOG.EXCEPTION_WHILE_CONNECTING_TO_SERVER(ex).tag("org.apache.hadoop.ipc").debug();

                        String msg = "Couldn't setup connection for "+ UserGroupInformation.getLoginUser().getUserName() + " to "+ serverPrincipal;
                        (573: 15) LOG.warn(msg);
                        LOG.UNABLE_TO_SETUP_CONNECTION_FOR_TO(UserGroupInformation.getLoginUser().getUserName(), serverPrincipal).tag("org.apache.hadoop.ipc").warn();

                        (577: 13) LOG.warn("Exception encountered while connecting to the server : " + ex);
                        LOG.EXCEPTION_WHILE_CONNECTING_TO_SERVER(ex).tag("org.apache.hadoop.ipc").warn();

                        (598: 11) LOG.debug("Connecting to "+server);
                        LOG.CONNECTING_TO_SERVER(server).tag("org.apache.hadoop.ipc").debug();

                        (677: 9) LOG.warn("Not able to close a socket", e);
                        LOG.UNABLE_TO_CLOSE_SOCKET(e).tag("org.apache.hadoop.ipc").warn();

                        (713: 7) LOG.info("Retrying connect to server: " + server + ". Already tried "+ curRetries + " time(s); maxRetries=" + maxRetries);
                        LOG.RETRYING_CONNECT_SERVER(server, curRetries, maxRetries).tag("org.apache.hadoop.ipc").info();

                        (729: 11) LOG.warn("Failed to connect to server: " + server + ": "+ action.reason, ioe);
                        LOG.FAILED_TO_CONNECT_SERVER(server, action.reason, ioe).tag("org.apache.hadoop.ipc").warn();

                        (741: 7) LOG.info("Retrying connect to server: " + server + ". Already tried "+ curRetries + " time(s); retry policy is " + connectionRetryPolicy);
                        LOG.RETRYING_CONNECT_SERVER(server, curRetries, connectionRetryPolicy).tag("org.apache.hadoop.ipc").info();

                        (840: 9) LOG.debug(getName() + ": starting, having connections " + connections.size());
                        LOG.STARTING_THREAD_WITH-CONNECTIONS(getName(), connections.size()).tag("org.apache.hadoop.ipc").debug();

                        (851: 9) LOG.warn("Unexpected error reading responses on connection " + this, t);
                        LOG.ERROR_READING_RESPONSES_ON_CONNECTION(this, t).tag("org.apache.hadoop.ipc").warn();

                        (858: 9) LOG.debug(getName() + ": stopped, remaining connections "+ connections.size());
                        LOG.THREAD_STOPPED_REMAINING_CONECTIONS(getName(), connections.size()).tag("org.apache.hadoop.ipc").debug();

                        (901: 19) LOG.debug(getName() + " sending #" + call.id);
                        LOG.THREAD_SENDING_RPC_CALL(getName(), call.id).tag("org.apache.hadoop.ipc").debug();

                        (956: 11) LOG.debug(getName() + " got value #" + callId);
                        LOG.THREAD_GOT_RPC_CALL_VALUE(getName(), callId).tag("org.apache.hadoop.ipc").debug();

                        (989: 9) LOG.error("The connection is not in the closed state");
                        LOG.CONNECTION_IS_NOT_IN_CLOSED_STATE().tag("org.apache.hadoop.ipc").error();

                        (1009: 11) LOG.warn("A connection is closed for no cause and calls are not empty");
                        LOG.CLOSED_CONNECTION_AND_CALLS_NOT_EMPTY().tag("org.apache.hadoop.ipc").warn();

                        (1019: 11) LOG.debug("closing ipc connection to " + server + ": " + closeException.getMessage(),closeException);
                        LOG.CLOSING_IPC_CONNECTION_TO_SERVER(server, closeException.getMessage(), closeException).tag("org.apache.hadoop.ipc").debug();

                        (1027: 9) LOG.debug(getName() + ": closed");
                        LOG.THREAD_CLOSED(getName()).tag("org.apache.hadoop.ipc").debug();

                        (1073: 7) LOG.debug("Stopping client");
                        LOG.STOPPING_CLIENT().tag("org.apache.hadoop.ipc").debug();

                        (1215: 7) LOG.warn("interrupted waiting to send rpc request to server", e);
                        LOG.INTERRUPTED_WAITING_TO_SEND_RPC_REQUEST_TO_SERVER(e).tag("org.apache.hadoop.ipc").warn();

                    ProtobufRpcEngine.java  (7 usages)
                        (197: 9) LOG.trace(Thread.currentThread().getId() + ": Call -> " + remoteId + ": " + method.getName() + " {" + TextFormat.shortDebugString((Message) args[1]) + "}");
                        LOG.CLIENT_RPC_METHOD_CALL_INVOKE(Thread.currentThread().getId(), remoteId, method.getName(), TextFormat.shortDebugString((Message) args[1])).tag("org.apache.hadoop.ipc").trace();

                        (207: 11) LOG.trace(Thread.currentThread().getId() + ": Exception <- " + remoteId + ": " + method.getName() +" {" + e + "}");
                        LOG.EXCEPTION_WHILE_INVOKING_CLIENT_RPC_METHOD(Thread.currentThread().getId(), remoteId, method.getName(), e).tag("org.apache.hadoop.ipc").trace();

                        (217: 9) LOG.debug("Call: " + method.getName() + " took " + callTime + "ms");
                        LOG.CLIENT_RPC_METHOD_CALL_TIME_MS(method.getName(), callTime).tag("org.apache.hadoop.ipc").debug();

                        (232: 11) LOG.trace(Thread.currentThread().getId() + ": Response <- " + remoteId + ": " + method.getName() +" {" + TextFormat.shortDebugString(returnMessage) + "}");
                        LOG.CLIENT_RPC_METHOD_CALL_RETURN_MESSAGE(Thread.currentThread().getId(), remoteId, method.getName(), TextFormat.shortDebugString(returnMessage)).tag("org.apache.hadoop.ipc").trace();

                        (453: 11) LOG.info("Call: connectionProtocolName=" + connectionProtocolName + ", method=" + methodName);
                        LOG.SERVER_RPC_METHOD_CALL(connectionProtocolName, methodName).tag("org.apache.hadoop.ipc").info();

                        String msg = "Unknown method " + methodName + " called on " + connectionProtocolName + " protocol.";
                        (464: 11) LOG.warn(msg);
                        LOG.UNKNOWN_SERVER_METHOD_CALLED_ON_PROTOCOL(methodName, connectionProtocolName).tag("org.apache.hadoop.ipc").warn();

                        (478: 13) LOG.info("Served: " + methodName + " queueTime= " + qTime +" procesingTime= " + processingTime);
                        LOG.RPC_SERVER_SERVED_METHOD_QUEUE_PROCESSING_TIME(methodName, qTime, processingTime).tag("org.apache.hadoop.ipc").info();

                    RPC.java  (9 usages)
                        (125: 9) LOG.warn("Interface " + childInterface +" ignored because it does not extend VersionedProtocol");
                        LOG.INTERFACE_DOES_NOT_EXTEND_VERSIONED_PROTOCOL_IGNORING(childInterface).tag("org.apache.hadoop.ipc").warn();

                        (385: 9) LOG.info("Server at " + addr + " not available yet, Zzzzz...");
                        LOG.SERVER_NOT_AVAILABLE(addr).tag("org.apache.hadoop.ipc").info();

                        (388: 9) LOG.info("Problem connecting to server: " + addr);
                        LOG.PROBLEM_CONNECTING_SERVER(addr).tag("org.apache.hadoop.ipc").info();

                        (391: 9) LOG.info("No route to host for server: " + addr);
                        LOG.NO_ROUTE_TO_HOST_FOR_SERVER(addr).tag("org.apache.hadoop.ipc").info();

                        (620: 7) LOG.error("Closing proxy or invocation handler caused exception", e);
                        LOG.EXCEPTION_CAUSED_BY_CLOSING_PROXY_OR_IVOCATION_HANDLER(e).tag("org.apache.hadoop.ipc").error();

                        (622: 7) LOG.error("RPC.stopProxy called on non proxy.", e);
                        LOG.RPC_STOPPROXY_METHOD_CALLED_ON_NON_PROXY_OBJECT(e).tag("org.apache.hadoop.ipc").error();

                        (913: 8) LOG.warn("Protocol "  + protocolClass + " NOT registered as cannot get protocol version ");
                        LOG.PROTOCOL_NOT_REGISTERED_UNABLE_TO_GET_PROTOCOL_VERSION(protocolClass).tag("org.apache.hadoop.ipc").warn();

                        (921: 6) LOG.debug("RpcKind = " + rpcKind + " Protocol Name = " + protocolName +  " version=" + version +
                        " ProtocolImpl=" + protocolImpl.getClass().getName() + " protocolClass=" + protocolClass.getName());
                        LOG.RPC_SERVER_INFORMATION(rpcKind, protocolName, version, protocolImpl.getClass().getName(), protocolClass.getName()).tag("org.apache.hadoop.ipc").debug();

                        (962: 8) LOG.debug("Size of protoMap for " + rpcKind + " ="+ getProtocolImplMap(rpcKind).size());
                        LOG.RPC_SERVER_SIZE_OF_PROTOMAP_FOR_RPC_KIND(rpcKind, getProtocolImplMap(rpcKind).size()).tag("org.apache.hadoop.ipc").debug();

                    Server.java  (60 usages)
                        (245: 5) LOG.debug("rpcKind=" + rpcKind + ", rpcRequestWrapperClass=" + rpcRequestWrapperClass + ", rpcInvoker=" + rpcInvoker);
                        LOG.RPC_REGISTER_KIND_AND_DESERIALIZE_CLASS(rpcKind, rpcRequestWrapperClass, rpcInvoker).tag("org.apache.hadoop.ipc").debug();

                        (531: 9) LOG.info("Starting " + getName());
                        LOG.STARTING_THREAD(getName()).tag("org.apache.hadoop.ipc").info();

                        (538: 13) LOG.error("Error closing read selector in " + this.getName(), ioe);
                        LOG.ERROR_CLOSING_READ_SELECTOR_IN_THREAD(this.getName(), ioe).tag("org.apache.hadoop.ipc").error();

                        (565: 15) LOG.info(getName() + " unexpectedly interrupted", e);
                        LOG.THREAD_UNEXPECTEDLY_INTERRUPTED(getName(), e).tag("org.apache.hadoop.ipc").info();

                        (568: 13) LOG.error("Error in Reader", ex);
                        LOG.RPC_SERVER_ERROR_IN_READER(ex).tag("org.apache.hadoop.ipc").error();

                        (640: 15) LOG.debug(getName() + ": disconnecting client " + c.getHostAddress());
                        LOG.RPC_SERVER_DISCONNECTING_CLIENT(getName(), c.getHostAddress).tag("org.apache.hadoop.ipc").debug();

                        (655: 7) LOG.info(getName() + ": starting");
                        LOG.RPC_STARTING_THREAD(getName()).tag("org.apache.hadoop.ipc").info();

                        (678: 11) LOG.warn("Out of Memory in server select", e);
                        LOG.OUT_OF_MEMORY_IN_SERVER_SELECT(e).tag("org.apache.hadoop.ipc").warn();

                        (687: 7) LOG.info("Stopping " + this.getName());
                        LOG.STOPPING_THREAD(this.getName()).tag("org.apache.hadoop.ipc").info();

                        (710: 13) LOG.debug(getName() + ": disconnecting client " + c.getHostAddress());
                        LOG.RPC_SERVER_DISCONNECTING_CLIENT(getName(), c.getHostAddress()).tag("org.apache.hadoop.ipc").debug();

                        (741: 13) LOG.debug("Server connection from " + c.toString() +"; # active connections: " + numConnections +"; # queued calls: " + callQueue.size());
                        LOG.SERVER_INFORMATION_CONNECTION_FROM_ACTIVE_CONNECTIONS_QUEUED_CALLS(c.toString(), numConnections, callQueue.size()).tag("org.apache.hadoop.ipc").debug();

                        (761: 9) LOG.info(getName() + ": readAndProcess caught InterruptedException", ieo);
                        LOG.INTERRUPTED_EXCEPTION_WHILE_READ_AND_PROCESS(ioe).tag("org.apache.hadoop.ipc").info();

                        (764: 9) LOG.info(getName() + ": readAndProcess threw exception " + e +" from client " + c.getHostAddress() +". Count of bytes read: " + count, e);
                        LOG.READ_AND_PROCESS_THREW_EXCEPTION_FROM_CLIENT(e, c.getHostAddress, "bytes read",count).tag("org.apache.hadoop.ipc").info();

-------------------------
                        (771: 11) LOG.debug(getName() + ": disconnecting client " + c + ". Number of active connections: "+numConnections);
                        String thread = getName();
                        LOG.RPC_SERVER_DISCONNECTING_CLIENT(thread, c, "activeConnections", numConnections).tag("org.apache.hadoop.ipc").debug();

                        (791: 11) LOG.info(getName() + ":Exception in closing listener socket. " + e);
                        LOG.EXCEPTION_IN_CLOSING_LISTENER_SOCKET(getName(), e).tag("org.apache.hadoop.ipc").info();

                        (824: 7) LOG.info(getName() + ": starting");
                        LOG.RPC_RESPONDER_STARTING(getName()).tag("org.apache.hadoop.ipc").info();

                        (829: 9) LOG.info("Stopping " + this.getName());
                        LOG.RPC_RESPONDER_STOPPING(this.getName()).tag("org.apache.hadoop.ipc").info();

                        (833: 11) LOG.error("Couldn't close write selector in " + this.getName(), ioe);
                        LOG.UNABLE_TO_CLoSE_WRITE_SELECTOR_IN(this.getName(), ioe).tag("org.apache.hadoop.ipc").error();

                        (854: 15) LOG.info(getName() + ": doAsyncWrite threw exception " + e);
                        LOG.EXCEPTION_DOASYNCWRITE(getName(), e).tag("org.apache.hadoop.ipc").info();

                        (867: 13) LOG.debug("Checking for old call responses.");
                        LOG.CHECKING_OLD_CALL_RESPONSES().tag("org.apache.hadoop.ipc").debug();

                        (888: 15) LOG.warn("Error in purging old calls " + e);
                        LOG.ERROR_PURGING_OLD_CALLS(e).tag("org.apache.hadoop.ipc").warn();

                        (897: 11) LOG.warn("Out of Memory in server select", e);
                        LOG.OUT_OF_MEMORY_IN_SERVER_SELECT(e).tag("org.apache.hadoop.ipc").warn();

                        (900: 11) LOG.warn("Exception in Responder", e);
                        LOG.EXCEPTION_IN_RESPONDER(e).tag("org.apache.hadoop.ipc").warn();

                        (924: 13) LOG.warn("Exception while changing ops : " + e);
                        LOG.EXCEPTION_WHILE_CHANGING_OPERATION(e).tag("org.apache.hadoop.ipc").warn();

                        (973: 13) LOG.debug(getName() + ": responding to #" + call.callId + " from " + call.connection);
                        LOG.RESPONDING_TO_CALLID(getName(), call.callId, call.connction).tag("org.apache.hadoop.ipc").debug();

                        (993: 15) LOG.debug(getName() + ": responding to #" + call.callId + " from " + call.connection + " Wrote " + numBytes + " bytes.");
                        LOG.RESPONDING_TO_CALLID_WROTE_BYTES(getName(), call.callId, call.connction, numBytes).tag("org.apache.hadoop.ipc").debug();

                        (1021: 15) LOG.debug(getName() + ": responding to #" + call.callId + " from " + call.connection + " Wrote partial " + numBytes + " bytes.");
                        LOG.RESPONDING_TO_CALLID_WROTE_PARTIAL_BYTES(getName(), call.callId, call.connction, numBytes).tag("org.apache.hadoop.ipc").debug();

                        (1030: 11) LOG.warn(getName()+", call " + call + ": output error");
                        LOG.CALL_OUTPUT_ERROR(getName(), call).tag("org.apache.hadoop.ipc").warn();

                        (1133: 11) LOG.warn("Connection: unable to set socket send buffer size to " +socketSendBufferSize);
                        LOG.UNABLE_TO_SET_SOCKET_SEND_BUFFER_SIZE_TO(socketSendBufferSize).tag("org.apache.hadoop.ipc").warn();

                        (1204: 13) LOG.debug("Have read input token of size " + saslToken.length+ " for processing by saslServer.evaluateResponse()");
                        LOG.FOR_PROCESSING_BY_SASL_SERVER_READ_INPUT_TOKEN_OF_SIZE(saslToken.length).tag("org.apache.hadoop.ipc").debug();

                        (1222: 16) AUDITLOG.warn(AUTH_FAILED_FOR + clientIP + ":" + attemptingUser);
                        LOG.AUTHORIZATION_FAILED_FOR_CLIENT(clientIP, attemptingUser).tag("org.apache.hadoop.ipc").tag("AUDITLOG").warn();

                        (1231: 13) LOG.debug("Will send token of size " + replyToken.length+ " from saslServer.");
                        LOG.SENDING_FROM_SASL_SERVER_TOKEN_OF_SIZE(replyToken.length).tag("org.apache.hadoop.ipc").debug();

                        (1238: 13) LOG.debug("SASL server context established. Negotiated QoP is "+ saslServer.getNegotiatedProperty(Sasl.QOP));
                        LOG.SASL_SERVER_CONTEXT_ESTABLISHED_NEGOTIATED_QOP_IS(saslServer.getNegotiatedProperty(Sasl.QOP)).tag("org.apache.hadoop.ipc").debug();

                        (1245: 13) LOG.debug("SASL server successfully authenticated client: " + user);
                        LOG.SASL_SERVER_SUCCESSFULLY_AUTHENTICATED_CLIENT(user).tag("org.apache.hadoop.ipc").debug();

                        (1248: 16) AUDITLOG.info(AUTH_SUCCESSFUL_FOR + user);
                        LOG.AUTHORIZATION_SUCCESSFUL_FOR_CLIENT(user).tag("org.apache.hadoop.ipc").tag("AUDITLOG").info();

                        (1253: 11) LOG.debug("Have read input token of size " + saslToken.length+ " for processing by saslServer.unwrap()");
                        LOG.FOR_PROCESSING_BY_SASL_SERVER_READ_INPUT_TOKEN_OF_SIZE(saslToken.length).tag("org.apache.hadoop.ipc").debug();

                        (1327: 13) LOG.warn("Incorrect header or version mismatch from " + hostAddress + ":" + remotePort +" got version " + version + " expected version " + CURRENT_VERSION);
                        LOG.INCORRECT_HEADER_OR_VERSION_MISMATCH_FROM_HOST(hostAddress, remotePort, version, "expected version", CURRENT_VERSION).tag("org.apache.hadoop.ipc").warn();

!                        (1365: 13) LOG.warn("Unexpected data length " + dataLength + "!! from " + getHostAddress());
!                        String error = "Unexpected data length " + dataLength + "!! from " + getHostAddress();
!                        LOG.warn(error);
!                        LOG.UNEXPECTED_DATA_LENGTH_FROM_HOST(dataLength, getHostAddress).tag("org.apache.hadoop.ipc").warn();

                        (1448: 13) LOG.debug("Kerberos principal name is " + fullName);
                        LOG.KERBEROS_PRINCIPAL_NAME_IS(fullName).tag("org.apache.hadoop.ipc").debug();

                        (1488: 9) LOG.debug("Created SASL server with mechanism = " + mechanism);
                        LOG.DEBUG().CREATED_SASL_SERVER_WITH_MECHANISM(mechanism).tag("org.apache.hadoop.ipc");

                        (1603: 15) LOG.debug("Received ping message");
                        LOG.RECEIVED_PING_MESSAGE().tag("org.apache.hadoop.ipc").debug();

                        (1644: 9) LOG.debug(" got #" + header.getCallId());
                        LOG.SERVER_PROCESS_DATA_GOT_CALL_ID(header.getCallId()).tag("org.apache.hadoop.ipc").debug();

                        (1662: 9) LOG.warn("Unknown rpc kind "  + header.getRpcKind() + " from client " + getHostAddress());
                        LOG.UNKNOWN_RPC_KIND_FROM_CLIENT(header.getRpcKind(), getHostAddress).tag("org.apache.hadoop.ipc").warn();

                        (1679: 9) LOG.warn("Unable to read call parameters for client " + getHostAddress() + "on connection protocol " +this.protocolName + " for rpcKind " + header.getRpcKind(),  t);
                        LOG.UNABLE_TO_READ_CLIENT_CALL_PARAMETERS(getHostAddress(), this.protocolName, header.getRpcKind(), t).tag("org.apache.hadoop.ipc").warn();

                        (1711: 11) LOG.debug("Successfully authorized " + connectionContext);
                        LOG.SUCCESSFULLY_AUTHORIZED(connectionContext).tag("org.apache.hadoop.ipc").debug();

                        (1731: 9) LOG.debug("Ignoring socket shutdown exception", e);
                        LOG.IGNORING_SOCKET_SHUTDOWN_EXCEPTION(e).tag("org.apache.hadoop.ipc").debug();

                        (1749: 7) LOG.debug(getName() + ": starting");
                        LOG.IPC_STARTING(getName()).tag("org.apache.hadoop.ipc").debug();

                        (1757: 13) LOG.debug(getName() + ": has Call#" + call.callId + "for RpcKind " + call.rpcKind + " from " + call.connection);
                        LOG.IPC_SERVER_INFORMATION(getName(), call.callId, call.rpcKind, call.connection).tag("org.apache.hadoop.ipc").debug();

                        (1791: 15) LOG.warn(logMsg, e);
                        LOG.SERVER_ERROR(logMsg, e).tag("org.apache.hadoop.ipc").warn();

                        (1795: 15) LOG.info(logMsg);
                        LOG.SERVER_ERROR(logMsg).tag("org.apache.hadoop.ipc").info();

                        (1797: 15) LOG.info(logMsg, e);
                        LOG.SERVER_ERROR(logMsg, e).tag("org.apache.hadoop.ipc").info();

                        (1820: 15) LOG.warn("Large response size " + buf.size() + " for call "+ call.toString());
                        LOG.LARGE_RESPONSE_SIZE_FOR_CALL(buf.size(), call.toString()).tag("org.apache.hadoop.ipc").warn();

                        (1828: 13) LOG.info(getName() + " unexpectedly interrupted", e);
                        LOG.UNEXPECTED_INTERRUPT(getName(), e).tag("org.apache.hadoop.ipc").info();

                        (1831: 11) LOG.info(getName() + " caught an exception", e);
                        LOG.CAUGHT_EXCEPTION(getName(), e).tag("org.apache.hadoop.ipc").info();

                        (1834: 7) LOG.debug(getName() + ": exiting");
                        LOG.EXITING(getName()).tag("org.apache.hadoop.ipc").debug();

                        (1954: 7) LOG.debug(AuthenticationMethod.TOKEN +" authentication enabled for secret manager");
                        LOG.AUTHENTICATION_ENABLED_FOR_SECRET_MANAGER(AuthenticationToken.TOKEN).tag("org.apache.hadoop.ipc").debug();

                        (1959: 5) LOG.debug("Server accepts auth methods:" + authMethods);
                        LOG.SERVER_ACCEPTS_AUTH_METHODS(authMethods).tag("org.apache.hadoop.ipc").debug();

                        (2002: 9) LOG.warn("Error serializing call response for call " + call, t);
                        LOG.ERROR_SERIALIZING_CALL_RESPONSE_FOR_CALL(call, t).tag("org.apache.hadoop.ipc").warn();

                        (2066: 9) LOG.debug("Adding saslServer wrapped token of size " + token.length+ " as call response.");
                        LOG.TO_CALL_RESPONSE_ADDING_SASL_SERVER_WRAPPED_TOKEN_OF_SIZE(token.length).tag("org.apache.hadoop.ipc").debug();

                        (2096: 5) LOG.info("Stopping server on " + port);
                        LOG.STOPPING_SERVER_ON_PORT(port).tag("org.apache.hadoop.ipc").info();
                    WritableRpcEngine.java  (4 usages)
                        (235: 9) LOG.debug("Call: " + method.getName() + " " + callTime);
                        LOG.RPC_METHOD_CALL_TIME(method.getName(), callTime).tag("org.apache.hadoop.ipc").debug();

??                        (411: 7) LOG.info(value);
                        LOG.LOGGING_MESSAGE(value).tag("org.apache.hadoop.ipc").info();

                        (486: 13) LOG.debug("Served: " + call.getMethodName() +" queueTime= " + qTime +" procesingTime= " + processingTime);
                        LOG.SERVED_RPC_METHOD_QUEUE_PROCESSING_TIME(call.getMethodName(), qTime, processingTime).tag("org.apache.hadoop.ipc").debug();

                        (509: 13) LOG.error("Unexpected throwable object ", e);
                        LOG.UNEXPECTED_THROWABLE_OBJECT(e).tag("org.apache.hadoop.ipc").error();

                org.apache.hadoop.ipc.metrics  (2 usages)
                    RpcDetailedMetrics.java  (1 usage)
                        (47: 5) LOG.debug(registry.info());
                        LOG.RPC_REGISTRY_METRICS(registry.info()).tag("org.apache.hadoop.ipc").debug();

                    RpcMetrics.java  (1 usage)
                        (50: 5) LOG.debug("Initialized "+ registry);
                        LOG.INITIALZED_RPC_REGISTRY_METRICS(registry).tag("org.apache.hadoop.ipc").debug();
=======================
27.3.
LOG>ERR
                org.apache.hadoop.jmx  (15 usages)
                    JMXJsonServlet.java  (15 usages)
                        (215: 7) LOG.error("Caught an exception while processing JMX request", e);
                        LOG.EXCEPTION_WHILE_PROCESSING_JMX_REQUEST_IOEXCEPTION(e).tag("org.apache.hadoop.jmx").error();

                        (218: 7) LOG.error("Caught an exception while processing JMX request", e);
                        LOG.EXCEPTION_WHILE_PROCESSING_JMX_REQUEST_MALFORMED_OBJECT_NAME(e).tag("org.apache.hadoop.jmx").error();

                        (227: 5) LOG.debug("Listing beans for "+qry);
                        LOG.LISTING_BEANS_FOR(qry).tag("org.apache.hadoop.jmx").debug();

                        (254: 11) LOG.error("getting attribute " + prs + " of " + oname + " threw an exception", e);
                        LOG.ATTRIBUTE_NOT_FOUND_EXCEPTION_GETTING_ATTRIBUTE_OF_OBJECT(e, prs, oname).tag("org.apache.hadoop.jmx").error();

                        (259: 11) LOG.error("getting attribute " + prs + " of " + oname + " threw an exception", e);
                        LOG.MBEAN_EXCEPTION_GETTING_ATTRIBUTE_OF_OBJECT(e, prs, oname).tag("org.apache.hadoop.jmx").error();

                        (265: 11) LOG.error("getting attribute " + prs + " of " + oname + " threw an exception", e);
                        LOG.RUNTIME_EXCEPTION_GETTING_ATTRIBUTE_OF_OBJECT(e, prs, oname).tag("org.apache.hadoop.jmx").error();

                        (271: 11) LOG.error("getting attribute " + prs + " of " + oname + " threw an exception", e);
                        LOG.REFLECTION_EXCEPTION_GETTING_ATTRIBUTE_OF_OBJECT(e, prs, oname).tag("org.apache.hadoop.jmx").error();

                        (280: 9) LOG.error("Problem while trying to process JMX query: " + qry + " with MBean " + oname, e);
                        LOG.PROBLEM_WHILE_TRYING_TO_PROCESS_JMX_QUERY_MBEAN(qry, oname, e).tag("org.apache.hadoop.jmx").error();

                        (286: 9) LOG.error("Problem while trying to process JMX query: " + qry + " with MBean " + oname, e);
                        LOG.PROBLEM_WHILE_TRYING_TO_PROCESS_JMX_QUERY_MBEAN(qry, oname, e).tag("org.apache.hadoop.jmx").error();

                        (338: 9) LOG.debug("getting attribute "+attName+" of "+oname+" threw an exception", e);
                        LOG.EXCEPTION_GETTING_ATTRIBUTE_OF_OBJECT(e, attName, oname).tag("org.apache.hadoop.jmx").tag("UnsupportedOperationException").debug();

                        (340: 9) LOG.error("getting attribute "+attName+" of "+oname+" threw an exception", e);
                        LOG.EXCEPTION_GETTING_ATTRIBUTE_OF_OBJECT(e, attName, oname).tag("org.apache.hadoop.jmx").debug();

                        (346: 7) LOG.debug("getting attribute "+attName+" of "+oname+" threw an exception", e);
                        LOG.EXCEPTION_GETTING_ATTRIBUTE_OF_OBJECT(e, attName, oname).tag("org.apache.hadoop.jmx").tag("RuntimeErrorException").debug();

                        (356: 7) LOG.error("getting attribute "+attName+" of "+oname+" threw an exception", e);
                        LOG.EXCEPTION_GETTING_ATTRIBUTE_OF_OBJECT(e, attName, oname).tag("org.apache.hadoop.jmx").tag("MBeanException").debug();

                        (361: 7) LOG.error("getting attribute "+attName+" of "+oname+" threw an exception", e);
                        LOG.EXCEPTION_GETTING_ATTRIBUTE_OF_OBJECT(e, attName, oname).tag("org.apache.hadoop.jmx").tag("RuntimeException").debug();

                        (366: 7) LOG.error("getting attribute "+attName+" of "+oname+" threw an exception", e);
                        LOG.EXCEPTION_GETTING_ATTRIBUTE_OF_OBJECT(e, attName, oname).tag("org.apache.hadoop.jmx").tag("ReflectionException").debug();

                org.apache.hadoop.metrics  (2 usages)
                    MetricsUtil.java  (2 usages)
                        (66: 7) LOG.error("Unable to create metrics context " + contextName, ex);
                        LOG.UNABLE_TO_CREATE_METRICS_CONTEXT(contextName, ex).tag("org.apache.hadoop.metrics").error();

                        (97: 7) LOG.info("Unable to obtain hostName", ex);
                        LOG.UNABLE_TO_OBTAIN_HOSTNAME(ex).tag("org.apache.hadoop.metrics").info();

                org.apache.hadoop.metrics.ganglia  (8 usages)
                    GangliaContext.java  (1 usage)
                        (154: 9) LOG.warn("Unknown metrics type: " + metric.getClass());
                        LOG.UNKNOWN_METRICS_TYPE(metric.getClass()).tag("org.apache.hadoop.metrics").warn();

                    GangliaContext31.java  (7 usages)
                        (50: 5) LOG.debug("Initializing the GangliaContext31 for Ganglia 3.1 metrics.");
                        LOG.INITIALIZING_GANGLIACONTEXT31_FOR_GANGLIA_3_1_METRICS().tag("org.apache.hadoop.metrics").warn();

                        (64: 9) LOG.error(uhe);
                        LOG.UNKNOWN_HOST_EXCEPTION(uhe).tag("org.apache.hadoop.metrics").error();

                        (75: 7) LOG.warn("Metric was emitted with no name.");
                        LOG.EMMITED_METRIC_WITH_NO_NAME().tag("org.apache.hadoop.metrics").warn();

                        (78: 7) LOG.warn("Metric name " + name +" was emitted with a null value.");
                        LOG.EMMITED_METRIC_WITH_NULL_VALUE(name).tag("org.apache.hadoop.metrics").warn();

                        (81: 7) LOG.warn("Metric name " + name + ", value " + value + " has no type.");
                        LOG.METRIC_HAS_NO_TYPE(name, value).tag("org.apache.hadoop.metrics").warn();

                        (85: 5) LOG.debug("Emitting metric " + name + ", type " + type + ", value " + value + " from hostname" + hostName);
                        LOG.EMMITING_METRIC_FROM_HOSTNAME(hostname, name, type, value).tag("org.apache.hadoop.metrics").debug();

                        (90: 7) LOG.warn("Metric name " + name + ", value " + value + " had 'null' units");
                        LOG.METRIC_CONTAINED_NULL_UNITS(name, value).tag("org.apache.hadoop.metrics").warn();

                org.apache.hadoop.metrics.jvm (2 usages)
                        (71: 13) log.info("Cannot initialize JVM Metrics with processName=" + processName + ", sessionId=" + sessionId + " - already initialized");
                        LOG.UNABLE_TO_INITIALIZE_JVM_METRICS_FOR_PROCESS_ALREADY_INITIALIZED_FOR_SESSION(sessionId, processName).tag("org.apache.hadoop.metrics").info();

                        (76: 13) log.info("Initializing JVM Metrics with processName=" + processName + ", sessionId=" + sessionId);
                        LOG.INITIALIZING_JVM_METRICS_WITH_PROCESS_SESSION(processName, sessionId).tag("org.apache.hadoop.metrics").info();

                org.apache.hadoop.metrics.spi  (4 usages)
                    CompositeContext.java  (4 usages)
                        (60: 7) LOG.error("Unable to initialize composite metric " + contextName +": could not init arity", e);
                        LOG.UNABLE_TO_INITIALIZE_ARITY_OF_COMPOSITE_METRIC(contextName, e).tag("org.apache.hadoop.metrics").error();

                        (94: 9) LOG.warn("emitRecord failed: " + ctxt.getContextName(), e);
                        LOG.ERROR_EMIT_RECORD_FAILED(ctxt.getContextName(), e).tag("org.apache.hadoop.metrics").warn();

                        (106: 9) LOG.warn("flush failed: " + ctxt.getContextName(), e);
                        LOG.ERROR_FLUSH_FAILED(ctxt.getContextname(), e).tag("org.apache.hadoop.metrics").warn();

                        (118: 9) LOG.warn("startMonitoring failed: " + ctxt.getContextName(), e);
                        LOG.ERROR_STARTMONITORING_FAILED(ctxt.getContextName(), e).tag("org.apache.hadoop.metrics").warn();

                org.apache.hadoop.metrics.util  (7 usages)
                    MetricsDynamicMBeanBase.java  (3 usages)
                        (115: 21) MetricsUtil.LOG.error("unknown metrics type: " + o.getClass().getName());
                        LOG.UNKNOWN_METRICS_TYPE(o.getClass().getName()).tag("org.apache.hadoop.metrics").error();

                        (163: 21) MetricsUtil.LOG.error("Unexpected attrubute suffix");
                        LOG.UNEXPECTED_ATTRIBUTE_SUFFIX().tag("org.apache.hadoop.metrics").error();

                        (167: 21) MetricsUtil.LOG.error("unknown metrics type: " + o.getClass().getName());
                        LOG.UNKNOWN_METRICS_TYPE(o.getClass().getName()).tag("org.apache.hadoop.metrics").tag("AttributeNotFoundException").error();

                    MetricsIntValue.java  (1 usage)
                        (99: 9) LOG.info("pushMetric failed for " + getName() + "\n", e);
                        LOG.ERROR_PUSHMETRIC_FAILED_FOR(getName(), e).tag("org.apache.hadoop.metrics").info();

                    MetricsTimeVaryingInt.java  (1 usage)
                        (108: 7) LOG.info("pushMetric failed for " + getName() + "\n" , e);
                        LOG.ERROR_PUSHMETRIC_FAILED_FOR(getName(), e).tag("org.apache.hadoop.metrics").info();

                    MetricsTimeVaryingLong.java  (1 usage)
                        (104: 7) LOG.info("pushMetric failed for " + getName() + "\n" , e);
                        LOG.ERROR_PUSHMETRIC_FAILED_FOR(getName(), e).tag("org.apache.hadoop.metrics").info();

                    MetricsTimeVaryingRate.java  (1 usage)
                        (152: 7) LOG.info("pushMetric failed for " + getName() + "\n" , e);
                        LOG.ERROR_PUSHMETRIC_FAILED_FOR(getName(), e).tag("org.apache.hadoop.metrics").info();

                org.apache.hadoop.metrics2.impl  (63 usages)
                    MBeanInfoBuilder.java  (1 usage)
                        (109: 23) MetricsSystemImpl.LOG.debug(attrs);
                        LOG.MBEAN_ATTRIBUTE_INFO_LIST(attrs).tag("org.apache.hadoop.metrics2").debug();

                    MetricsConfig.java  (9 usages)
                        (111: 9) LOG.info("loaded properties from "+ fname);
                        LOG.PROPERTIES_LOADED_FROM(fname).tag("org.apache.hadoop.metrics2").info();

                        (112: 9) LOG.debug(toString(cf));
                        LOG.CONFIGURATION_FILE(toString(cf)).tag("org.apache.hadoop.metrics2").debug();

                        (114: 9) LOG.debug(mc);
                        LOG.METRICS_CONFIG(mc).tag("org.apache.hadoop.metrics2").debug();

                        (123: 5) LOG.warn("Cannot locate configuration: tried "+ Joiner.on(",").join(fileNames));
                        LOG.UNABLE_TO_LOCATE_CONFIGURATION_TRIED(Joiner.on(",").join(fileNames)).tag("org.apache.hadoop.metrics2").warn();

                        (178: 9) LOG.debug("poking parent '"+ getParent().getClass().getSimpleName() + "' for key: "+ key);
                        LOG.POKING_PARENT_FOR_KEY(getParent().getClass().getSimpleName(), key).tag("org.apache.hadoop.metrics2").debug();

                        (185: 7) LOG.debug("returning '"+ value +"' for key: "+ key);
                        LOG.RETURNING_VALUE_FOR_KEY(value, key).tag("org.apache.hadoop.metrics2").debug();

                        (207: 5) LOG.debug(clsName);
                        LOG.CLASS_NAME(clsName).tag("org.apache.hadoop.metrics2").debug();

                        (226: 11) LOG.debug(jar);
                        LOG.JAR_PLUGIN_LOADER(jar).tag("org.apache.hadoop.metrics2").debug();

                        (233: 9) LOG.debug("using plugin jars: "+ Iterables.toString(jars));
                        LOG.USING_PLUGIN_JARS(Iterables.toString(jars)).tag("org.apache.hadoop.metrics2").debug();
-------------------
                    MetricsSinkAdapter.java  (11 usages)
                        (95: 7) LOG.debug("enqueue, logicalTime="+ logicalTime);
                        (107: 7) LOG.warn(name + " has a full queue and can't consume the given metrics.");
                        (112: 7) LOG.warn(name +
                        (132: 9) LOG.info(name +" thread interrupted.");
                        (138: 13) LOG.error("Got sink exception, retry in "+ awhile +"ms", e);
                        (143: 13) LOG.info(name +" thread interrupted while waiting for retry", e2);
                        (148: 13) LOG.error("Got sink exception and over retry limit, "+
                        (167: 15) LOG.debug("Pushing record "+ entry.name() +"."+ record.context() +
                        (185: 5) LOG.debug("Done");
                        (190: 5) LOG.info("Sink "+ name +" started");
                        (199: 7) LOG.warn("Stop interrupted", e);
                    MetricsSourceAdapter.java  (10 usages)
                        (108: 9) LOG.debug(attribute +": "+ a);
                        (129: 11) LOG.debug(key +": "+ attr);
                        (196: 7) LOG.error("Error getting metrics from source "+ name, e);
                        (215: 7) LOG.warn("MBean "+ name +" already initialized!");
                        (216: 7) LOG.debug("Stacktrace: ", new Throwable());
                        (220: 5) LOG.debug("MBean for source "+ name +" registered.");
                        (231: 5) LOG.debug("Updating info cache...");
                        (233: 5) LOG.debug("Done");
                        (237: 5) LOG.debug("Updating attr cache...");
                        (251: 5) LOG.debug("Done. # tags & metrics="+ numMetrics);
                    MetricsSystemImpl.java  (32 usages)
                        (144: 7) LOG.warn(this.prefix +" metrics system already initialized!");
                        (151: 7) LOG.info(this.prefix +" metrics system started (again)");
                        (160: 11) LOG.warn("Metrics system not started: "+ e.getMessage());
                        (161: 11) LOG.debug("Stacktrace: ", e);
                        (165: 9) LOG.info(prefix +" metrics system started in standby mode");
                        (175: 7) LOG.warn(prefix +" metrics system already started!",
                        (183: 5) LOG.info(prefix +" metrics system started");
                        (190: 7) LOG.warn(prefix +" metrics system not yet started!",
                        (196: 7) LOG.info(prefix +" metrics system stopped (again)");
                        (200: 5) LOG.info("Stopping "+ prefix +" metrics system...");
                        (206: 5) LOG.info(prefix +" metrics system stopped.");
                        (220: 5) LOG.debug(finalName +", "+ finalDesc);
                        (243: 5) LOG.debug("Registered source "+ name);
                        (248: 5) LOG.debug(name +", "+ description);
                        (250: 7) LOG.warn("Sink "+ name +" already exists!");
                        (275: 5) LOG.info("Registered sink "+ name);
                        (290: 15) LOG.warn("Caught exception in callback "+ method.getName(), e);
                        (325: 7) LOG.warn(prefix +" metrics system timer already started!");
                        (337: 15) LOG.warn(e);
                        (341: 5) LOG.info("Scheduled snapshot period at "+ period +" second(s).");
                        (387: 5) LOG.debug("Snapshotted source "+ sa.name());
                        (414: 7) LOG.warn(prefix +" metrics system timer already stopped!");
                        (424: 7) LOG.debug("Stopping metrics source "+ entry.getKey() +
                        (435: 7) LOG.debug("Stopping metrics sink "+ entry.getKey() +
                        (470: 9) LOG.warn("Error creating sink '"+ sinkName +"'", e);
                        (512: 7) LOG.error("Error getting localhost name. Using 'localhost'...", e);
                        (548: 5) LOG.debug("refCount="+ refCount);
                        (550: 7) LOG.debug("Redundant shutdown", new Throwable());
                        (557: 9) LOG.warn("Error stopping the metrics system", e);
                        (567: 5) LOG.info(prefix +" metrics system shutdown complete.");
                        (577: 5) LOG.debug("from system property: "+ System.getProperty(MS_INIT_MODE_KEY));
                        (578: 5) LOG.debug("from environment variable: "+ System.getenv(MS_INIT_MODE_KEY));
========================

                org.apache.hadoop.metrics2.lib  (11 usages)
                    Interns.java  (2 usages)
                        (50: 11) LOG.warn("Metrics intern cache overflow at "+ size() +" for "+ e);
                        (70: 15) LOG.warn("Metrics intern cache overflow at "+ size() +" for "+ e);
                    MethodMetric.java  (3 usages)
                        (80: 13) LOG.error("Error invoking method "+ method.getName(), ex);
                        (116: 13) LOG.error("Error invoking method "+ method.getName(), ex);
                        (132: 13) LOG.error("Error invoking method "+ method.getName(), ex);
                    MetricsSourceBuilder.java  (2 usages)
                        (108: 9) LOG.warn("Error accessing field "+ field, e);
                        (141: 9) LOG.warn("Error accessing field "+ field +" annotated with"+
                    MutableMetricsFactory.java  (2 usages)
                        (42: 7) LOG.debug("field "+ field +" with annotation "+ annotation);
                        (82: 7) LOG.debug("method "+ method +" with annotation "+ annotation);
                    MutableRates.java  (2 usages)
                        (59: 7) LOG.debug(name);
                        (62: 9) LOG.error("Error creating rate metrics for "+ method.getName(), e);
                org.apache.hadoop.metrics2.sink.ganglia  (12 usages)
                    AbstractGangliaSink.java  (4 usages)
                        (114: 5) LOG.debug("Initializing the GangliaSink for Ganglia metrics.");
                        (127: 9) LOG.error(uhe);
                        (146: 7) LOG.error(se);
                        (171: 11) LOG.error("Invalid propertylist for " + gtype.name());
                    GangliaSink30.java  (4 usages)
                        (227: 7) LOG.warn("Metric was emitted with no name.");
                        (230: 7) LOG.warn("Metric name " + name + " was emitted with a null value.");
                        (233: 7) LOG.warn("Metric name " + name + ", value " + value + " has no type.");
                        (238: 7) LOG.debug("Emitting metric " + name + ", type " + type + ", value "
                    GangliaSink31.java  (4 usages)
                        (52: 7) LOG.warn("Metric was emitted with no name.");
                        (55: 7) LOG.warn("Metric name " + name +" was emitted with a null value.");
                        (58: 7) LOG.warn("Metric name " + name + ", value " + value + " has no type.");
                        (63: 7) LOG.debug("Emitting metric " + name + ", type " + type + ", value " + value
                org.apache.hadoop.metrics2.util  (9 usages)
                    MBeans.java  (8 usages)
                        (59: 9) LOG.debug("Registered " + name);
                        (63: 11) LOG.trace("Failed to register MBean \"" + name + "\"", iaee);
                        (65: 11) LOG.warn("Failed to register MBean \"" + name
                        (69: 9) LOG.warn("Failed to register MBean \"" + name + "\"", e);
                        (76: 5) LOG.debug("Unregistering "+ mbeanName);
                        (79: 7) LOG.debug("Stacktrace: ", new Throwable());
                        (85: 7) LOG.warn("Error unregistering "+ mbeanName, e);
                        (95: 7) LOG.warn("Error creating MBean object name: "+ nameStr, e);
                    MetricsCache.java  (1 usage)
                        (59: 9) LOG.warn("Metrics cache overflow at "+ size() +" for "+ eldest);

--------------------
                org.apache.hadoop.net  (29 usages)
                    DNS.java  (5 usages)
                        (173: 7) LOG.warn("I/O error finding interface " + strInterface +
                        (243: 7) LOG.warn("Unable to determine hostname for interface " + strInterface);
                        (261: 7) LOG.warn("Unable to determine local hostname "
                        (284: 9) LOG.warn("Unable to determine address of the host"
                        (290: 11) LOG.error("Unable to determine local loopback address "
                    NetUtils.java  (3 usages)
                        (540: 7) LOG.info("Detected a loopback TCP socket, disconnecting it");
                        (783: 7) LOG.warn("Unable to wrap exception of type " +
                        (866: 7) LOG.error("Unable to get host interfaces", e);
                    NetworkTopology.java  (7 usages)
                        (404: 9) LOG.info("Adding a new node: "+NodeBase.getPath(node));
                        (413: 15) LOG.error("Error: can't add leaf node at depth " +
                        (423: 9) LOG.debug("NetworkTopology became:\n" + this.toString());
                        (459: 5) LOG.info("Removing a node: "+NodeBase.getPath(node));
                        (469: 9) LOG.debug("NetworkTopology became:\n" + this.toString());
                        (587: 7) LOG.warn("The cluster does not contain node: "+NodeBase.getPath(node1));
                        (591: 7) LOG.warn("The cluster does not contain node: "+NodeBase.getPath(node2));
                    NetworkTopologyWithNodeGroup.java  (4 usages)
                        (200: 9) LOG.info("Adding a new node: " + NodeBase.getPath(node));
                        (207: 9) LOG.debug("NetworkTopology became:\n" + this.toString());
                        (225: 5) LOG.info("Removing a node: "+NodeBase.getPath(node));
                        (239: 9) LOG.debug("NetworkTopology became:\n" + this.toString());
                    ScriptBasedMapping.java  (3 usages)
                        (189: 11) LOG.error("Script " + scriptName + " returned "
                        (219: 9) LOG.warn("Invalid value " + Integer.toString(maxArgs)
                        (244: 11) LOG.warn("Exception running " + s, e);
                    SocketIOWithTimeout.java  (3 usages)
                        (295: 13) LOG.warn("Unexpected exception while closing selector : ", e);
                        (367: 11) LOG.info("Unexpected Exception while clearing selector : ", e);
                        (398: 11) //LOG.info("Creating new ProviderInfo : " + provider.toString());
                    TableMapping.java  (4 usages)
                        (90: 9) LOG.warn(NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY + " not configured. "
                        (106: 15) LOG.warn("Line does not have two columns. Ignoring. " + line);
                        (112: 9) LOG.warn(filename + " cannot be read. " + NetworkTopology.DEFAULT_RACK
                        (120: 13) LOG.warn(filename + " cannot be read. "
                org.apache.hadoop.security  (62 usages)
                    Credentials.java  (1 usage)
                        (103: 7) LOG.warn("Null token ignored for " + alias);
                    Groups.java  (7 usages)
                        (66: 7) LOG.debug("Group mapping impl=" + impl.getClass().getName() +
                        (83: 9) LOG.debug("Returning cached groups for '" + user + "'");
                        (95: 7) LOG.debug("Returning fetched groups for '" + user + "'");
                        (104: 5) LOG.info("clearing userToGroupsMap cache");
                        (108: 7) LOG.warn("Error refreshing groups cache", e);
                        (122: 7) LOG.warn("Error caching groups", e);
                        (180: 9) LOG.debug(" Creating new Groups object");
                    JniBasedUnixGroupsMapping.java  (3 usages)
                        (51: 5) LOG.debug("Using JniBasedUnixGroupsMapping for Group resolution");
                        (61: 9) LOG.debug("Error getting groups for " + user, e);
                        (63: 9) LOG.info("Error getting groups for " + user + ": " + e.getMessage());
                    JniBasedUnixGroupsMappingWithFallback.java  (2 usages)
                        (40: 7) LOG.debug("Falling back to shell based");
                        (44: 7) LOG.debug("Group mapping impl=" + impl.getClass().getName());
                    JniBasedUnixGroupsNetgroupMapping.java  (3 usages)
                        (55: 5) LOG.debug("Using JniBasedUnixGroupsNetgroupMapping for Netgroup resolution");
                        (119: 9) LOG.debug("Error getting users for netgroup " + netgroup, e);
                        (121: 9) LOG.info("Error getting users for netgroup " + netgroup +
                    JniBasedUnixGroupsNetgroupMappingWithFallback.java  (2 usages)
                        (40: 7) LOG.info("Falling back to shell based");
                        (44: 7) LOG.debug("Group mapping impl=" + impl.getClass().getName());
                    LdapGroupsMapping.java  (1 usage)
                        (207: 7) LOG.warn("Exception trying to get groups for user " + user, e);
                    SaslInputStream.java  (1 usage)
                        (99: 9) LOG.debug("Actual length is " + length);
                    SaslRpcClient.java  (8 usages)
                        (107: 7) LOG.debug("Creating SASL " + mechanism
                        (153: 11) LOG.debug("Have sent token of size " + saslToken.length
                        (159: 13) LOG.debug("Server asks us to fall back to simple auth.");
                        (167: 11) LOG.debug("Will read input token of size " + saslToken.length
                        (176: 9) LOG.debug("SASL client context established. Negotiated QoP: "
                        (258: 11) LOG.debug("SASL client callback: setting username: " + userName);
                        (263: 11) LOG.debug("SASL client callback: setting userPassword");
                        (268: 11) LOG.debug("SASL client callback: setting realm: "
                    SaslRpcServer.java  (3 usages)
                        (224: 11) LOG.debug("SASL server DIGEST-MD5 callback: setting password "
                        (241: 13) LOG.debug("SASL server DIGEST-MD5 callback: setting "
                        (276: 13) LOG.debug("SASL server GSSAPI callback: setting "
                    SecurityUtil.java  (3 usages)
                        (400: 9) LOG.debug("Acquired token "+token);  // Token#toString() prints service
                        (403: 7) LOG.warn("Failed to get token for service "+service);
                        (449: 9) LOG.fatal("Exception while getting login user", e);
                    ShellBasedUnixGroupsMapping.java  (1 usage)
                        (86: 7) LOG.warn("got exception trying to get groups for user " + user, e);
                    ShellBasedUnixGroupsNetgroupMapping.java  (1 usage)
                        (141: 7) LOG.warn("error getting users for netgroup " + netgroup, e);
                    UserGroupInformation.java  (26 usages)
                        (126: 9) LOG.debug("hadoop login commit");
                        (131: 11) LOG.debug("using existing subject:"+subject.getPrincipals());
                        (140: 11) LOG.debug("using kerberos user:"+user);
                        (156: 11) LOG.debug("using local user:"+user);
                        (164: 7) LOG.error("Can't find user in " + subject);
                        (177: 9) LOG.debug("hadoop login");
                        (185: 9) LOG.debug("hadoop logout");
                        (342: 7) LOG.error("Unable to find JAAS classes:" + e.getMessage());
                        (613: 9) LOG.warn("found more than one principal in the ticket cache file " +
                        (672: 9) LOG.debug("UGI loginUser:"+loginUser);
                        (704: 11) LOG.debug("Found tgt " + ticket);
                        (739: 19) LOG.debug("Current time is " + now);
                        (740: 19) LOG.debug("Next refresh is " + nextRefresh);
                        (747: 19) LOG.debug("renewed ticket");
                        (752: 19) LOG.warn("No TGT after renewal. Aborting renew thread for " +
                        (759: 17) LOG.warn("Terminating renewal thread");
                        (762: 17) LOG.warn("Exception encountered while running the" +
                        (812: 5) LOG.info("Login successful for user " + keytabPrincipal
                        (871: 7) LOG.info("Initiating logout for " + getUserName());
                        (882: 9) LOG.info("Initiating re-login for " + keytabPrincipal);
                        (923: 7) LOG.info("Initiating logout for " + getUserName());
                        (933: 7) LOG.info("Initiating re-login for " + getUserName());
                        (993: 7) LOG.warn("Not attempting to re-login since the last re-login was " +
                        (1316: 7) LOG.warn("No groups available for user " + getShortUserName());
                        (1455: 7) LOG.error("PriviledgedActionException as:"+this+" cause:"+cause);
                        (1474: 7) LOG.debug("PrivilegedAction as:"+this+" from:"+where);
                org.apache.hadoop.security.authorize  (2 usages)
                    ServiceAuthorizationManager.java  (2 usages)
                        (105: 12) AUDITLOG.warn(AUTHZ_FAILED_FOR + user + " for protocol=" + protocol
                        (111: 10) AUDITLOG.info(AUTHZ_SUCCESSFUL_FOR + user + " for protocol="+protocol);
                org.apache.hadoop.security.ssl  (6 usages)
                    FileBasedKeyStoresFactory.java  (4 usages)
                        (155: 7) LOG.debug(mode.toString() + " KeyStore: " + keystoreLocation);
                        (163: 7) LOG.debug(mode.toString() + " Loaded KeyStore: " + keystoreLocation);
                        (197: 5) LOG.debug(mode.toString() + " TrustStore: " + truststoreLocation);
                        (204: 5) LOG.debug(mode.toString() + " Loaded TrustStore: " + truststoreLocation);
                    ReloadingX509TrustManager.java  (2 usages)
                        (167: 7) LOG.debug("Loaded truststore '" + file + "'");
                        (197: 11) LOG.warn("Could not load truststore (keep using existing one) : " +
                org.apache.hadoop.security.token  (2 usages)
                    Token.java  (2 usages)
                        (129: 7) LOG.warn("Cannot find class for token kind " + kind);
                        (352: 5) LOG.warn("No TokenRenewer defined for token kind " + this.kind);
                org.apache.hadoop.security.token.delegation  (8 usages)
                    AbstractDelegationTokenSecretManager.java  (8 usages)
                        (153: 5) LOG.info("Updating the current master key for generating delegation tokens");
                        (210: 5) LOG.info("Creating password for identifier: " + identifier);
                        (262: 5) LOG.info("Token renewal requested for identifier: "+id);
                        (314: 5) LOG.info("Token cancelation requested for identifier: "+id);
                        (379: 7) LOG.debug("Stopping expired delegation token remover thread");
                        (409: 7) LOG.info("Starting expired delegation token remover thread, "
                        (420: 15) LOG.error("Master key updating failed: ", e);
                        (436: 9) LOG.error("ExpiredTokenRemover thread received unexpected exception. "
                org.apache.hadoop.util  (28 usages)
                    AsyncDiskService.java  (4 usages)
                        (112: 5) LOG.info("Shutting down all AsyncDiskService threads...");
                        (137: 9) LOG.warn("AsyncDiskService awaitTermination timeout.");
                        (141: 5) LOG.info("All AsyncDiskService threads are terminated.");
                        (150: 5) LOG.info("Shutting down all AsyncDiskService threads immediately...");
                    ExitUtil.java  (2 usages)
                        (84: 5) LOG.info("Exiting with status " + status);
                        (87: 7) LOG.fatal("Terminate called", ee);
                    GenericOptionsParser.java  (3 usages)
                        (324: 9) LOG.debug("setting conf tokensFile: " + fileName);
                        (350: 9) LOG.warn("The libjars file " + tmp + " is not on the local " +
                        (424: 7) LOG.warn("options parsing failed: "+e.getMessage());
                    HostsFileReader.java  (4 usages)
                        (67: 15) LOG.info("Adding " + nodes[i] + " to the list of hosts from " + filename);
                        (82: 5) LOG.info("Refreshing hosts (include/exclude) list");
                        (106: 5) LOG.info("Setting the includes file to " + includesFile);
                        (111: 5) LOG.info("Setting the excludes file to " + excludesFile);
                    NativeCodeLoader.java  (5 usages)
                        (46: 7) LOG.debug("Trying to load the custom-built native-hadoop library...");
                        (50: 7) LOG.debug("Loaded the native-hadoop library");
                        (55: 9) LOG.debug("Failed to load native-hadoop with error: " + t);
                        (56: 9) LOG.debug("java.library.path=" +
                        (62: 7) LOG.warn("Unable to load native-hadoop library for your platform... " +
                    Progress.java  (1 usage)
                        (101: 7) LOG.warn("Sum of weightages can not be more than 1.0; But sum = " + sum);
                    Shell.java  (4 usages)
                        (176: 11) LOG.warn("Error reading the error stream", ioe);
                        (196: 9) LOG.warn("Interrupted while reading the error stream", ie);
                        (214: 9) LOG.warn("Error while closing the input stream", ioe);
                        (222: 9) LOG.warn("Error while closing the error stream", ioe);
                    ShutdownHookManager.java  (1 usage)
                        (56: 15) LOG.warn("ShutdownHook '" + hook.getClass().getSimpleName() +
                    StringUtils.java  (2 usages)
                        (576: 5) LOG.info(
                        (595: 11) LOG.info(toStartupShutdownString("SHUTDOWN_MSG: ", new String[]{
                    ThreadUtil.java  (1 usage)
                        (45: 9) LOG.warn("interrupted while sleeping", ie);
                    VersionInfo.java  (1 usage)
                        (153: 5) LOG.debug("version: "+ getVersion());
            hadoop-datajoin  (2 usages)
                org.apache.hadoop.contrib.utils.join  (2 usages)
                    DataJoinReducerBase.java  (1 usage)
                        (117: 7) LOG.info("key: " + key.toString() + " this.largestNumOfValues: "
                    JobBase.java  (1 usage)
                        (138: 5) LOG.info(getReport());
            hadoop-distcp  (61 usages)
                org.apache.hadoop.tools  (18 usages)
                    DistCp.java  (10 usages)
                        (109: 7) LOG.info("Input Options: " + inputOptions);
                        (111: 7) LOG.error("Invalid arguments: ", e);
                        (120: 7) LOG.error("Invalid input: ", e);
                        (123: 7) LOG.error("Duplicate files in input path: ", e);
                        (126: 7) LOG.error("Exception encountered ", e);
                        (164: 5) LOG.info("DistCp job-id: " + jobID);
                        (308: 7) LOG.info("DistCp job log path: " + logPath);
                        (356: 7) LOG.debug("Meta folder location: " + metaFolderPath);
                        (377: 7) LOG.error("Couldn't complete DistCp operation: ", e);
                        (401: 7) LOG.error("Unable to cleanup meta folder: " + metaFolder, e);
                    OptionsParser.java  (3 usages)
                        (42: 9) LOG.debug("Adding option " + option.getOption());
                        (216: 7) LOG.warn(DistCpOptionSwitch.FILE_LIMIT.getSwitch() + " is a deprecated" +
                        (230: 7) LOG.warn(DistCpOptionSwitch.SIZE_LIMIT.getSwitch() + " is a deprecated" +
                    SimpleCopyListing.java  (5 usages)
                        (128: 15) LOG.debug("Recording source-path: " + sourceStatus.getPath() + " for copy.");
                        (134: 17) LOG.debug("Traversing non-empty source dir: " + sourceStatus.getPath());
                        (222: 11) LOG.debug("Recording source-path: "
                        (227: 13) LOG.debug("Traversing non-empty source dir: "
                        (242: 7) LOG.debug("REL PATH: " + DistCpUtils.getRelativePath(sourcePathRoot,
                org.apache.hadoop.tools.mapred  (29 usages)
                    CopyCommitter.java  (14 usages)
                        (119: 7) LOG.warn("Unable to cleanup temp files", t);
                        (132: 9) LOG.info("Cleaning up " + file.getPath());
                        (147: 7) LOG.info("Cleaning up temporary work folder: " + metaFolder);
                        (150: 7) LOG.error("Exception encountered ", ignore);
                        (158: 5) LOG.info("About to preserve attributes: " + attrSymbols);
                        (199: 5) LOG.info("Preserved status on " + preservedEntries + " dir entries on target");
                        (205: 5) LOG.info("-delete option is enabled. About to remove entries from " +
                        (254: 11) LOG.info("Deleted " + trgtFileStatus.getPath() + " - Missing at source");
                        (267: 5) LOG.info("Deleted " + deletedEntries + " from target: " + targets.get(0));
                        (276: 5) LOG.info("Atomic commit enabled. Moving " + workDir + " to " + finalDir);
                        (278: 7) LOG.error("Pre-existing final-path found at: " + finalDir);
                        (285: 7) LOG.warn("Rename failed. Perhaps data already moved. Verifying...");
                        (289: 7) LOG.info("Data committed successfully to " + finalDir);
                        (292: 7) LOG.error("Unable to commit data to " + finalDir);
                    CopyMapper.java  (8 usages)
                        (113: 5) LOG.info("Initializing SSL configuration");
                        (122: 7) LOG.warn("SSL Client config file not found. Was looking for " + sslConfFileName +
                        (145: 7) LOG.warn("Unable to write out the ssl configuration. " +
                        (180: 7) LOG.debug("DistCpMapper::map(): Received " + sourcePath + ", " + relPath);
                        (191: 5) LOG.info(description);
                        (209: 11) LOG.debug("Path could not be found: " + target, ignore);
                        (223: 9) LOG.info("Skipping copy of " + sourceCurrStatus.getPath()
                        (290: 5) LOG.error("Failure in copying " + sourceFileStatus.getPath() + " to " +
                    RetriableFileCopyCommand.java  (3 usages)
                        (97: 9) LOG.debug("Copying " + sourceFileStatus.getPath() + " to " + target);
                        (98: 9) LOG.debug("Tmp-file path: " + tmpTargetPath);
                        (167: 5) LOG.info("Creating temp file: " +
                    UniformSizeInputFormat.java  (4 usages)
                        (88: 7) LOG.debug("Average bytes per map: " + nBytesPerSplit +
                        (101: 13) LOG.debug ("Creating split : " + split + ", bytes in split: " + currentSplitSize);
                        (114: 11) LOG.info ("Creating split : " + split + ", bytes in split: " + currentSplitSize);
                        (148: 7) LOG.error("Couldn't find listing file at: " + listingFilePath, exception);
                org.apache.hadoop.tools.mapred.lib  (10 usages)
                    DynamicInputChunk.java  (5 usages)
                        (139: 7) LOG.warn(chunkFilePath + " could not be assigned to " + taskId);
                        (182: 7) LOG.info("Acquiring pre-assigned chunk: " + acquiredFilePath);
                        (188: 9) LOG.info(taskId + " acquired " + chunkFile.getPath());
                        (192: 9) LOG.warn(taskId + " could not acquire " + chunkFile.getPath());
                        (207: 7) LOG.error("Unable to release chunk at path: " + chunkFilePath);
                    DynamicInputFormat.java  (3 usages)
                        (73: 5) LOG.info("DynamicInputFormat: Getting splits for job:"
                        (167: 5) LOG.info("Number of dynamic-chunk-files created: " + chunksFinal.size());
                        (257: 7) LOG.warn("nMaps == 1. Why use DynamicInputFormat?");
                    DynamicRecordReader.java  (2 usages)
                        (100: 9) LOG.debug(taskId + ": RecordReader is null. No records to be read.");
                        (110: 7) LOG.debug(taskId + ": Current chunk exhausted. " +
                org.apache.hadoop.tools.util  (4 usages)
                    DistCpUtils.java  (3 usages)
                        (60: 7) LOG.debug("Retrieving file size for: " + path);
                        (299: 7) LOG.error("Unable to retrieve checksum for " + source + " or " + target, e);
                        (325: 11) LOG.debug("Could not compare file-systems. Unknown host: ", ue);
                    RetriableCommand.java  (1 usage)
                        (89: 9) LOG.error("Failure in Retriable command: " + description, exception);
            hadoop-extras  (40 usages)
                org.apache.hadoop.tools  (40 usages)
                    DistCh.java  (9 usages)
                        (267: 7) LOG.info("numSplits="  + numSplits + ", splits.size()=" + splits.size());
                        (312: 9) LOG.info(s);
                        (382: 7) LOG.info("ops=" + ops);
                        (383: 7) LOG.info("isIgnoreFailures=" + isIgnoreFailures);
                        (411: 7) LOG.error("Input error:", e);
                        (414: 7) LOG.error(NAME + " failed: ", e);
                        (444: 5) LOG.info(JOB_DIR_LABEL + "=" + jobdir);
                        (450: 5) LOG.info("log=" + log);
                        (493: 5) LOG.info(OP_COUNT_LABEL + "=" + opCount);
                    DistCpV1.java  (24 usages)
                        (451: 9) LOG.info("Copying file " + srcPath + " of size " +
                        (597: 11) LOG.warn("Copy of " + srcstat.getPath() + " failed.", e);
                        (599: 13) LOG.info("Retrying copy of file " + srcstat.getPath());
                        (658: 9) LOG.info(sfailure);
                        (663: 11) LOG.error(s);
                        (675: 15) LOG.debug("Ignoring cleanup exception", ex);
                        (772: 5) LOG.info("srcPaths=" + args.srcs);
                        (774: 7) LOG.info("destPath=" + args.dst);
                        (899: 9) LOG.trace("this = " + this);
                        (1113: 9) LOG.warn("Could not fully delete " + tmp);
                        (1392: 19) LOG.trace("adding file " + child.getPath());
                        (1408: 19) LOG.trace("skipping file " + child.getPath());
                        (1437: 5) LOG.info("sourcePathsCount(files+directories)=" + srcCount);
                        (1438: 5) LOG.info("filesToCopyCount=" + fileCount);
                        (1439: 5) LOG.info("bytesToCopyCount=" + StringUtils.humanReadableInt(byteCount));
                        (1441: 7) LOG.info("filesToSkipCopyCount=" + skipFileCount);
                        (1442: 7) LOG.info("bytesToSkipCopyCount=" +
                        (1456: 7) LOG.info(args.dst + " does not exist.");
                        (1472: 7) LOG.info("deletedPathsFromDestCount(files+directories)=" +
                        (1485: 5) LOG.info("sourcePathsCount=" + srcCount);
                        (1486: 5) LOG.info("filesToCopyCount=" + fileCount);
                        (1487: 5) LOG.info("bytesToCopyCount=" + StringUtils.humanReadableInt(byteCount));
                        (1521: 7) LOG.debug("Skipping the CRC check");
                        (1688: 9) LOG.warn(StringUtils.stringifyException(ioe));
                    Logalyzer.java  (7 usages)
                        (173: 9) LOG.fatal("Caught " + ioe);
                        (297: 5) LOG.info("analysisDir = " + outputDirectory);
                        (298: 5) LOG.info("archiveDir = " + archiveDir);
                        (299: 5) LOG.info("logListURI = " + logListURI);
                        (300: 5) LOG.info("grepPattern = " + grepPattern);
                        (301: 5) LOG.info("sortColumns = " + sortColumns);
                        (302: 5) LOG.info("separator = " + columnSeparator);
            hadoop-gridmix  (131 usages)
                org.apache.hadoop.mapred.gridmix  (131 usages)
                    ClusterSummarizer.java  (1 usage)
                        (62: 7) LOG.info("Error in processing cluster status at "
                    CompressionEmulationUtil.java  (6 usages)
                        (159: 7) LOG.error("Error while adding input path ", e);
                        (275: 7) LOG.info("GridMix is configured to generate compressed input data with "
                        (335: 5) LOG.info("Gridmix is configured to use compressed input data.");
                        (337: 5) LOG.info("Total size of compressed input data : "
                        (339: 5) LOG.info("Total number of compressed input data files : "
                        (358: 7) LOG.info("Input Data Compression Ratio : " + ratio);
                    DistributedCacheEmulator.java  (6 usages)
                        (172: 9) LOG.warn("Gridmix will not emulate Distributed Cache load because "
                        (177: 9) LOG.warn("Gridmix will not emulate Distributed Cache load because "
                        (190: 15) LOG.warn("Gridmix will not emulate Distributed Cache load "
                        (207: 7) LOG.warn("Gridmix will not emulate Distributed Cache load because "
                        (456: 5) LOG.info("Number of HDFS based distributed cache files to be generated is "
                        (461: 7) LOG.error("Missing " + fileCount + " distributed cache files under the "
                    EchoUserResolver.java  (1 usage)
                        (35: 5) LOG.info(" Current user resolver is EchoUserResolver ");
                    FilePool.java  (1 usage)
                        (228: 7) LOG.debug(size + " bytes in " + thisDir.getPath());
                    GenerateData.java  (3 usages)
                        (162: 5) LOG.info("Total size of input data : "
                        (164: 5) LOG.info("Total number of input data files : " + fileCount);
                        (198: 11) LOG.error("Error while adding input path ", e);
                    GenerateDistCacheData.java  (1 usage)
                        (110: 11) LOG.error("Error while adding input path ", e);
                    Gridmix.java  (25 usages)
                        (185: 9) LOG.error("Gridmix input data directory " + inputDir
                        (194: 7) LOG.info("Generating " + StringUtils.humanReadableInt(genbytes) +
                        (200: 9) LOG.info("Changing the permissions for inputPath " + inputDir.toString());
                        (203: 9) LOG.error("Couldnt change the file permissions " , e);
                        (207: 7) LOG.info("Input data generation successful.");
                        (225: 7) LOG.info("Generating distributed cache data of size " + conf.getLong(
                        (296: 7) LOG.info(" Submission policy is " + policy.name());
                        (328: 7) LOG.error(" Exception at start " ,e);
                        (389: 7) LOG.error("Too few arguments to Gridmix.\n");
                        (406: 13) LOG.error("size of input data to be generated specified using "
                        (413: 11) LOG.error("Unknown option " + argv[i] + " specified.\n");
                        (422: 13) LOG.warn("Ignoring the user resource '" + userRsrc + "'.");
                        (425: 11) LOG.error(userResolver.getClass()
                        (431: 9) LOG.warn("Ignoring the user resource '" + userRsrc + "'.");
                        (437: 7) LOG.error(e.toString() + "\n");
                        (457: 9) LOG.error("Failed creation of <ioPath> directory " + ioPath + "\n");
                        (528: 9) LOG.error("Startup failed. " + e.toString() + "\n");
                        (543: 11) LOG.error("Error in trace", badTraceException);
                        (644: 9) LOG.warn("Interrupted waiting for " + component);
                        (651: 7) LOG.info("Exiting...");
                        (665: 9) LOG.info("Killing running jobs...");
                        (671: 15) LOG.info("Killed " + job.getJobName() + " (" + job.getJobID() + ")");
                        (680: 13) LOG.warn("Failure killing " + job.getJobName(), e);
                        (682: 13) LOG.error("Unexcpected exception", e);
                        (685: 9) LOG.info("Done.");
                    GridmixJob.java  (1 usage)
                        (250: 7) LOG.debug("For the job configuration parameter '" + jobValueKey
                    InputStriper.java  (1 usage)
                        (59: 7) LOG.warn("Using " + inputBytes + "/" + mapBytes + " bytes");
                    JobFactory.java  (2 usages)
                        (108: 7) LOG.debug(" The submission thread name is " + rThread.getName());
                        (210: 9) LOG.debug("Ignoring job " + job.getJobID() + " from the input trace."
                    JobMonitor.java  (7 usages)
                        (102: 5) LOG.info("Job submission failed notification for job " + jobID);
                        (112: 5) LOG.info(job.getJobName() + " (" + job.getJobID() + ")" + " success");
                        (119: 5) LOG.info(job.getJobName() + " (" + job.getJobID() + ")" + " failure");
                        (193: 17) LOG.debug("Status polling for job " + job.getJobID() + " took "
                        (214: 21) LOG.error("Lost job " + (null == job.getJobName()
                        (229: 17) LOG.warn("Lost job " + (null == job.getJobName()
                        (251: 11) LOG.warn("Unexpected exception: ", e);
                    JobSubmitter.java  (10 usages)
                        (93: 11) LOG.info("[JobSubmitter] Time taken to build splits for job "
                        (96: 11) LOG.warn("Failed to submit " + job.getJob().getJobName() + " as "
                        (101: 11) LOG.warn("Failed to submit " + job.getJob().getJobName() + " as "
                        (117: 11) LOG.info("[JobSubmitter] Time taken to submit the job "
                        (131: 13) LOG.debug("Original job '" + jobID + "' is being simulated as '"
                        (133: 13) LOG.debug("SUBMIT " + job + "@" + System.currentTimeMillis()
                        (137: 11) LOG.warn("Failed to submit " + job.getJob().getJobName() + " as "
                        (145: 11) LOG.warn("Failed to submit " + job.getJob().getJobName(), e);
                        (156: 9) LOG.info(" Job " + job.getJob().getJobID() + " submission failed " , e);
                        (171: 7) LOG.info("Total number of queued jobs: "
                    LoadJob.java  (17 usages)
                        (203: 7) LOG.info("Resource usage matcher thread started.");
                        (217: 9) LOG.info("Resource usage emulation complete! Matcher exiting");
                        (219: 9) LOG.info("Exception while running the resource-usage-emulation matcher"
                        (248: 7) LOG.info("Status reporter thread started.");
                        (260: 9) LOG.info("Status reporter thread exiting");
                        (262: 9) LOG.info("Exception while running the status reporter thread!", e);
                        (302: 11) LOG.info("GridMix is configured to use a compression ratio of "
                        (344: 11) LOG.info("GridMix is configured to use a compression ratio of "
                        (396: 11) LOG.debug("Error in resource usage emulation! Message: ", e);
                        (404: 7) LOG.info("Starting the cleanup phase.");
                        (418: 13) LOG.debug("Error in resource usage emulation! Message: ", e);
                        (428: 9) LOG.info("Boosting the map phase progress.");
                        (473: 9) LOG.info("Spec output bytes w/o records. Using input record count");
                        (484: 9) LOG.info("GridMix is configured to use a compression ratio of "
                        (519: 13) LOG.debug("Error in resource usage emulation! Message: ", e);
                        (536: 11) LOG.debug("Error in resource usage emulation! Message: ", e);
                        (638: 11) LOG.debug(String.format("SPEC(%d) %d -> %d %d %d %d %d %d %d", id(), i,
                    RandomTextDataGenerator.java  (2 usages)
                        (108: 7) LOG.debug("Random text data generator is configured to use a dictionary "
                        (127: 7) LOG.debug("Random text data generator is configured to use a dictionary "
                    ReplayJobFactory.java  (2 usages)
                        (82: 9) LOG.info("START REPLAY @ " + initTime);
                        (96: 15) LOG.warn("Job " + job.getJobID() + " out of order");
                    RoundRobinUserResolver.java  (1 usage)
                        (88: 11) LOG.error("Error while creating a proxy user " ,ioe);
                    SerialJobFactory.java  (6 usages)
                        (87: 9) LOG.info("START SERIAL @ " + System.currentTimeMillis());
                        (97: 15) LOG.debug(
                        (108: 15) LOG.info(" Submitted the job " + prevJob);
                        (127: 19) LOG.error(
                        (133: 19) LOG.info(" job " + job.getName() + " completed ");
                        (175: 5) LOG.info(" Starting Serial submission ");
                    SleepJob.java  (4 usages)
                        (203: 7) LOG.info(msg);
                        (266: 11) LOG.info(msg);
                        (367: 7) LOG.warn("No sucessful attempts tasktype " + type +" task "+ task);
                        (389: 11) LOG.debug(
                    Statistics.java  (6 usages)
                        (129: 7) LOG.info("Not tracking job " + stats.getJob().getJobName()
                        (153: 7) LOG.error("[Statistics] Missing entry for job "
                        (166: 9) LOG.debug(
                        (216: 9) LOG.error(
                        (226: 13) LOG.error("Statistics interrupt while waiting for completion of "
                        (241: 13) LOG.error(
                    StressJobFactory.java  (27 usages)
                        (159: 11) LOG.warn("[STRESS] Interrupted before start!. Exiting..");
                        (162: 9) LOG.info("START STRESS @ " + System.currentTimeMillis());
                        (168: 17) LOG.debug("Updating the overload status.");
                        (173: 17) LOG.warn("[STRESS] Check failed!", ioe);
                        (180: 19) LOG.debug("[STRESS] Cluster overloaded in run! Sleeping...");
                        (187: 19) LOG.warn("[STRESS] Interrupted while sleeping! Exiting.", ie);
                        (195: 17) LOG.debug("[STRESS] Cluster underloaded in run! Stressing...");
                        (201: 19) LOG.warn("[STRESS] Finished consuming the input trace. "
                        (206: 19) LOG.debug("Job Selected: " + job.getJobID());
                        (236: 17) LOG.error("[STRESS] Error while submitting the job ", e);
                        (247: 9) LOG.error("[STRESS] Interrupted in the main block!", e);
                        (275: 7) LOG.error("Couldn't get the new Status",e);
                        (307: 9) LOG.debug(System.currentTimeMillis() + " [JobLoad] Overloaded is "
                        (334: 7) LOG.debug("Total submitted map tasks: " + totalMapTasks);
                        (335: 7) LOG.debug("Total submitted reduce tasks: " + totalReduceTasks);
                        (336: 7) LOG.debug("Max map load: " + maxMapLoad);
                        (337: 7) LOG.debug("Max reduce load: " + maxReduceLoad);
                        (365: 11) LOG.warn("Ignoring blacklisted job: " + id);
                        (381: 13) LOG.warn("Blacklisting completed job: " + id);
                        (405: 15) LOG.debug("Terminating overload check due to high map load.");
                        (424: 15) LOG.debug("Terminating overload check due to high reduce load.");
                        (429: 11) LOG.warn("Blacklisting empty job: " + id);
                        (444: 9) LOG.debug("Blacklisted jobs count: " + blacklistedJobs.size());
                        (454: 9) LOG.debug(System.currentTimeMillis() + " [MAP-LOAD] Overloaded is "
                        (463: 9) LOG.debug(System.currentTimeMillis() + " [REDUCE-LOAD] Overloaded is "
                        (471: 7) LOG.debug(System.currentTimeMillis() + " [OVERALL] Overloaded is "
                        (606: 5) LOG.info(" Starting Stress submission ");
                    SubmitterUserResolver.java  (1 usage)
                        (36: 5) LOG.info(" Current user resolver is SubmitterUserResolver ");
            hadoop-hdfs  (1,180 usages)
                /home/michalxo/IdeaProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs  (1 usage)
                    CHANGES.txt  (1 usage)
                        (4281: 56) HDFS-1320. Add LOG.isDebugEnabled() guard for each LOG.debug(..).
                /home/michalxo/IdeaProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/test  (7 usages)
                    TestFuseDFS.java  (7 usages)
                        (90: 5) LOG.debug("EXEC "+cmd);
                        (101: 5) LOG.debug("EXEC "+cmd);
                        (107: 5) LOG.debug("EXEC "+cmd);
                        (113: 5) LOG.debug("EXEC "+cmd);
                        (153: 11) LOG.error("FUSE_LINE:" + line);
                        (173: 5) LOG.debug("LD_LIBRARY_PATH=" + lp);
                        (208: 5) LOG.info("now mounting with:" + cmdStr);
                /home/michalxo/IdeaProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/site/apt  (1 usage)
                    FaultInjectFramework.apt.vm  (1 usage)
                        (147: 11)           LOG.info("Before the injection point");
                /home/michalxo/IdeaProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/aop/org/apache/hadoop/fi  (17 usages)
                    DataTransferTestUtil.java  (9 usages)
                        (121: 18) FiTestUtil.LOG.info("FI: pipeline not found; id=" + id
                        (208: 20) FiTestUtil.LOG.info(s);
                        (230: 20) FiTestUtil.LOG.info(s);
                        (248: 20) FiTestUtil.LOG.info(s);
                        (269: 20) FiTestUtil.LOG.info(s);
                        (296: 20) FiTestUtil.LOG.info(s);
                        (349: 20) FiTestUtil.LOG.info(toString(id));
                        (397: 20) FiTestUtil.LOG.info(s);
                        (434: 20) FiTestUtil.LOG.info(this + ", successfully verified.");
                    FiHFlushTestUtil.java  (1 usage)
                        (53: 20) FiTestUtil.LOG.info(s);
                    FiTestUtil.java  (5 usages)
                        (37: 7) LOG.info(Thread.currentThread() + ": seed=" + seed);
                        (79: 5) LOG.info("Sleep " + ms + " ms");
                        (83: 7) LOG.info("Sleep is interrupted", e);
                        (95: 5) LOG.info(Thread.currentThread().getName() + " sleeps for " + n  +"ms");
                        (195: 7) LOG.info("Marking this " + this);
                    ProbabilityModel.java  (2 usages)
                        (61: 5) LOG.info(ALL_PROBABILITIES + "=" + conf.get(ALL_PROBABILITIES));
                        LOG.ALL_PROBABILITIES(conf.get(ALL_PROBABILITIES)).tag("org.apache.hadoop.Common").tag("hadoop-hdfs").info();

                        (99: 7) LOG.debug("Request for " + newProbName + " returns=" + ret);
                /home/michalxo/IdeaProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/aop/org/apache/hadoop/fs  (6 usages)
                    TestFiListPath.java  (3 usages)
                        (87: 5) LOG.info("Test Target Delete For listStatus");
                        (93: 7) LOG.info(StringUtils.stringifyException(e));
                        (100: 5) LOG.info("Test Target Delete For listLocatedStatus");
                    TestFiRename.java  (3 usages)
                        (214: 9) LOG.warn("Exception " + " count " + count + " " + e.getMessage());
                        (250: 9) LOG.warn("Exception " + " count " + count + " " + e.getMessage());
                        (265: 7) LOG.warn("Exception ", e);
                /home/michalxo/IdeaProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/aop/org/apache/hadoop/hdfs  (29 usages)
                    DFSClientAspects.aj  (5 usages)
                        (48: 5) LOG.info("FI: after pipelineInitNonAppend: hasError="
                        (63: 5) LOG.info("FI: after pipelineInitAppend: hasError=" + datastreamer.hasError
                        (73: 5) LOG.info("FI: before pipelineErrorAfterInit: errorIndex="
                        (86: 5) LOG.info("FI: before pipelineClose:");
                        (98: 7) LOG.debug("FI: Recording packet # " + cp.seqno
                    HFlushAspects.aj  (4 usages)
                        (43: 5) LOG.info("FI: hflush for any datanode");
                        (44: 5) LOG.info("FI: hflush " + thisJoinPoint.getThis());
                        (47: 9) LOG.info("No pipeline is built");
                        (52: 9) LOG.info("No test has been initialized");
                    PipelinesTestUtil.java  (2 usages)
                        (68: 24) FiTestUtil.LOG.debug("FI: before compare of Recv bytes. Expected "
                        (107: 24) FiTestUtil.LOG.debug(
                    TestFiHFlush.java  (1 usage)
                        (50: 16) FiTestUtil.LOG.info("Running " + methodName + " ...");
                    TestFiHftp.java  (6 usages)
                        (143: 9) LOG.info("bytesRead=" + bytesRead);
                        (162: 5) LOG.info("dn=" + dn + ", blk=" + blk + " (length=" + blk.getNumBytes() + ")");
                        (168: 5) LOG.info("hftpfs.getUri() = " + hftpfs.getUri());
                        (170: 5) LOG.info("hftpfs.getContentSummary = " + cs);
                        (181: 7) LOG.info("GOOD: get an exception", ioe);
                        (183: 7) LOG.info("bytesRead=" + bytesRead);
                    TestFiPipelines.java  (11 usages)
                        (82: 7) LOG.debug("Running " + METHOD_NAME);
                        (105: 7) LOG.debug("Running " + METHOD_NAME);
                        (134: 7) LOG.debug("Running " + METHOD_NAME);
                        (158: 11) LOG.debug("_06(): " + cnt++ + " sending another " +
                        (164: 7) LOG.warn("Getting unexpected exception: ", e);
                        (167: 7) LOG.debug("Last queued packet number " + pipst.getLastQueued());
                        (186: 11) LOG.debug("_06: checking for the limit " + test.getLastQueued() +
                        (191: 13) LOG.debug("FI: Resume packets acking");
                        (199: 15) LOG.debug("_06: MAX isn't reached yet. Current=" +
                        (209: 11) LOG.debug("_06: shutting down the cluster");
                        (220: 9) LOG.debug("End QueueChecker thread");
                /home/michalxo/IdeaProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/aop/org/apache/hadoop/hdfs/protocol  (1 usage)
                    ClientProtocolAspects.aj  (1 usage)
                        (35: 7) LOG.info("FI: addBlock "
                /home/michalxo/IdeaProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/aop/org/apache/hadoop/hdfs/server/datanode  (36 usages)
                    BlockReceiverAspects.aj  (17 usages)
                        (47: 5) LOG.info("FI: getReceiver() " + getClass().getName());
                        (58: 5) LOG.info("FI: callReceivePacket, datanode=" + dnName);
                        (64: 7) LOG.info("Before the injection point");
                        (76: 5) LOG.info("FI: callWritePacketToDisk");
                        (102: 5) LOG.debug("FI: Received bytes To: " + br.datanode.getStorageId() + ": " + offset);
                        (105: 7) LOG.debug("FI: no pipeline has been found in receiving");
                        (115: 7) LOG.fatal("FI: no exception is expected here!");
                        (129: 7) LOG.debug("FI: no pipeline has been found in acking");
                        (132: 5) LOG.debug("FI: Acked total bytes from: " +
                        (145: 7) LOG.fatal("No exception should be happening at this point");
                        (159: 7) LOG.debug("FI: remove first ack as expected");
                        (164: 7) LOG.debug("FI: remove first ack as expected");
                        (169: 9) LOG.debug("FI: suspend the ack");
                        (172: 5) LOG.debug("FI: remove first ack as expected");
                        (189: 7) LOG.info("FI: pipelineClose, datanode=" + dnId.getName()
                        (210: 5) LOG.info("FI: fiPipelineAck, datanode=" + dnId);
                        (225: 5) LOG.info("FI: blockFileClose, datanode=" + dnId);
                    DataTransferProtocolAspects.aj  (3 usages)
                        (52: 5) LOG.info("FI: receiverOp " + op + ", datanode="
                        (62: 5) LOG.info("FI: statusRead " + status + ", datanode="
                        (74: 5) LOG.info("FI: receiverOpWriteBlock");
                    FSDatasetAspects.aj  (2 usages)
                        (48: 7) LOG.info("Before the injection point");
                        (57: 7) LOG.info("Before the injection point");
                    TestFiDataTransferProtocol.java  (4 usages)
                        (99: 16) FiTestUtil.LOG.info("Running " + methodName + " ...");
                        (109: 16) FiTestUtil.LOG.info("Running " + methodName + " ...");
                        (121: 16) FiTestUtil.LOG.info("Running " + methodName + " ...");
                        (133: 16) FiTestUtil.LOG.info("Running " + methodName + " ...");
                    TestFiDataTransferProtocol2.java  (7 usages)
                        (91: 16) FiTestUtil.LOG.info("size=" + size + ", nPackets=" + nPackets
                        (133: 16) FiTestUtil.LOG.info("Running " + methodName + " ...");
                        (146: 16) FiTestUtil.LOG.info("Running " + methodName + " ...");
                        (159: 16) FiTestUtil.LOG.info("Running " + methodName + " ...");
                        (210: 16) FiTestUtil.LOG.info("Running " + methodName + " ...");
                        (224: 16) FiTestUtil.LOG.info("Running " + methodName + " ...");
                        (238: 16) FiTestUtil.LOG.info("Running " + methodName + " ...");
                    TestFiPipelineClose.java  (3 usages)
                        (40: 16) FiTestUtil.LOG.info("Running " + methodName + " ...");
                        (86: 16) FiTestUtil.LOG.info("Running " + name + " ...");
                        (209: 16) FiTestUtil.LOG.info("Running " + methodName + " ...");
                /home/michalxo/IdeaProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/aop/org/apache/hadoop/hdfs/server/namenode  (5 usages)
                    FileDataServletAspects.aj  (2 usages)
                        (40: 5) LOG.info("FI: original url = " + original);
                        (43: 5) LOG.info("FI: replaced url = " + replaced);
                    ListPathAspects.aj  (1 usage)
                        (45: 5) LOG.info("FI: callGetListing");
                    RenameAspects.aj  (2 usages)
                        (43: 5) LOG.info("FI: callRenameRemove");
                        (60: 5) LOG.info("FI: callAddChildNoQuotaCheck");
                org.apache.hadoop.hdfs  (134 usages)
                    BlockReaderLocal.java  (6 usages)
                        (187: 9) LOG.debug("New BlockReaderLocal for file " + blkfile + " of size "
                        (202: 11) LOG.warn("Wrong version (" + version + ") for metadata file for "
                        (218: 17) DFSClient.LOG.warn("BlockReaderLocal: Removing " + blk
                        (257: 11) LOG.debug("Cached location of block " + blk + " as " + pathinfo);
                        (565: 7) LOG.trace("read off " + off + " len " + len);
                        (586: 7) LOG.debug("skip " + n);
                    BlockStorageLocationUtil.java  (6 usages)
                        (147: 11) LOG.warn("Invalid access token when trying to retrieve "
                        (152: 11) LOG.info("Datanode " + datanode.getIpcAddr(false) + " does not support"
                        (156: 11) LOG.info("Failed to connect to datanode " +
                        (160: 11) LOG.debug("Could not fetch information from datanode", t);
                        (164: 9) LOG.info("Interrupted while fetching HdfsBlocksMetadata");
                        (252: 13) LOG.debug("Datanode responded with a block volume id we did" +
                    DFSClient.java  (26 usages)
                        (300: 9) LOG.warn("Bad checksum type: " + checksum + ". Using default "
                        (424: 7) LOG.debug("Short circuit read is " + shortCircuitLocalReads);
                        (430: 7) LOG.debug("Using local interfaces [" +
                        (487: 7) LOG.debug("Using local interface " + addr);
                        (624: 11) LOG.warn("Failed to renew lease for " + clientName + " for "
                        (655: 8) LOG.info("Exception occurred while aborting the client " + ioe);
                        (680: 11) LOG.error("Failed to " + (abort? "abort": "close") + " file " + src,
                        (728: 7) LOG.warn("Problem getting block size", ie);
                        (766: 5) LOG.info("Created " + DelegationTokenIdentifier.stringifyToken(token));
                        (780: 5) LOG.info("Renewing " + DelegationTokenIdentifier.stringifyToken(token));
                        (816: 9) LOG.trace("Address " + targetAddr +
                        (825: 7) LOG.trace("Address " + targetAddr +
                        (841: 5) LOG.info("Cancelling " + DelegationTokenIdentifier.stringifyToken(token));
                        (885: 7) LOG.info("Cancelling " +
                        (1240: 7) LOG.debug(src + ": masked=" + masked);
                        (1573: 5) LOG.debug("Clearing encryption key");
                        (1596: 11) LOG.debug("Getting new encryption token from NN");
                        (1665: 13) LOG.debug("write to " + datanodes[j] + ": "
                        (1713: 13) LOG.debug("Retrieving checksum from an earlier-version DataNode: " +
                        (1732: 15) LOG.debug("set bytesPerCRC=" + bytesPerCRC
                        (1735: 13) LOG.debug("got reply from " + datanodes[j] + ": md5=" + md5);
                        (1740: 15) LOG.debug("Got access token error in response to OP_BLOCK_CHECKSUM "
                        (1752: 11) LOG.warn("src=" + src + ", datanodes["+j+"]=" + datanodes[j], ie);
                        (1802: 9) LOG.debug("Connecting to datanode " + dnAddr);
                        (2112: 7) LOG.debug(src + ": masked=" + absPermission);
                        (2214: 7) LOG.info("Found corruption while reading " + file
                    DFSInputStream.java  (22 usages)
                        (132: 19) DFSClient.LOG.warn("Last block locations not available. "
                        (159: 17) DFSClient.LOG.debug("newInfo = " + newInfo);
                        (220: 21) DFSClient.LOG.debug("Failed to getReplicaVisibleLength from datanode "
                        (459: 21) DFSClient.LOG.info("Successfully connected to " + targetAddr +
                        (465: 21) DFSClient.LOG.info("Will fetch a new encryption key and retry, "
                        (472: 21) DFSClient.LOG.info("Will fetch a new access token and retry, "
                        (489: 21) DFSClient.LOG.warn("Failed to connect to " + targetAddr + " for block"
                        (596: 19) DFSClient.LOG.warn("Found Checksum error for "
                        (606: 21) DFSClient.LOG.warn("Exception while reading from "
                        (664: 23) DFSClient.LOG.warn("DFS Read", e);
                        (727: 21) DFSClient.LOG.debug("Connecting to datanode " + dnAddr);
                        (739: 21) DFSClient.LOG.info("No node available for " + blockInfo);
                        (741: 19) DFSClient.LOG.info("Could not obtain " + block.getBlock()
                        (756: 21) DFSClient.LOG.warn("DFS chooseDataNode: got # " + (failures + 1) + " IOException, will wait for " + waitTime + " msec.");
                        (803: 19) DFSClient.LOG.warn("fetchBlockByteRange(). Got a checksum exception for " +
                        (809: 19) DFSClient.LOG.warn("Short circuit access failed ", ex);
                        (814: 21) DFSClient.LOG.info("Will fetch a new encryption key and retry, "
                        (821: 21) DFSClient.LOG.info("Will get a new access token and retry, "
                        (828: 21) DFSClient.LOG.warn("Failed to connect to " + targetAddr +
                        (831: 23) DFSClient.LOG.debug("Connection failure ", e);
                        (949: 19) DFSClient.LOG.debug("Error making BlockReader. Closing stale " + sock, ex);
                        (1101: 23) DFSClient.LOG.debug("Exception while seek to " + targetPos
                    DFSOutputStream.java  (38 usages)
                        (382: 19) DFSClient.LOG.debug("Closing old block " + block);
                        (407: 23) DFSClient.LOG.warn("Caught exception ", e);
                        (435: 27) DFSClient.LOG.warn("Caught exception ", e);
                        (455: 25) DFSClient.LOG.debug("Allocating new block");
                        (461: 25) DFSClient.LOG.debug("Append to block " + block);
                        (485: 29) DFSClient.LOG.warn("Caught exception ", e);
                        (506: 23) DFSClient.LOG.debug("DataStreamer block " + block +
                        (557: 21) DFSClient.LOG.warn("DataStreamer Exception", e);
                        (605: 21) DFSClient.LOG.warn("Caught exception ", e);
                        (668: 25) DFSClient.LOG.debug("DFSClient " + ack);
                        (719: 25) DFSClient.LOG.warn("DFSOutputStream ResponseProcessor exception "
                        (739: 19) DFSClient.LOG.info("Error Recovery for " + block +
                        (814: 30) DataTransferProtocol.LOG.debug("lastAckedSeqno = " + lastAckedSeqno);
                        (913: 19) DFSClient.LOG.warn(msg);
                        (941: 21) DFSClient.LOG.warn("Error Recovery for block " + block +
                        (1017: 21) DFSClient.LOG.info("Abandoning " + block);
                        (1020: 21) DFSClient.LOG.info("Excluding datanode " + nodes[errorIndex]);
                        (1040: 21) DFSClient.LOG.debug("pipeline = " + nodes[i]);
                        (1100: 21) DFSClient.LOG.info("Exception in createBlockOutputStream", ie);
                        (1102: 23) DFSClient.LOG.info("Will fetch a new encryption key and retry, "
                        (1170: 27) DFSClient.LOG.info("Exception while adding a block", e);
                        (1172: 29) DFSClient.LOG.info("Waiting for replication for "
                        (1177: 29) DFSClient.LOG.warn("NotReplicatedYetException sleeping " + src
                        (1182: 29) DFSClient.LOG.warn("Caught exception ", ie);
                        (1224: 17) DFSClient.LOG.debug("Connecting to datanode " + dnAddr);
                        (1233: 17) DFSClient.LOG.debug("Send buf size " + sock.getSendBufferSize());
                        (1274: 17) DFSClient.LOG.debug(
                        (1361: 17) DFSClient.LOG.debug("computePacketChunkSize: src=" + src +
                        (1374: 19) DFSClient.LOG.debug("Queued packet " + currentPacket.seqno);
                        (1428: 19) DFSClient.LOG.debug("DFSClient writeChunk allocating new packet seqno=" +
                        (1447: 19) DFSClient.LOG.debug("DFSClient writeChunk packet full seqno=" +
                        (1554: 21) DFSClient.LOG.debug(
                        (1619: 21) DFSClient.LOG.warn("Unable to persist blocks in hflush for " + src, ioe);
                        (1641: 17) DFSClient.LOG.warn("Error while syncing", e);
                        (1700: 17) DFSClient.LOG.debug("Waiting for ack for: " + seqno);
                        (1806: 23) DFSClient.LOG.info(msg);
                        (1812: 23) DFSClient.LOG.info("Could not complete " + src + " retrying...");
                        (1815: 21) DFSClient.LOG.warn("Caught exception ", ie);
                    DFSUtil.java  (2 usages)
                        (1044: 11) LOG.trace(String.format("addressKey: %s nsId: %s nnId: %s",
                        (1059: 11) LOG.warn("Exception in creating socket address " + addr, e);
                    DistributedFileSystem.java  (4 usages)
                        (733: 7) LOG.error("Error: Current block in data stream is null! ");
                        (738: 5) LOG.info("Found checksum error in data stream at "
                        (746: 7) LOG.error("Error: Current block in checksum stream is null! ");
                        (751: 5) LOG.info("Found checksum error in checksum stream at "
                    HAUtil.java  (2 usages)
                        (272: 9) LOG.debug("Mapped HA service delegation token for logical URI " +
                        (276: 7) LOG.debug("No HA service delegation token found for logical URI " +
                    HftpFileSystem.java  (6 usages)
                        (212: 9) LOG.debug("Created new DT for " + token.getService());
                        (214: 9) LOG.debug("Found existing DT for " + token.getService());
                        (259: 15) LOG.warn("Couldn't connect to " + nnHttpUrl +
                        (264: 15) LOG.debug("Exception getting delegation token", e);
                        (270: 15) LOG.debug("Got dt for " + getUri() + ";t.service="
                        (300: 7) LOG.trace("url=" + url);
                    HsftpFileSystem.java  (1 usage)
                        (179: 13) LOG.warn(sb.toString());
                    LeaseRenewer.java  (11 usages)
                        (294: 17) LOG.debug("Lease renewer daemon for " + clientsString()
                        (300: 17) LOG.debug(LeaseRenewer.this.getClass().getSimpleName()
                        (308: 17) LOG.debug("Lease renewer daemon for " + clientsString()
                        (388: 9) LOG.debug("Wait for lease checker to terminate");
                        (413: 13) LOG.debug("Did not renew lease for client " +
                        (420: 11) LOG.debug("Lease renewed for client " + previousName);
                        (438: 13) LOG.debug("Lease renewer daemon for " + clientsString()
                        (443: 11) LOG.warn("Failed to renew lease for " + clientsString() + " for "
                        (452: 11) LOG.warn("Failed to renew lease for " + clientsString() + " for "
                        (461: 15) LOG.debug("Lease renewer daemon for " + clientsString()
                        (464: 16) LOG.debug("Lease renewer daemon for " + clientsString()
                    NameNodeProxies.java  (2 usages)
                        (186: 7) LOG.error(message);
                        (362: 9) LOG.debug(message, e);
                    RemoteBlockReader.java  (2 usages)
                        (230: 9) LOG.debug("DFSClient readChunk got header " + header);
                        (468: 7) LOG.info("Could not send read status (" + statusCode + ") to datanode " +
                    RemoteBlockReader2.java  (3 usages)
                        (164: 7) LOG.trace("DFSClient readNextPacket got header " + curHeader);
                        (235: 7) LOG.trace("Reading empty packet at end of read");
                        (318: 7) LOG.info("Could not send read status (" + statusCode + ") to datanode " +
                    SocketCache.java  (3 usages)
                        (90: 9) LOG.info("SocketCache disabled.");
                        (99: 9) LOG.info("capacity and expiry periods already set to " + capacity +
                        (183: 7) LOG.warn("Cannot cache (unconnected) socket with no remote address: " +
                org.apache.hadoop.hdfs.client  (2 usages)
                    HdfsUtils.java  (2 usages)
                        (72: 9) LOG.debug("Is namenode in safemode? " + safemode + "; uri=" + uri);
                        (80: 9) LOG.debug("Got an exception for uri=" + uri, e);
                org.apache.hadoop.hdfs.protocol.datatransfer  (4 usages)
                    DataTransferEncryptor.java  (2 usages)
                        (128: 7) LOG.debug("Server using encryption algorithm " + encryptionAlgorithm);
                        (195: 7) LOG.debug("Client using encryption algorithm " +
                    PacketReceiver.java  (1 usage)
                        (151: 7) LOG.trace("readNextPacket: dataPlusChecksumLen = " + dataPlusChecksumLen +
                    Sender.java  (1 usage)
                        (66: 7) LOG.trace("Sending DataTransferOp " + proto.getClass().getSimpleName()
                org.apache.hadoop.hdfs.protocolPB  (2 usages)
                    ClientDatanodeProtocolTranslatorPB.java  (2 usages)
                        (110: 7) LOG.debug("Connecting to datanode " + dnAddr + " addr=" + addr);
                        (123: 7) LOG.debug("Connecting to datanode " + dnAddr + " addr=" + addr);
                org.apache.hadoop.hdfs.qjournal.client  (16 usages)
                    IPCLoggerChannel.java  (2 usages)
                        (357: 34) QuorumJournalManager.LOG.warn(
                        (476: 34) QuorumJournalManager.LOG.info(
                    QuorumCall.java  (2 usages)
                        (124: 32) QuorumJournalManager.LOG.warn(msg);
                        (126: 32) QuorumJournalManager.LOG.info(msg);
                    QuorumJournalManager.java  (11 usages)
                        (249: 5) LOG.info("Beginning recovery of unclosed segment starting at txid " +
                        (258: 5) LOG.info("Recovery prepare phase complete. Responses:\n" +
                        (278: 7) LOG.info("Using already-accepted recovery for segment " +
                        (282: 7) LOG.info("Using longest log: " + bestEntry);
                        (305: 7) LOG.info("None of the responders had a log to recover: " +
                        (370: 7) LOG.warn("Quorum journal URI '" + uri + "' has an even number " +
                        (411: 5) LOG.info("Purging remote journals older than txid " + minTxIdToKeep);
                        (419: 5) LOG.info("Starting recovery process for unclosed journal segments...");
                        (421: 5) LOG.info("Successfully started new epoch " + loggers.getEpoch());
                        (424: 7) LOG.debug("newEpoch(" + loggers.getEpoch() + ") responses:\n" +
                        (459: 5) LOG.debug("selectInputStream manifests:\n" +
                    QuorumOutputStream.java  (1 usage)
                        (72: 26) QuorumJournalManager.LOG.warn("Aborting " + this);
                org.apache.hadoop.hdfs.qjournal.server  (37 usages)
                    GetJournalEditServlet.java  (8 usages)
                        (80: 7) LOG.warn("Received null remoteUser while authorizing access to " +
                        (86: 7) LOG.debug("Validating request made by " + remotePrincipal +
                        (101: 9) LOG.debug("isValidRequestor is comparing to valid requestor: " + v);
                        (104: 11) LOG.debug("isValidRequestor is allowing: " + remotePrincipal);
                        (115: 9) LOG.debug("isValidRequestor is allowing other JN principal: " +
                        (121: 7) LOG.debug("isValidRequestor is rejecting: " + remotePrincipal);
                        (132: 7) LOG.warn("Received non-NN/JN request for edits from "
                        (153: 7) LOG.warn("Received an invalid request file transfer request from " +
                    JNStorage.java  (4 usages)
                        (160: 13) LOG.info("Purging no-longer needed file " + txid);
                        (162: 15) LOG.warn("Unable to delete no-longer-needed data " +
                        (174: 5) LOG.info("Formatting journal " + sd + " with nsid: " + getNamespaceID());
                        (213: 5) LOG.info("Closing journal storage for " + sd);
                    Journal.java  (20 usages)
                        (178: 5) LOG.info("Scanning storage " + fjm);
                        (184: 7) LOG.info("Latest log is " + latestLog);
                        (187: 9) LOG.warn("Latest log " + latestLog + " has no transactions. " +
                        (195: 5) LOG.info("No files in " + fjm);
                        (206: 5) LOG.info("Formatting " + this + " with namespace info: " +
                        (305: 5) LOG.info("Updating lastPromisedEpoch from " + lastPromisedEpoch.get() +
                        (356: 7) LOG.trace("Writing txid " + firstTxnId + "-" + lastTxnId);
                        (487: 7) LOG.warn("Client is requesting a new log segment " + txid +
                        (518: 7) LOG.info("Updating lastWriterEpoch from " + curLastWriterEpoch +
                        (570: 9) LOG.info("Validating log segment " + elf.getFile() + " about to be " +
                        (647: 7) LOG.info("Edit log file " + elf + " appears to be empty. " +
                        (657: 5) LOG.info("getSegmentInfo(" + segmentTxId + "): " + elf + " -> " +
                        (701: 5) LOG.info("Prepared recovery for segment " + segmentTxId + ": " +
                        (747: 9) LOG.info("Synchronizing log " + TextFormat.shortDebugString(segment) +
                        (754: 9) LOG.info("Synchronizing log " + TextFormat.shortDebugString(segment) +
                        (787: 7) LOG.info("Skipping download of log " +
                        (820: 5) LOG.info("Accepted recovery for segment " + segmentTxId + ": " +
                        (843: 5) LOG.info("Synchronizing log " +
                        (857: 19) LOG.warn("Failed to delete temporary file " + tmpFile);
                        (895: 7) LOG.info("Rolling forward previously half-completed synchronization: " +
                    JournalNode.java  (3 usages)
                        (77: 7) LOG.info("Initializing journal in directory " + logDir);
                        (183: 9) LOG.warn("Unable to stop HTTP server for " + this, ioe);
                        (227: 7) LOG.fatal("Error reported on file " + f + "... exiting",
                    JournalNodeHttpServer.java  (2 usages)
                        (66: 5) LOG.info("Starting web server as: "+ SecurityUtil.getServerPrincipal(conf
                        (90: 5) LOG.info("Journal Web-server up at: " + bindAddr + ":" + infoPort);
                org.apache.hadoop.hdfs.security.token.block  (5 usages)
                    BlockTokenSecretManager.java  (5 usages)
                        (170: 7) LOG.debug("Exporting access keys");
                        (193: 5) LOG.info("Setting block keys");
                        (222: 5) LOG.info("Updating block keys");
                        (264: 7) LOG.debug("Checking access for user=" + userId + ", block=" + block
                        (357: 7) LOG.debug("Generating block token for " + identifier.toString());
                org.apache.hadoop.hdfs.server.balancer  (21 usages)
                    Balancer.java  (14 usages)
                        (279: 17) LOG.debug("Decided to move block "+ block.getBlockId()
                        (359: 9) LOG.info( "Moving block " + block.getBlock().getBlockId() +
                        (365: 9) LOG.warn("Error moving block "+block.getBlockId()+
                        (421: 13) LOG.debug("Starting moving "+ block.getBlockId() +
                        (777: 13) LOG.warn("Exception while getting block list", e);
                        (933: 5) LOG.info(nodes.size() + " " + name + ": " + nodes);
                        (1067: 5) LOG.info("Decided to move "+StringUtils.byteDesc(size)+" bytes from "
                        (1118: 9) LOG.warn("Dispatcher thread failed", e.getCause());
                        (1365: 9) LOG.info( "Need to move "+ StringUtils.byteDesc(bytesLeftToMove)
                        (1379: 9) LOG.info( "Will move " + StringUtils.byteDesc(bytesToMove) +
                        (1429: 5) LOG.info("namenodes = " + namenodes);
                        (1430: 5) LOG.info("p         = " + p);
                        (1558: 17) LOG.info( "Using a threshold of " + threshold );
                        (1604: 7) LOG.error("Exiting balancer due an exception", e);
                    NameNodeConnector.java  (7 usages)
                        (95: 7) LOG.info("Block token params received from NN: keyUpdateInterval="
                        (109: 7) LOG.info("Balancer will update its block keys every "
                        (206: 7) LOG.warn("Exception shutting down access key updater thread", e);
                        (215: 9) LOG.warn("Failed to delete " + BALANCER_ID_PATH, ioe);
                        (238: 13) LOG.error("Failed to set keys", e);
                        (243: 9) LOG.debug("InterruptedException in block key updater thread", e);
                        (245: 9) LOG.error("Exception in block key updater thread", e);
                org.apache.hadoop.hdfs.server.blockmanagement  (66 usages)
                    BlockManager.java  (37 usages)
                        (295: 5) LOG.info("defaultReplication         = " + defaultReplication);
                        (296: 5) LOG.info("maxReplication             = " + maxReplication);
                        (297: 5) LOG.info("minReplication             = " + minReplication);
                        (298: 5) LOG.info("maxReplicationStreams      = " + maxReplicationStreams);
                        (299: 5) LOG.info("shouldCheckForEnoughRacks  = " + shouldCheckForEnoughRacks);
                        (300: 5) LOG.info("replicationRecheckInterval = " + replicationRecheckInterval);
                        (301: 5) LOG.info("encryptDataTransfer        = " + encryptDataTransfer);
                        (309: 5) LOG.info(DFSConfigKeys.DFS_BLOCK_ACCESS_TOKEN_ENABLE_KEY + "=" + isEnabled);
                        (323: 5) LOG.info(DFSConfigKeys.DFS_BLOCK_ACCESS_KEY_UPDATE_INTERVAL_KEY
                        (741: 7) LOG.warn("Inconsistent number of corrupt replicas for "
                        (784: 9) LOG.debug("blocks = " + java.util.Arrays.asList(blocks));
                        (1166: 15) LOG.debug("Block " + block + " cannot be repl from any node");
                        (1568: 9) LOG.info("BLOCK* processReport: Received first block report from "
                        (1600: 11) LOG.debug("BLOCK* rescanPostponedMisreplicatedBlocks: " +
                        (1610: 9) LOG.debug("BLOCK* rescanPostponedMisreplicatedBlocks: " +
                        (1793: 7) LOG.debug("Reported block " + block
                        (1817: 7) LOG.debug("In memory blockUCState = " + ucState);
                        (1867: 7) LOG.debug("Queueing reported block " + block +
                        (1893: 9) LOG.debug("Processing previouly queued message " + rbi);
                        (1912: 7) LOG.info("Processing " + count + " messages from DataNodes " +
                        (1970: 11) LOG.info("Received an RBW replica for " + storedBlock +
                        (1986: 7) LOG.warn(msg);
                        (2153: 7) LOG.warn("Inconsistent number of corrupt replicas for " +
                        (2224: 9) LOG.trace("block " + block + ": " + res);
                        (2250: 5) LOG.info("Total number of blocks            = " + blocksMap.size());
                        (2251: 5) LOG.info("Number of invalid blocks          = " + nrInvalid);
                        (2252: 5) LOG.info("Number of under-replicated blocks = " + nrUnderReplicated);
                        (2253: 5) LOG.info("Number of  over-replicated blocks = " + nrOverReplicated +
                        (2255: 5) LOG.info("Number of blocks being written    = " + nrUnderConstruction);
                        (2319: 7) LOG.info("Decreasing replication from " + oldRepl + " to " + newRepl
                        (2325: 7) LOG.info("Increasing replication from " + oldRepl + " to " + newRepl
                        (2349: 9) LOG.info("BLOCK* processOverReplicatedBlock: " +
                        (2726: 5) LOG.info("Block: " + block + ", Expected Replicas: "
                        (2757: 5) LOG.info("Invalidated " + numOverReplicated + " over-replicated blocks on " +
                        (2921: 9) LOG.debug("In safemode, not computing replication work");
                        (3069: 11) LOG.warn("ReplicationMonitor thread received InterruptedException.", ie);
                        (3072: 11) LOG.fatal("ReplicationMonitor thread received Runtime exception. ", t);
                    BlockPlacementPolicyDefault.java  (1 usage)
                        (234: 7) LOG.warn("Not able to place enough replicas, still in need of "
                    BlocksMap.java  (3 usages)
                        (88: 23) LightWeightGSet.LOG.debug("VM type       = " + vmBit + "-bit");
                        (89: 23) LightWeightGSet.LOG.debug("2% max memory = " + twoPC/(1 << 20) + " MB");
                        (90: 23) LightWeightGSet.LOG.debug("capacity      = 2^" + exponent
                    DatanodeDescriptor.java  (1 usage)
                        (365: 20) BlockManager.LOG.info(block + " is already in the recovery queue");
                    DatanodeManager.java  (16 usages)
                        (195: 5) LOG.info(DFSConfigKeys.DFS_BLOCK_INVALIDATE_LIMIT_KEY
                        (236: 7) LOG.warn("The given interval for marking stale datanode = "
                        (246: 7) LOG.warn("The given interval for marking stale datanode = "
                        (377: 7) LOG.debug("remove datanode " + nodeInfo);
                        (439: 7) LOG.debug(getClass().getSimpleName() + ".addDatanode: "
                        (451: 7) LOG.debug(getClass().getSimpleName() + ".wipeDatanode("
                        (470: 7) LOG.error("The resolve call returned null! Using " +
                        (575: 9) LOG.info("Decommission complete for " + node);
                        (584: 7) LOG.info("Start Decommissioning " + node + " with " +
                        (597: 7) LOG.info("Stop Decommissioning " + node);
                        (643: 9) LOG.warn("Unresolved datanode registration from " + ip);
                        (667: 16) NameNode.LOG.info("BLOCK* registerDatanode: " + nodeN);
                        (933: 9) LOG.info(message);
                        (937: 9) LOG.debug(message);
                        (975: 9) LOG.warn("Invalid hostname " + hostStr + " in hosts file");
                        (1177: 5) LOG.info("Marking all datandoes as stale");
                    DecommissionManager.java  (2 usages)
                        (78: 11) LOG.warn(this.getClass().getSimpleName() + " interrupted: " + ie);
                        (95: 13) LOG.warn("entry=" + entry, e);
                    HeartbeatManager.java  (2 usages)
                        (78: 7) LOG.info("Setting heartbeat recheck interval to " + staleInterval
                        (292: 11) LOG.error("Exception while checking heartbeat", e);
                    PendingReplicationBlocks.java  (4 usages)
                        (97: 11) LOG.debug("Removing pending replication for " + block);
                        (214: 13) LOG.debug("PendingReplicationMonitor thread is interrupted.", ie);
                        (229: 11) LOG.debug("PendingReplicationMonitor checking Q");
                        (239: 13) LOG.warn("PendingReplicationMonitor timed out " + block);
                org.apache.hadoop.hdfs.server.common  (22 usages)
                    JspHelper.java  (1 usage)
                        (613: 7) LOG.debug("getUGI is returning: " + ugi.getShortUserName());
                    Storage.java  (18 usages)
                        (438: 13) LOG.warn("Storage directory " + rootPath + " does not exist");
                        (441: 11) LOG.info(rootPath + " does not exist. Creating ...");
                        (447: 11) LOG.warn(rootPath + "is not a directory");
                        (451: 11) LOG.warn("Cannot access storage directory " + rootPath);
                        (455: 9) LOG.warn("Cannot access storage directory " + rootPath, ex);
                        (543: 9) LOG.info("Completing previous upgrade for storage directory "
                        (548: 9) LOG.info("Recovering storage directory " + rootPath
                        (555: 9) LOG.info("Completing previous rollback for storage directory "
                        (560: 9) LOG.info("Recovering storage directory " + rootPath
                        (565: 9) LOG.info("Completing previous finalize for storage directory "
                        (570: 9) LOG.info("Completing previous checkpoint for storage directory "
                        (578: 9) LOG.info("Recovering storage directory " + rootPath
                        (628: 9) LOG.info("Locking is disabled");
                        (635: 9) LOG.info(msg);
                        (664: 9) LOG.info("Lock on " + lockF + " acquired by nodename " + jvmName);
                        (666: 9) LOG.error("It appears that another namenode " + file.readLine()
                        (671: 9) LOG.error("Failed to acquire lock on " + lockF + ". If this storage directory is mounted via NFS, "
                        (810: 7) LOG.error(msg);
                    Util.java  (3 usages)
                        (50: 7) LOG.error("Syntax error in URI " + s
                        (56: 7) LOG.warn("Path " + s + " should be specified as a URI "
                        (100: 9) LOG.error("Error while processing URI: " + name, e);
                org.apache.hadoop.hdfs.server.datanode  (254 usages)
                    BlockPoolManager.java  (6 usages)
                        (99: 9) LOG.info("Removed " + bpos);
                        (105: 7) LOG.warn("Couldn't remove BPOS " + t + " from bpByNameserviceId map");
                        (148: 5) LOG.info("Refresh request received for nameservices: "
                        (193: 9) LOG.info("Starting BPOfferServices for nameservices: " +
                        (211: 7) LOG.info("Stopping BPOfferServices for nameservices: " +
                        (224: 7) LOG.info("Refreshing list of NNs for nameservices: " +
                    BlockPoolSliceScanner.java  (20 usages)
                        (156: 5) LOG.info("Periodic Block Verification Scanner initialized with interval "
                        (175: 7) LOG.warn("Could not open verfication log. " +
                        (241: 7) LOG.warn("Adding an already existing block " + block);
                        (313: 5) LOG.info("Reporting bad " + block);
                        (318: 7) LOG.warn("Cannot report bad " + block.getBlockId());
                        (362: 11) LOG.warn("Cannot parse line: " + line, nfe);
                        (399: 9) LOG.info((second ? "Second " : "") +
                        (414: 11) LOG.info(block + " is no longer in the dataset");
                        (426: 11) LOG.info("Verification failed for " + block +
                        (432: 9) LOG.warn((second ? "Second " : "First ") + "Verification failed for "
                        (528: 9) LOG.warn("Failed to read previous verification times.", e);
                        (565: 5) LOG.info("Starting a new period : work left in prev period : "
                        (577: 9) LOG.debug("Skipping scan since bytesLeft=" + bytesLeft + ", Start=" +
                        (617: 7) LOG.debug("Starting to scan blockpool: " + blockPoolId);
                        (636: 13) LOG.debug("All remaining blocks were processed recently, "
                        (643: 7) LOG.warn("RuntimeException during BlockPoolScanner.scan()", e);
                        (648: 9) LOG.debug("Done scanning block pool: " + blockPoolId);
                        (658: 9) LOG.warn("Received exception: ", ex);
                        (761: 9) LOG.warn("Failed to append to " + logs + ", m=" + m, e);
                        (769: 9) LOG.warn("Failed to close the appender of " + logs, e);
                    BlockPoolSliceStorage.java  (12 usages)
                        (116: 11) LOG.info("Storage directory " + dataDir + " does not exist.");
                        (120: 11) LOG.info("Storage directory " + dataDir + " is not formatted.");
                        (121: 11) LOG.info("Formatting ...");
                        (175: 5) LOG.info("Formatting block pool " + blockpoolID + " directory "
                        (300: 5) LOG.info("Upgrading block pool storage directory " + bpSd.getRoot()
                        (338: 5) LOG.info("Upgrade of block pool " + blockpoolID + " at " + bpSd.getRoot()
                        (401: 5) LOG.info("Rolling back storage directory " + bpSd.getRoot()
                        (416: 5) LOG.info("Rollback of " + bpSd.getRoot() + " is complete");
                        (432: 5) LOG.info("Finalizing upgrade for storage directory " + dataDirPath
                        (448: 11) LOG.error("Finalize upgrade for " + dataDirPath + " failed.", ex);
                        (450: 9) LOG.info("Finalize upgrade for " + dataDirPath + " is complete.");
                        (476: 5) LOG.info( hardLink.linkStats.report() );
                    BlockReceiver.java  (29 usages)
                        (144: 9) LOG.debug(getClass().getSimpleName() + ": " + block
                        (213: 9) LOG.warn("Could not get file descriptor for outputstream of class " +
                        (233: 16) DataNode.LOG.warn("IOException in BlockReceiver constructor. Cause is ",
                        (359: 5) LOG.info(datanode.getDNRegistrationForBP(bpid)
                        (379: 7) LOG.warn("Checksum error in block " + block + " from " + inAddr, ce);
                        (382: 11) LOG.info("report corrupt " + block + " from datanode " +
                        (386: 11) LOG.warn("Failed to report bad " + block +
                        (418: 7) LOG.debug("Receiving one packet for block " + block +
                        (474: 9) LOG.debug("Receiving an empty packet or the end of the block " + block);
                        (522: 13) LOG.info("Packet starts at " + firstByteInBlock +
                        (556: 15) LOG.debug("Writing out partial crc for data len " + len);
                        (617: 7) LOG.warn("Couldn't drop os cache behind writer for " + block, t);
                        (671: 7) LOG.info("Exception for " + block, ioe);
                        (741: 5) LOG.info("computePartialChunkCrc sizePartialChunk " +
                        (766: 5) LOG.info("Read in partial CRC chunk from disk for " + block);
                        (843: 9) LOG.debug(myString + ": enqueue " + p);
                        (857: 11) LOG.debug(myString + ": seqno=" + seqno +
                        (879: 9) LOG.debug(myString + ": closing");
                        (908: 17) LOG.debug(myString + " got " + ack);
                        (936: 21) LOG.debug("Calculated invalid ack time: " + ackTimeNanos
                        (955: 15) LOG.info(myString, ioe);
                        (968: 13) LOG.info(myString + ": Thread is interrupted.");
                        (985: 11) LOG.warn("IOException in BlockReceiver.run(): ", e);
                        (990: 15) LOG.warn("DataNode.checkDiskError failed in run() with: ", ioe);
                        (992: 13) LOG.info(myString, e);
                        (1000: 13) LOG.info(myString, e);
                        (1006: 7) LOG.info(myString + " terminating");
                        (1028: 9) LOG.info("Received " + block + " size " + block.getNumBytes()
                        (1065: 9) LOG.debug(myString + ", replyAck=" + replyAck);
                    BlockSender.java  (9 usages)
                        (212: 18) DataNode.LOG.debug("block=" + block + ", replica=" + replica);
                        (243: 13) LOG.warn("Wrong version (" + version + ") for metadata file for "
                        (248: 11) LOG.warn("Could not find metadata file for " + block);
                        (283: 9) LOG.warn(datanode.getDNRegistrationForBP(block.getBlockPoolId()) +
                        (318: 18) DataNode.LOG.debug("replica=" + replica);
                        (345: 9) LOG.warn("Unable to drop cache on file close", e);
                        (525: 11) LOG.info("exception: ", e);
                        (538: 11) LOG.error("BlockSender.sendChunks() exception: ", e);
                        (566: 7) LOG.warn(" Could not read or failed to veirfy checksum for data"
                    BPOfferService.java  (16 usages)
                        (153: 7) LOG.warn("Block pool ID needed, but service not yet registered with NN",
                        (379: 9) LOG.warn("Couldn't report bad block " + block + " to " + actor,
                        (420: 7) LOG.info("Namenode " + actor + " trying to claim ACTIVE state with " +
                        (426: 9) LOG.warn("NN " + actor + " tried to claim ACTIVE state at txid=" +
                        (432: 11) LOG.info("Acknowledging ACTIVE Namenode " + actor);
                        (434: 11) LOG.info("Namenode " + actor + " taking over ACTIVE state from " +
                        (440: 7) LOG.info("Namenode " + actor + " relinquishing ACTIVE state with " +
                        (552: 7) LOG.info("DatanodeCommand action: DNA_REGISTER");
                        (568: 7) LOG.info("DatanodeCommand action: DNA_ACCESSKEYUPDATE");
                        (576: 7) LOG.info("DatanodeCommand action: DNA_BALANCERBANDWIDTHUPDATE");
                        (582: 9) LOG.info("Updating balance throttler bandwidth from "
                        (589: 7) LOG.warn("Unknown DatanodeCommand action: " + cmd.getAction());
                        (601: 7) LOG.info("DatanodeCommand action from standby: DNA_REGISTER");
                        (605: 7) LOG.info("DatanodeCommand action from standby: DNA_ACCESSKEYUPDATE");
                        (617: 7) LOG.warn("Got a command from standby NN - ignoring command:" + cmd.getAction());
                        (620: 7) LOG.warn("Unknown DatanodeCommand action: " + cmd.getAction());
                    BPServiceActor.java  (27 usages)
                        (164: 9) LOG.debug(this + " received versionRequest response: " + nsInfo);
                        (167: 9) LOG.warn("Problem connecting to server: " + nnAddr);
                        (169: 9) LOG.warn("Problem connecting to server: " + nnAddr);
                        (192: 7) LOG.warn(ive.getMessage());
                        (197: 7) LOG.info("Reported NameNode version '" + nnVersion + "' does not match " +
                        (203: 7) LOG.warn("DataNode and NameNode layout versions must be the same." +
                        (255: 7) LOG.warn("Failed to report bad block " + block + " to namenode : "
                        (409: 7) LOG.info("BlockReport of " + bReport.getNumberOfBlocks()
                        (428: 7) LOG.info("sent block report, processed command:" + cmd);
                        (436: 7) LOG.debug("Sending heartbeat from service actor: " + this);
                        (499: 5) LOG.info("For namenode " + nnAddr + " using DELETEREPORT_INTERVAL of "
                        (543: 15) LOG.info("Took " + (endProcessCommands - startProcessCommands)
                        (575: 15) LOG.warn("BPOfferService for " + this + " interrupted");
                        (584: 11) LOG.warn(this + " is shutting down", re);
                        (588: 9) LOG.warn("RemoteException in offerService", re);
                        (596: 9) LOG.warn("IOException in offerService", e);
                        (618: 5) LOG.info(this + " beginning handshake with NN");
                        (626: 9) LOG.info("Problem connecting to server: " + nnAddr);
                        (631: 5) LOG.info("Block pool " + this + " successfully registered with NN");
                        (644: 7) LOG.info("BPOfferService " + this + " interrupted while " + stateString);
                        (658: 5) LOG.info(this + " starting to offer service");
                        (668: 9) LOG.fatal("Initialization failed for block pool " + this, ioe);
                        (678: 11) LOG.error("Exception in BPOfferService for " + this, ex);
                        (683: 7) LOG.warn("Unexpected exception in block pool " + this, ex);
                        (685: 7) LOG.warn("Ending block pool service for: " + this);
                        (708: 11) LOG.warn("Error processing datanode Command", ioe);
                        (719: 7) LOG.warn("Error reporting an error to NameNode " + nnAddr,
                    DataBlockScanner.java  (9 usages)
                        (96: 9) LOG.warn("Block Pool " + currentBpId + " is not alive");
                        (164: 7) LOG.warn("No block pool is up, going to wait");
                        (168: 9) LOG.warn("Received exception: " + ex);
                        (195: 7) LOG.warn("No block pool scanner found for block pool id: "
                        (225: 7) LOG.warn("No block pool scanner found for block pool id: "
                        (235: 7) LOG.warn("No block pool scanner found for block pool id: "
                        (265: 5) LOG.info("Added bpid=" + blockPoolId + " to blockPoolScannerMap, new size="
                        (271: 5) LOG.info("Removed bpid="+blockPoolId+" from blockPoolScannerMap");
                        (316: 9) LOG.warn("Periodic block scanner is not running");
                    DataNode.java  (55 usages)
                        (313: 7) LOG.info("Configured hostname is " + hostName);
                        (365: 5) LOG.info("Opened info server at " + infoHost + ":" + tmpInfoPort);
                        (376: 9) LOG.debug("Datanode listening for SSL on " + secInfoSocAddr);
                        (401: 9) LOG.info("Started plug-in " + p);
                        (403: 9) LOG.warn("ServicePlugin " + p + " could not be started", t);
                        (436: 5) LOG.info("Opened IPC server at " + ipcServer.getListenerAddress());
                        (484: 7) LOG.info("Periodic Block Verification scan disabled because " + reason);
                        (512: 7) LOG.info("Periodic Directory Tree Verification scan is disabled because " +
                        (539: 5) LOG.info("Opened streaming server at " + streamingAddr);
                        (552: 7) LOG.error("Cannot find BPOfferService for reporting block received for bpid="
                        (563: 7) LOG.error("Cannot find BPOfferService for reporting block receiving for bpid="
                        (574: 7) LOG.error("Cannot find BPOfferService for reporting block deleted for bpid="
                        (728: 7) LOG.info("New storage id " + bpRegistration.getStorageID()
                        (764: 7) LOG.info("Block token params received from NN: for block pool " +
                        (852: 7) LOG.info("Setting up storage: nsid=" + bpStorage.getNamespaceID()
                        (947: 7) LOG.debug("Connecting to datanode " + dnAddr + " addr=" + addr);
                        (969: 5) LOG.info("Datanode is " + dnId);
                        (990: 7) LOG.warn("Could not find an IP address for the \"default\" inteface.");
                        (1029: 11) LOG.trace("getBlockLocalPathInfo successful block=" + block
                        (1035: 11) LOG.trace("getBlockLocalPathInfo for block=" + block
                        (1071: 9) LOG.debug("Got: " + id.toString());
                        (1088: 11) LOG.info("Stopped plug-in " + p);
                        (1090: 11) LOG.warn("ServicePlugin " + p + " could not be stopped", t);
                        (1108: 9) LOG.warn("Exception shutting down DataNode", e);
                        (1124: 11) LOG.info("Waiting for threadgroup to exit, active threads is " +
                        (1149: 9) LOG.warn("Received exception in BlockPoolManager#shutDownAll: ", ie);
                        (1157: 9) LOG.warn("Exception when unlocking storage: " + ie, ie);
                        (1174: 5) LOG.warn("checkDiskError: exception: ", e);
                        (1197: 5) LOG.warn("DataNode.handleDiskError: Keep Running: " + hasEnoughResources);
                        (1215: 5) LOG.warn("DataNode is shutting down: " + errMsgr);
                        (1237: 7) LOG.info(errStr);
                        (1248: 7) LOG.warn("Can't replicate block " + block
                        (1262: 9) LOG.info(bpReg + " Starting thread to transfer " +
                        (1277: 9) LOG.warn("Failed to transfer block " + blocks[i], ie);
                        (1390: 30) DataTransferProtocol.LOG.debug(getClass().getSimpleName() + ": "
                        (1420: 11) LOG.debug("Connecting to datanode " + dnAddr);
                        (1463: 9) LOG.info(getClass().getSimpleName() + ": Transmitted " + b
                        (1471: 13) LOG.debug(getClass().getSimpleName() + ": close-ack=" + closeAck);
                        (1485: 9) LOG.warn(bpReg + ":Failed to transfer " + b + " to " +
                        (1512: 7) LOG.warn("Cannot find BPOfferService for reporting block received for bpid="
                        (1615: 9) LOG.warn("Received exception in Datanode#join: " + ex);
                        (1652: 9) LOG.warn("Unsupported URI schema in " + dirURI + ". Ignoring ...");
                        (1662: 9) LOG.warn("Invalid " + DFS_DATANODE_DATA_DIR_KEY + " "
                        (1698: 9) LOG.error("-r, --rack arguments are not supported anymore. RackID " +
                        (1755: 7) LOG.fatal("Exception in secureMain", e);
                        (1762: 7) LOG.warn("Exiting Datanode");
                        (1788: 13) LOG.warn("recoverBlocks FAILED: " + b, e);
                        (1887: 31) InterDatanodeProtocol.LOG.warn(
                        (1894: 31) InterDatanodeProtocol.LOG.warn(
                        (1940: 7) LOG.debug("block=" + block + ", (length=" + block.getNumBytes()
                        (2010: 31) InterDatanodeProtocol.LOG.warn("Failed to updateBlock (newblock="
                        (2045: 5) LOG.info(who + " calls recoverBlock(" + block
                        (2067: 11) LOG.debug("Got: " + id.toString());
                        (2207: 5) LOG.info("deleteBlockPool command received for block pool " + blockPoolId
                        (2210: 7) LOG.warn("The block pool "+blockPoolId+
                    DataStorage.java  (13 usages)
                        (159: 11) LOG.info("Storage directory " + dataDir + " does not exist");
                        (163: 11) LOG.info("Storage directory " + dataDir + " is not formatted");
                        (164: 11) LOG.info("Formatting ...");
                        (172: 9) LOG.warn("Ignoring storage directory " + dataDir
                        (260: 9) LOG.warn("Invalid directory in: " + data.getCanonicalPath() + ": "
                        (447: 5) LOG.info("Upgrading storage directory " + sd.getRoot()
                        (486: 5) LOG.info("Upgrade of " + sd.getRoot()+ " is complete");
                        (547: 5) LOG.info("Rolling back storage directory " + sd.getRoot()
                        (560: 5) LOG.info("Rollback of " + sd.getRoot() + " is complete");
                        (578: 5) LOG.info("Finalizing upgrade for storage directory "
                        (600: 13) LOG.error("Finalize upgrade for " + dataDirPath + " failed", ex);
                        (602: 11) LOG.info("Finalize upgrade for " + dataDirPath + " is complete");
                        (667: 5) LOG.info( hardLink.linkStats.report() );
                    DataXceiver.java  (35 usages)
                        (121: 7) LOG.debug("Number of active connections is: "
                        (171: 11) LOG.info("Failed to read expected encryption handshake from client " +
                        (205: 15) LOG.debug("Cached " + s.toString() + " closing after " + opsProcessed + " ops");
                        (223: 7) LOG.error(datanode.getDisplayName() + ":DataXceiver error processing " +
                        (229: 9) LOG.debug(datanode.getDisplayName() + ":Number of active connections is: "
                        (273: 9) LOG.info(msg);
                        (290: 13) LOG.warn("Client " + s.getInetAddress() + " did not send a valid status " +
                        (295: 11) LOG.debug("Error reading client status response. Will close connection.", ioe);
                        (305: 9) LOG.trace(dnR + ":Ignoring exception while serving " + block + " to " +
                        (315: 7) LOG.warn(dnR + ":Got exception while serving " + block + " to "
                        (353: 7) LOG.debug("opWriteBlock: stage=" + stage + ", clientname=" + clientname
                        (359: 7) LOG.debug("isDatanode=" + isDatanode
                        (362: 7) LOG.debug("writeBlock receive buf size " + s.getReceiveBufferSize() +
                        (371: 5) LOG.info("Receiving " + block + " src: " + remoteAddress + " dest: "
                        (410: 11) LOG.debug("Connecting to datanode " + mirrorNode);
                        (453: 15) LOG.info("Datanode " + targets.length +
                        (477: 13) LOG.error(datanode + ":Exception transfering block " +
                        (481: 13) LOG.info(datanode + ":Exception transfering " +
                        (491: 11) LOG.info("Datanode " + targets.length +
                        (512: 13) LOG.trace("TRANSFER: send close-ack");
                        (531: 9) LOG.info("Received " + block + " src: " + remoteAddress + " dest: "
                        (537: 7) LOG.info("opWriteBlock " + block + " received exception " + ioe);
                        (599: 9) LOG.debug("block=" + block + ", bytesPerCRC=" + bytesPerCRC
                        (635: 9) LOG.warn("Invalid access token in request from " + remoteAddress
                        (647: 7) LOG.info(msg);
                        (675: 7) LOG.info("Copied " + block + " to " + s.getRemoteSocketAddress());
                        (678: 7) LOG.info("opCopyBlock " + block + " received exception " + ioe);
                        (711: 9) LOG.warn("Invalid access token in request from " + remoteAddress
                        (722: 7) LOG.warn(msg);
                        (738: 9) LOG.debug("Connecting to datanode " + dnAddr);
                        (798: 7) LOG.info("Moved " + block + " from " + s.getRemoteSocketAddress());
                        (803: 7) LOG.info(errMsg);
                        (821: 9) LOG.warn("Error writing reply back to " + s.getRemoteSocketAddress());
                        (882: 9) LOG.debug("Checking block access token for block '" + blk.getBlockId()
                        (906: 11) LOG.warn("Block token verification failed: op=" + op
                    DataXceiverServer.java  (7 usages)
                        (77: 6) LOG.info("Balancing bandwith is "+ bandwidth + " bytes/s");
                        (157: 11) LOG.warn(datanode.getDisplayName() + ":DataXceiverServer: ", ace);
                        (161: 9) LOG.warn(datanode.getDisplayName() + ":DataXceiverServer: ", ie);
                        (167: 9) LOG.warn("DataNode is out of memory. Will retry in 30 seconds.", ie);
                        (174: 9) LOG.error(datanode.getDisplayName()
                        (182: 7) LOG.warn(datanode.getDisplayName()
                        (193: 7) LOG.warn(datanode.getDisplayName() + ":DataXceiverServer.kill(): ", ie);
                    DirectoryScanner.java  (11 usages)
                        (243: 5) LOG.info("Periodic Directory Tree Verification scan starting at "
                        (268: 9) LOG.warn("this cycle terminating immediately because 'shouldRun' has been deactivated");
                        (277: 7) LOG.error("Exception during DirectoryScanner execution - will continue next cycle", e);
                        (280: 7) LOG.error("System Error during DirectoryScanner execution - permanently terminating periodic scanner", er);
                        (287: 7) LOG.warn("DirectoryScanner: shutdown has been called, but periodic scanner not started");
                        (289: 7) LOG.warn("DirectoryScanner: shutdown has been called");
                        (298: 9) LOG.error("interrupted while waiting for masterThread to " +
                        (306: 9) LOG.error("interrupted while waiting for reportCompileThreadPool to " +
                        (395: 9) LOG.info(statsRecord.toString());
                        (457: 9) LOG.error("Error compiling report", ex);
                        (507: 9) LOG.warn("Exception occured while compiling report: ", ioe);
                    DNConf.java  (1 usage)
                        (111: 16) DataNode.LOG.info("dfs.blockreport.initialDelay is greater than " +
                    ReplicaInfo.java  (2 usages)
                        (182: 18) DataNode.LOG.info("detachFile failed to delete temporary file " +
                        (208: 16) DataNode.LOG.info("CopyOnWrite for block " + this);
                    ReplicaInPipeline.java  (2 usages)
                        (175: 16) DataNode.LOG.debug("writeTo blockfile is " + blockFile +
                        (177: 16) DataNode.LOG.debug("writeTo metafile is " + metaFile +
                org.apache.hadoop.hdfs.server.datanode.fsdataset.impl  (62 usages)
                    BlockPoolSlice.java  (3 usages)
                        (200: 23) FsDatasetImpl.LOG.warn("Two block files with the same block id exist " +
                        (237: 23) FsDatasetImpl.LOG.warn("Wrong version (" + version + ") for metadata file "
                        (266: 21) FsDatasetImpl.LOG.warn(e);
                    FsDatasetAsyncDiskService.java  (6 usages)
                        (137: 7) LOG.warn("AsyncDiskService has already shut down.");
                        (139: 7) LOG.info("Shutting down all async disk service threads");
                        (147: 7) LOG.info("All async disk service threads have been shut down");
                        (157: 5) LOG.info("Scheduling " + block.getLocalBlock()
                        (193: 9) LOG.warn("Unexpected error trying to delete block "
                        (201: 9) LOG.info("Deleted " + block.getBlockPoolId() + " "
                    FsDatasetImpl.java  (49 usages)
                        (198: 7) LOG.info("Added volume - " + dir);
                        (297: 9) LOG.debug("b=" + b + ", volumeMap=" + volumeMap);
                        (415: 7) LOG.debug("addBlock: Moved " + srcmeta + " to " + dstmeta
                        (423: 5) LOG.info("truncateBlock: blockFile=" + blockFile
                        (488: 5) LOG.info("Appending to " + replicaInfo);
                        (537: 7) LOG.debug("Renaming " + oldmeta + " to " + newmeta);
                        (549: 7) LOG.debug("Renaming " + blkfile + " to " + newBlkFile
                        (558: 9) LOG.warn("Cannot move meta file " + newmeta +
                        (623: 5) LOG.info("Recover failed append to " + b);
                        (640: 5) LOG.info("Recover failed close " + b);
                        (668: 7) LOG.debug("Renaming " + oldmeta + " to " + newmeta);
                        (704: 5) LOG.info("Recover RBW replica " + b);
                        (715: 5) LOG.info("Recovering " + rbw);
                        (751: 5) LOG.info("Convert " + b + " from Temporary to RBW, visible length="
                        (838: 7) LOG.debug("Changing meta file offset of block " + b + " from " +
                        (907: 9) LOG.warn("Block " + b + " unfinalized and removed. " );
                        (921: 7) LOG.warn("No file exists for block: " + b);
                        (926: 7) LOG.warn("Not able to delete the block file: " + blockFile);
                        (930: 9) LOG.warn("Not able to delete the meta block file: " + metaFile);
                        (1032: 7) LOG.debug("b=" + b + ", f=" + f);
                        (1074: 11) LOG.warn("Failed to delete replica " + invalidBlks[i]
                        (1080: 11) LOG.warn("Failed to delete replica " + invalidBlks[i]
                        (1087: 11) LOG.warn("Failed to delete replica " + invalidBlks[i]
                        (1093: 11) LOG.warn("Failed to delete replica " + invalidBlks[i]
                        (1100: 11) LOG.warn("Failed to delete replica " + invalidBlks[i]
                        (1171: 15) LOG.warn("Removing replica " + bpid + ":" + b.getBlockId()
                        (1181: 5) LOG.warn("Removed " + removedBlocks + " out of " + totalBlocks +
                        (1219: 7) LOG.warn("Error registering FSDatasetState MBean", e);
                        (1221: 5) LOG.info("Registered FSDatasetState MBean");
                        (1288: 13) LOG.warn("Deleted a metadata file without a block "
                        (1301: 11) LOG.warn("Removed block " + blockId
                        (1306: 13) LOG.warn("Deleted a metadata file for the deleted block "
                        (1324: 9) LOG.warn("Added missing block to memory " + diskBlockInfo);
                        (1334: 11) LOG.warn("Block file " + memFile.getAbsolutePath()
                        (1344: 9) LOG.warn("Block file in volumeMap "
                        (1351: 9) LOG.warn("Updating generation stamp for block " + blockId
                        (1362: 13) LOG.warn("Metadata file in memory "
                        (1375: 11) LOG.warn("Updating generation stamp for block " + blockId
                        (1386: 9) LOG.warn("Updating size of block " + blockId + " from "
                        (1394: 7) LOG.warn("Reporting the block " + corruptBlock
                        (1399: 9) LOG.warn("Failed to repot bad block " + corruptBlock, e);
                        (1430: 5) LOG.info("initReplicaRecovery: " + block + ", recoveryId=" + recoveryId
                        (1478: 7) LOG.info("initReplicaRecovery: update recovery id for " + block
                        (1484: 7) LOG.info("initReplicaRecovery: changing replica state for "
                        (1499: 5) LOG.info("updateReplica: " + oldBlock
                        (1589: 5) LOG.info("Adding block pool " + bpid);
                        (1597: 5) LOG.info("Removing block pool " + bpid);
                        (1633: 9) LOG.warn(e.getMessage());
                        (1663: 11) LOG.warn(bpid + " has some block files, cannot delete unless forced");
                    FsDatasetUtil.java  (1 usage)
                        (90: 19) FsDatasetImpl.LOG.warn("Block " + blockFile + " does not have a metafile!");
                    FsVolumeList.java  (2 usages)
                        (126: 23) FsDatasetImpl.LOG.warn("Removing failed volume " + fsv + ": ",e);
                        (140: 21) FsDatasetImpl.LOG.warn("Completed checkDirs. Removed " + removedVols.size()
                    RollingLogsImpl.java  (1 usage)
                        (208: 26) DataBlockScanner.LOG.warn("Failed to read next line.", e);
                org.apache.hadoop.hdfs.server.datanode.web.resources  (1 usage)
                    DatanodeWebHdfsMethods.java  (1 usage)
                        (96: 7) LOG.trace("HTTP " + op.getValue().getType() + ": " + op + ", " + path
                org.apache.hadoop.hdfs.server.journalservice  (16 usages)
                    JournalService.java  (16 usages)
                        (145: 9) LOG.warn("Ignore stop request since the service is in " + current
                        (190: 5) LOG.info("Starting rpc server");
                        (199: 11) LOG.info("handshake completed");
                        (206: 11) LOG.info("Registration completed");
                        (210: 9) LOG.warn("Encountered exception ", ioe);
                        (212: 9) LOG.warn("Encountered exception ", e);
                        (218: 9) LOG.warn("Encountered exception ", ie);
                        (226: 7) LOG.warn("Encountered exception ", e);
                        (244: 7) LOG.trace("Received journal " + firstTxnId + " " + numTxns);
                        (255: 7) LOG.trace("Received startLogSegment " + txid);
                        (266: 5) LOG.info("Fenced by " + fencerInfo + " with epoch " + epoch);
                        (296: 7) LOG.warn(errorMsg);
                        (306: 7) LOG.warn(errorMsg);
                        (320: 7) LOG.warn(errorMsg);
                        (326: 7) LOG.warn(errorMsg);
                        (351: 7) LOG.error(msg);
                org.apache.hadoop.hdfs.server.namenode  (369 usages)
                    BackupImage.java  (14 usages)
                        (122: 11) LOG.info("Storage directory " + sd.getRoot() + " is not formatted.");
                        (123: 11) LOG.info("Formatting ...");
                        (164: 7) LOG.trace("Got journal, " +
                        (202: 9) LOG.debug("data:" + StringUtils.byteToHexString(data));
                        (252: 7) LOG.info("Loading edits into backupnode to try to catch up from txid "
                        (275: 9) LOG.debug("Logs rolled while catching up to current segment");
                        (292: 9) LOG.warn("Unable to find stream starting with " + editLog.getCurSegmentTxId()
                        (300: 9) LOG.info("Going to finish converging with remaining " + remainingTxns
                        (312: 7) LOG.info("Successfully synced BackupNode with NameNode at txnid " +
                        (324: 7) LOG.debug("State transition " + bnState + " -> " + newState);
                        (343: 9) LOG.info("Stopped applying edits to prepare for checkpoint.");
                        (367: 5) LOG.info("Waiting until the NameNode rolls its edit logs in order " +
                        (376: 9) LOG.warn("Interrupted waiting for namespace to freeze", ie);
                        (380: 5) LOG.info("BackupNode namespace frozen.");
                    BackupNode.java  (10 usages)
                        (201: 9) LOG.error("Failed to report to name-node.", e);
                        (253: 9) LOG.warn(errorMsg);
                        (259: 9) LOG.warn(errorMsg);
                        (290: 7) LOG.info("Fenced by " + fencerInfo + " with epoch " + epoch);
                        (325: 9) LOG.info("Problem connecting to server: " + nnAddress);
                        (329: 11) LOG.warn("Encountered exception ", e);
                        (377: 9) LOG.info("Problem connecting to name-node: " + nnRpcAddress);
                        (381: 11) LOG.warn("Encountered exception ", e);
                        (394: 7) LOG.error(msg);
                        (411: 7) LOG.fatal(errorMsg);
                    CancelDelegationTokenServlet.java  (2 usages)
                        (52: 7) LOG.info("Request for token received with no authentication from "
                        (78: 7) LOG.info("Exception while cancelling token. Re-throwing. ", e);
                    CheckpointConf.java  (1 usage)
                        (60: 9) LOG.warn("Configuration key " + key + " is deprecated! Ignoring..." +
                    Checkpointer.java  (10 usages)
                        (81: 7) LOG.warn("Checkpointer got exception", e);
                        (102: 5) LOG.info("Checkpoint Period : " +
                        (105: 5) LOG.info("Transactions count is  : " +
                        (150: 9) LOG.error("Exception in doCheckpoint: ", e);
                        (152: 9) LOG.error("Throwable Exception in doCheckpoint: ", e);
                        (205: 5) LOG.debug("Doing checkpoint. Last applied: " + lastApplied);
                        (215: 9) LOG.info("Unable to roll forward using only logs. Downloading " +
                        (237: 9) LOG.info("Loading image with txid " + sig.mostRecentCheckpointTxId);
                        (272: 5) LOG.info("Checkpoint completed in "
                        (298: 5) LOG.info("Checkpointer about to load edits from " +
                    ClusterJspHelper.java  (1 usage)
                        (229: 9) LOG.warn("Cluster console encounters a not handled situtation.");
                    EditLogBackupOutputStream.java  (2 usages)
                        (68: 15) Storage.LOG.error("Error connecting to: " + bnAddress, e);
                        (124: 7) LOG.info("Nothing to flush");
                    EditLogFileInputStream.java  (4 usages)
                        (168: 9) LOG.error("caught exception initializing " + this, e);
                        (199: 17) LOG.debug("skipping " + skipAmt + " bytes at the end " +
                        (225: 7) LOG.error("nextValidOp: got exception while reading " + this, e);
                        (278: 7) LOG.warn("Log file " + file + " has no valid header", e);
                    EditLogFileOutputStream.java  (2 usages)
                        (182: 7) LOG.info("Nothing to flush");
                        (219: 20) FSNamesystem.LOG.debug("Preallocated " + total + " bytes at the end of " +
                    FileJournalManager.java  (13 usages)
                        (110: 7) LOG.warn("Unable to start log segment " + txid +
                        (125: 5) LOG.info("Finalizing edits file " + inprogressFile + " -> " + dstFile);
                        (152: 5) LOG.info("Purging logs older than " + minTxIdToKeep);
                        (219: 11) LOG.error("Edits file " + f + " has improperly formatted " +
                        (233: 11) LOG.error("In-progress edits file " + f + " has improperly " +
                        (247: 5) LOG.debug(this + ": selecting input streams starting at " + fromTxId +
                        (258: 11) LOG.debug("passing over " + elf + " because it is in progress " +
                        (265: 11) LOG.error("got IOException while trying to validate header of " +
                        (272: 9) LOG.debug("passing over " + elf + " because it ends at " +
                        (279: 7) LOG.debug("selecting edit log stream " + elf);
                        (287: 5) LOG.info("Recovering unfinalized segments in " + currentDir);
                        (298: 11) LOG.info("Deleting zero-length edit log file " + elf);
                        (317: 11) LOG.info("Moving aside edit log file that seems to have zero " +
                    FSDirectory.java  (5 usages)
                        (148: 14) NameNode.LOG.info("Caching file names occuring more than " + threshold
                        (1296: 16) NameNode.LOG.warn("FSDirectory.updateCountNoQuotaCheck - unexpected ", e);
                        (1618: 16) NameNode.LOG.error("FSDirectory.verifyFsLimits - " + e.getLocalizedMessage());
                        (1668: 16) NameNode.LOG.warn("FSDirectory.addChildNoQuotaCheck - unexpected", e);
                        (1780: 18) NameNode.LOG.warn("Quota violation in image for " + path +
                    FSEditLog.java  (18 usages)
                        (219: 7) LOG.warn("Initializing shared journals for READ, already open for READ",
                        (251: 7) LOG.error("No edits directories configured!");
                        (317: 7) LOG.debug("Closing log when already closed");
                        (330: 9) LOG.warn("Error closing journalSet", ioe);
                        (585: 13) LOG.fatal(msg, new Exception());
                        (608: 11) LOG.fatal(msg, new Exception());
                        (650: 5) LOG.info(buf);
                        (908: 5) LOG.info("Rolling edit logs");
                        (923: 5) LOG.info("Started a new log segment at txid " + txid);
                        (936: 11) LOG.warn(mess);
                        (954: 5) LOG.info("Starting log segment at " + segmentTxId);
                        (997: 5) LOG.info("Ending log segment " + curSegmentTxId);
                        (1033: 7) LOG.warn("All journals failed to abort", e);
                        (1117: 7) LOG.info("Backup node " + bnReg + " re-registers");
                        (1121: 5) LOG.info("Registering new backup node: " + bnReg);
                        (1130: 7) LOG.info("Removing backup journal " + bjm);
                        (1221: 9) LOG.error(e);
                        (1293: 7) LOG.warn("No class configured for " +uriScheme
                    FSEditLogLoader.java  (15 usages)
                        (90: 15) FSImage.LOG.info("Edits file " + edits.getName()
                        (109: 7) LOG.trace("Acquiring write lock to replay edit log");
                        (139: 21) FSImage.LOG.error(errorMessage, e);
                        (175: 13) LOG.error("Encountered exception on operation " + op, e);
                        (195: 15) LOG.info("replaying edit log: " + deltaTxId + "/" + numTxns
                        (202: 31) MetaRecoveryContext.LOG.warn("Stopped reading edit log at " +
                        (216: 9) LOG.trace("replaying edit log finished");
                        (231: 7) LOG.trace("replaying edit log: " + op);
                        (238: 22) FSNamesystem.LOG.debug(op.opCode + ": " + addCloseOp.path +
                        (270: 26) FSNamesystem.LOG.debug("Reopening an already-closed file " +
                        (293: 22) FSNamesystem.LOG.debug(op.opCode + ": " + addCloseOp.path +
                        (332: 22) FSNamesystem.LOG.debug(op.opCode + ": " + updateOp.path +
                        (613: 13) FSImage.LOG.debug(sb.toString());
                        (667: 17) FSImage.LOG.warn("Caught exception after reading " + numValid +
                        (671: 17) FSImage.LOG.warn("After resync, position is " + in.getPosition());
                    FSImage.java  (31 usages)
                        (204: 7) LOG.trace("Data dir states:\n  " +
                        (240: 9) LOG.info("Storage directory " + sd.getRoot() + " is not formatted.");
                        (241: 9) LOG.info("Formatting ...");
                        (345: 7) LOG.info("Starting upgrade of image directory " + sd.getRoot()
                        (366: 9) LOG.error("Failed to move aside pre-upgrade storage " +
                        (389: 9) LOG.error("Unable to rename temp to previous for " + sd.getRoot(), ioe);
                        (393: 7) LOG.info("Upgrade of " + sd.getRoot() + " is complete.");
                        (417: 9) LOG.info("Storage directory " + sd.getRoot()
                        (449: 7) LOG.info("Rolling back storage directory " + sd.getRoot()
                        (463: 7) LOG.info("Rollback of " + sd.getRoot()+ " is complete.");
                        (471: 7) LOG.info("Directory " + prevDir + " does not exist.");
                        (472: 7) LOG.info("Finalize upgrade for " + sd.getRoot()+ " is not required.");
                        (475: 5) LOG.info("Finalizing upgrade for storage directory "
                        (486: 5) LOG.info("Finalize upgrade for " + sd.getRoot()+ " is complete.");
                        (565: 5) LOG.debug("Reloading namespace from " + file);
                        (611: 5) LOG.debug("Planning to load image :\n" + imageFile);
                        (613: 7) LOG.debug("Planning to load edit log stream: " + l);
                        (616: 7) LOG.info("No edit log streams selected.");
                        (692: 5) LOG.debug("About to load edits:\n  " + Joiner.on("\n  ").join(editStreams));
                        (700: 9) LOG.info("Reading " + editIn + " expecting start txid #" +
                        (762: 5) LOG.info("Loaded image for txid " + txId + " from " + curFile);
                        (809: 9) LOG.info("Cancelled image saving for " + sd.getRoot() +
                        (813: 9) LOG.error("Unable to save image for " + sd.getRoot(), t);
                        (831: 11) LOG.error("Caught interrupted exception while waiting for thread " +
                        (943: 7) LOG.warn("Unable to purge old storage", e);
                        (957: 9) LOG.warn("Unable to rename checkpoint in " + sd, ioe);
                        (977: 9) LOG.warn("Unable to delete cancelled checkpoint in " + sd);
                        (992: 7) LOG.debug("renaming  " + ckpt.getAbsolutePath()
                        (1031: 5) LOG.info("Start checkpoint at txid " + getEditLog().getLastWrittenTxId());
                        (1051: 7) LOG.error(msg);
                        (1071: 5) LOG.info("End checkpoint at txid " + getEditLog().getLastWrittenTxId());
                    FSImageFormat.java  (6 usages)
                        (218: 9) LOG.info("Loading image file " + curFile + " using " + compression);
                        (222: 9) LOG.info("Number of files = " + numFiles);
                        (244: 7) LOG.info("Image file of size " + curFile.length() + " loaded in "
                        (436: 7) LOG.info("Number of files under construction = " + size);
                        (572: 9) LOG.info("Saving image file " + newFile +
                        (599: 7) LOG.info("Image file of size " + newFile.length() + " saved in "
                    FSImagePreTransactionalStorageInspector.java  (6 usages)
                        (165: 7) LOG.error("This is a rare failure scenario!!!");
                        (166: 7) LOG.error("Image checkpoint time " + latestNameCheckpointTime +
                        (168: 7) LOG.error("Name-node will treat the image as the latest state of " +
                        (193: 5) LOG.debug(
                        (229: 13) LOG.warn("Unable to delete dir " + curFile + " before rename");
                        (259: 7) LOG.debug(
                    FSImageTransactionalStorageInspector.java  (6 usages)
                        (54: 7) LOG.info("No version file in " + sd.getRoot());
                        (64: 7) LOG.warn("Unable to determine the max transaction ID seen by " + sd, ioe);
                        (73: 7) LOG.warn("Unable to inspect storage directory " + currentDir,
                        (79: 7) LOG.debug("Checking file " + f);
                        (90: 13) LOG.error("Image file " + f + " has improperly formatted " +
                        (95: 11) LOG.warn("Found image file at " + f + " but storage directory is " +
                    FSNamesystem.java  (57 usages)
                        (459: 7) LOG.warn("Only one image storage directory ("
                        (464: 7) LOG.warn("Only one namespace edits storage directory ("
                        (481: 5) LOG.info("Finished loading FSImage in " + timeTakenToLoadFSImage + " msecs");
                        (513: 7) LOG.info("fsOwner             = " + fsOwner);
                        (514: 7) LOG.info("supergroup          = " + supergroup);
                        (515: 7) LOG.info("isPermissionEnabled = " + isPermissionEnabled);
                        (527: 9) LOG.info("Determined nameservice ID: " + nameserviceId);
                        (529: 7) LOG.info("HA Enabled: " + haEnabled);
                        (531: 9) LOG.warn("Configured NNs:\n" + DFSUtil.nnAddressesAsString(conf));
                        (561: 7) LOG.info("Append Enabled: " + supportAppends);
                        (583: 7) LOG.error(getClass().getSimpleName() + " initialization failed.", e);
                        (587: 7) LOG.error(getClass().getSimpleName() + " initialization failed.", re);
                        (725: 5) LOG.info("Starting services required for active state");
                        (736: 9) LOG.info("Catching up to latest edits from old active before " +
                        (747: 11) LOG.info("Reprocessing replication and invalidation queues");
                        (752: 11) LOG.debug("NameNode metadata after re-processing " +
                        (758: 9) LOG.info("Will take over writing edit logs at txnid " +
                        (792: 5) LOG.info("Stopping services started for active state");
                        (822: 5) LOG.info("Starting services required for standby state");
                        (853: 5) LOG.info("Stopping services started for standby state");
                        (907: 9) LOG.warn("!!! WARNING !!!" +
                        (957: 11) LOG.warn("Edits URI " + dir + " listed multiple times in " +
                        (965: 9) LOG.warn("Edits URI " + dir + " listed multiple times in " +
                        (1063: 9) LOG.error("Error closing FSDirectory", ie);
                        (1402: 20) FSNamesystem.LOG.debug("concat " + Arrays.toString(srcs) +
                        (2074: 9) LOG.info("recoverLease: " + lease + ", src=" + src +
                        (2086: 11) LOG.info("startFile: recover " + lease + ", src=" + src + " client "
                        (2618: 13) LOG.info("BLOCK* checkFileProgress: " + block
                        (2630: 11) LOG.info("BLOCK* checkFileProgress: " + b
                        (2918: 9) LOG.debug("Adjusting safe-mode totals for deletion of " + src + ":" +
                        (3149: 5) LOG.info("Recovering " + lease + ", src=" + src);
                        (3291: 9) LOG.warn("Unexpected exception while updating disk space.", e);
                        (3330: 7) LOG.info("commitBlockSynchronization(lastblock=" + lastblock
                        (3407: 7) LOG.info("commitBlockSynchronization(newblock=" + lastblock
                        (3413: 7) LOG.info("commitBlockSynchronization(" + lastblock + ") successful");
                        (3607: 28) FSNamesystem.LOG.warn(lowResourcesMsg + "Entering safe mode.");
                        (3609: 28) FSNamesystem.LOG.warn(lowResourcesMsg + "Already in safe mode.");
                        (3620: 22) FSNamesystem.LOG.error("Exception in NameNodeResourceMonitor: ", e);
                        (3783: 7) LOG.info("New namespace image has been created");
                        (3909: 9) LOG.warn("The threshold value should't be greater than 1, threshold: " + threshold);
                        (3918: 7) LOG.info(DFS_NAMENODE_SAFEMODE_THRESHOLD_PCT_KEY + " = " + threshold);
                        (3919: 7) LOG.info(DFS_NAMENODE_SAFEMODE_MIN_DATANODES_KEY + " = " + datanodeThreshold);
                        (3920: 7) LOG.info(DFS_NAMENODE_SAFEMODE_EXTENSION_KEY + "     = " + extension);
                        (4023: 7) LOG.info("initializing replication queues");
                        (4293: 9) LOG.debug("Adjusting block totals from " +
                        (4327: 9) LOG.info("NameNode is being shutdown, exit SafeModeMonitor thread");
                        (4347: 9) LOG.error("Unexpected safe mode action");
                        (4482: 7) LOG.info("Number of blocks under construction: " + numUCBlocks);
                        (4564: 7) LOG.info("Roll Edit Log from " + Server.getRemoteAddress());
                        (4582: 7) LOG.info("Start checkpoint for " + bnReg.getAddress());
                        (4600: 7) LOG.info("End checkpoint for " + registration.getAddress());
                        (4783: 5) LOG.info("Registered FSNamesystemState MBean");
                        (4952: 7) LOG.info("updatePipeline(block=" + oldBlock
                        (4963: 5) LOG.info("updatePipeline(" + oldBlock + ") successfully to " + newBlock);
                        (4983: 7) LOG.warn(msg);
                        (5148: 7) LOG.info("list corrupt file blocks returned: " + count);
                        (5216: 9) LOG.warn("trying to get DT with no secret manager running");
                    FSPermissionChecker.java  (1 usage)
                        (115: 7) LOG.debug("ACCESS CHECK: " + this
                    GetDelegationTokenServlet.java  (3 usages)
                        (53: 7) LOG.info("Request for token received with no authentication from "
                        (59: 5) LOG.info("Sending token: {" + ugi.getUserName() + "," + req.getRemoteAddr() +"}");
                        (80: 7) LOG.info("Exception while sending token. Re-throwing ", e);
                    GetImageServlet.java  (6 usages)
                        (98: 9) LOG.warn("Received non-NN/SNN/administrator request for image or edits from "
                        (110: 9) LOG.warn("Received an invalid request file transfer request " +
                        (239: 7) LOG.warn("Received null remoteUser while authorizing access to getImage servlet");
                        (264: 9) LOG.info("GetImageServlet allowing checkpointer: " + remoteUser);
                        (270: 7) LOG.info("GetImageServlet allowing administrator: " + remoteUser);
                        (274: 5) LOG.info("GetImageServlet rejecting: " + remoteUser);
                    INodeDirectory.java  (1 usage)
                        (371: 18) NameNode.LOG.warn("Inconsistent diskspace for directory "
                    JournalSet.java  (11 usages)
                        (121: 9) LOG.error("Unable to abort stream " + stream, ioe);
                        (247: 9) LOG.info("Skipping jas " + jas + " since it's disabled");
                        (253: 9) LOG.warn("Unable to determine input streams from " + jas.getManager() +
                        (318: 7) LOG.error("Disabling journal " + j);
                        (356: 11) LOG.fatal(msg, t);
                        (368: 11) LOG.error("Error: " + status + " failed for (journal " + jas + ")", t);
                        (377: 7) LOG.error("Error: " + message);
                        (528: 7) LOG.error("Error in setting outputbuffer capacity");
                        (599: 11) LOG.warn("Cannot list edit logs in " + fjm, t);
                        (623: 13) LOG.debug("Found gap in logs at " + curStartTxId + ": " +
                        (641: 7) LOG.debug("Generated manifest for logs since " + fromTxId + ":"
                    LeaseManager.java  (14 usages)
                        (140: 9) LOG.debug(src + " not found in lease.paths (=" + lease.paths + ")");
                        (147: 9) LOG.error(lease + " not found in sortedLeases");
                        (160: 7) LOG.warn("Removing non-existent lease! holder=" + holder +
                        (336: 7) LOG.debug(getClass().getSimpleName() + ".changelease: " +
                        (348: 9) LOG.debug("changeLease: replacing " + oldpath + " with " + newpath);
                        (360: 9) LOG.debug(LeaseManager.class.getSimpleName()
                        (370: 7) LOG.debug(LeaseManager.class.getSimpleName() + ".findLease: prefix=" + prefix);
                        (422: 13) LOG.debug(name + " is interrupted", ie);
                        (443: 9) LOG.error(ioe);
                        (461: 7) LOG.info(oldest + " has expired hard limit");
                        (476: 15) LOG.debug("Lease recovery for " + p + " is complete. File closed.");
                        (478: 15) LOG.debug("Started block recovery " + p + " lease " + oldest);
                        (486: 11) LOG.error("Cannot release the path " + p + " in the lease "
                        (523: 9) LOG.warn("Encountered exception ", ie);
                    ListPathsServlet.java  (1 usage)
                        (207: 7) LOG.warn("ListPathServlet encountered InterruptedException", e);
                    MetaRecoveryContext.java  (6 usages)
                        (64: 7) LOG.info(prompt);
                        (66: 9) LOG.info("automatically choosing " + firstChoice);
                        (85: 7) LOG.error("I'm sorry, I cannot understand your response.\n");
                        (96: 5) LOG.error(prompt);
                        (105: 7) LOG.info("Continuing");
                        (119: 5) LOG.error("Exiting on user request.");
                    NameCache.java  (1 usage)
                        (143: 5) LOG.info("initialized with " + size() + " entries " + lookups + " lookups");
                    NameNode.java  (28 usages)
                        (292: 5) LOG.info("Setting ADDRESS " + address);
                        (396: 5) LOG.info("Web-server up at: " + hostPort);
                        (441: 7) LOG.fatal(e.toString());
                        (490: 9) LOG.warn("ServicePlugin " + p + " could not be started", t);
                        (493: 5) LOG.info(getRole() + " RPC up at: " + rpcServer.getRpcAddress());
                        (495: 7) LOG.info(getRole() + " service RPC up at: "
                        (508: 11) LOG.warn("ServicePlugin " + p + " could not be stopped", t);
                        (557: 7) LOG.error("Exception while stopping httpserver", e);
                        (634: 7) LOG.info("Caught interrupted exception ", ie);
                        (652: 7) LOG.warn("Encountered exception while exiting state ", e);
                        (799: 7) LOG.fatal("No shared edits directory configured for namespace " +
                        (848: 7) LOG.error("Could not initialize shared edits dir", ioe);
                        (857: 11) LOG.warn("Could not unlock storage directories", ioe);
                        (888: 7) LOG.debug("Beginning to copy stream " + stream + " to shared edits");
                        (893: 11) LOG.trace("copying op: " + op);
                        (905: 11) LOG.debug("ending log segment because of END_LOG_SEGMENT op in " + stream);
                        (911: 9) LOG.debug("ending log segment because of end of stream in " + stream);
                        (957: 15) LOG.fatal("Must specify a valid cluster ID after the "
                        (967: 15) LOG.fatal("Must specify a valid cluster ID after the "
                        (1015: 13) LOG.fatal("Invalid argument: " + args[i]);
                        (1067: 25) MetaRecoveryContext.LOG.info("starting recovery...");
                        (1074: 27) MetaRecoveryContext.LOG.info("RECOVERY COMPLETE");
                        (1076: 27) MetaRecoveryContext.LOG.info("RECOVERY FAILED: caught exception", e);
                        (1079: 27) MetaRecoveryContext.LOG.info("RECOVERY FAILED: caught exception", e);
                        (1198: 7) LOG.debug("Setting " + FS_DEFAULT_NAME_KEY + " to " + defaultUri.toString());
                        (1224: 7) LOG.fatal("Exception in namenode join", e);
                        (1309: 7) LOG.fatal(message, t);
                        (1431: 9) LOG.warn("Allowing manual HA control from " +
                    NamenodeFsck.java  (15 usages)
                        (190: 7) LOG.info(msg);
                        (235: 7) LOG.warn(errMsg, e);
                        (409: 9) LOG.info("Fsck: ignoring open file " + path);
                        (430: 7) LOG.info("Fsck: deleted corrupt file " + path);
                        (432: 7) LOG.error("Fsck: error deleting corrupted file " + path, e);
                        (461: 9) LOG.warn("Fsck: can't copy the remains of " + fullName + " to " +
                        (496: 11) LOG.error("Fsck: could not copy block " + lblock.getBlock() +
                        (506: 9) LOG.warn("Fsck: there were errors copying the remains of the " +
                        (509: 9) LOG.info("Fsck: copied the remains of the corrupted file " +
                        (513: 7) LOG.error("copyBlocksToLostFound: error processing " + fullName, e);
                        (545: 9) LOG.info("Could not obtain block from any node:  " + ie);
                        (568: 9) LOG.info("Failed to connect to " + targetAddr + ":" + ex);
                        (596: 7) LOG.error("Error reading block", e);
                        (634: 9) LOG.warn("Cannot use /lost+found : a regular file with this name exists.");
                        (645: 7) LOG.warn("Cannot initialize /lost+found .");
                    NameNodeHttpServer.java  (3 usages)
                        (90: 11) LOG.info("Added filter '" + name + "' (class=" + classname + ")");
                        (111: 11) LOG.error("WebHDFS and security are enabled, but configuration property '" +
                        (125: 11) LOG.error("WebHDFS and security are enabled, but configuration property '" +
                    NameNodeResourceChecker.java  (3 usages)
                        (85: 9) LOG.debug("Space available on volume '" + volume + "' is "
                        (89: 9) LOG.warn("Space available on volume '" + volume + "' is "
                        (193: 7) LOG.debug("Going to check the following volumes disk space: " + volumes);
                    NameNodeRpcServer.java  (13 usages)
                        (358: 5) LOG.info("Error report from " + registration + ": " + msg);
                        (516: 7) LOG.debug("getAdditionalDatanode: src=" + src
                        (945: 7) LOG.info("Error report from " + dnName + ": " + msg);
                        (951: 7) LOG.warn("Disk error on " + dnName + ": " + msg);
                        (953: 7) LOG.warn("Fatal disk error on " + dnName + ": " + msg);
                        (956: 7) LOG.info("Error report from " + dnName + ": " + msg);
                        (975: 7) LOG.warn("Invalid registrationID - expected: "
                        (996: 5) LOG.info("Refreshing all user-to-groups mappings. Requested by user: " +
                        (1003: 5) LOG.info("Refreshing SuperUser proxy group mapping list ");
                        (1011: 7) LOG.debug("Getting groups for user " + user);
                        (1059: 7) LOG.warn(ive.getMessage() + " DN: " + dnReg);
                        (1073: 9) LOG.warn(ive);
                        (1076: 9) LOG.info(messagePrefix +
                    NNStorage.java  (19 usages)
                        (203: 5) LOG.warn("set restore failed storage to " + val);
                        (225: 7) LOG.info("NNStorage.attemptRestoreRemovedStorage: check removed(failed) "+
                        (231: 9) LOG.info("currently disabled dir " + root.getAbsolutePath() +
                        (235: 11) LOG.info("restoring dir " + sd.getRoot().getAbsolutePath());
                        (331: 7) LOG.warn("Error converting file to URI", ioe);
                        (476: 9) LOG.warn("writeTransactionIdToStorage failed on " + sd,
                        (525: 5) LOG.info("Storage directory " + sd.getRoot()
                        (762: 5) LOG.error("Error reported on storage directory " + sd);
                        (765: 5) LOG.debug("current list of storage dirs:" + lsd);
                        (767: 5) LOG.warn("About to remove corresponding storage: "
                        (772: 7) LOG.warn("Unable to unlock bad storage directory: "
                        (781: 5) LOG.debug("at the end current list of storage dirs:" + lsd);
                        (810: 11) LOG.warn("Clusterid mismatch - current clusterid: " + getClusterID()
                        (814: 7) LOG.info("Using clusterid: " + getClusterID());
                        (879: 9) LOG.info("current cluster id for sd="+sd.getCurrentDir() +
                        (885: 9) LOG.warn("this sd not available: " + e.getLocalizedMessage());
                        (888: 5) LOG.warn("couldn't find any VERSION file containing valid ClusterId");
                        (902: 7) LOG.warn("Could not find ip address of \"default\" inteface.");
                        (969: 17) FSImage.LOG.warn("Storage directory " + sd + " contains no VERSION file. Skipping...");
                    NNStorageRetentionManager.java  (4 usages)
                        (171: 5) LOG.info("Going to retain " + toRetain + " images with txid >= " +
                        (187: 7) LOG.info("Purging old edit log " + log);
                        (193: 7) LOG.info("Purging old image " + image);
                        (202: 9) LOG.warn("Could not delete " + file);
                    RedundantEditLogInputStream.java  (3 usages)
                        (171: 13) LOG.info("Fast-forwarding stream '" + streams[curIdx].getName() +
                        (216: 9) LOG.error("Got error reading edit log input stream " +
                        (232: 11) LOG.error("failing over to edit log " +
                    RenewDelegationTokenServlet.java  (2 usages)
                        (56: 7) LOG.info("Request for token received with no authentication from "
                        (88: 7) LOG.info("Exception while renewing token. Re-throwing. s=" + strException, e);
                    SecondaryNameNode.java  (20 usages)
                        (272: 5) LOG.info("Web server init done");
                        (278: 5) LOG.info("Secondary Web-server up at: " + infoBindAddress + ":" + infoPort);
                        (279: 5) LOG.info("Checkpoint Period   :" + checkpointConf.getPeriod() + " secs " +
                        (281: 5) LOG.info("Log Size Trigger    :" + checkpointConf.getTxnCount() + " txns");
                        (306: 9) LOG.info("Interrupted waiting to join on checkpointer thread");
                        (313: 7) LOG.warn("Exception shutting down SecondaryNameNode", e);
                        (318: 7) LOG.warn("Exception while closing CheckpointStorage", e);
                        (365: 9) LOG.error("Exception in doCheckpoint", e);
                        (368: 9) LOG.fatal("Throwable Exception in doCheckpoint", e);
                        (414: 15) LOG.info("Image has not changed. Will not download image.");
                        (416: 15) LOG.info("Image has changed. Downloading updated image from NN.");
                        (454: 5) LOG.debug("Will connect to NameNode at HTTP address: " + address);
                        (516: 5) LOG.warn("Checkpoint done. New Image Size: "
                        (567: 9) LOG.error(cmd + ": " + content[0]);
                        (569: 9) LOG.error(cmd + ": " + ex.getLocalizedMessage());
                        (576: 7) LOG.error(cmd + ": " + e.getLocalizedMessage());
                        (603: 7) LOG.fatal("Failed to parse options");
                        (616: 7) LOG.fatal("Failed to start secondary namenode", ioe);
                        (748: 7) LOG.error(pe.getMessage());
                        (853: 11) LOG.info("Formatting storage directory " + sd);
                    StreamFile.java  (1 usage)
                        (117: 9) LOG.debug("response.isCommitted()=" + response.isCommitted(), ioe);
                    TransferFsImage.java  (11 usages)
                        (87: 5) LOG.info("Downloaded file " + dstFiles.get(0).getName() + " size " +
                        (106: 9) LOG.info("Skipping download of remote edit log " +
                        (110: 9) LOG.debug("Dest file: " + f);
                        (115: 5) LOG.info("Downloaded file " + dstFiles.get(0).getName() + " size " +
                        (142: 9) LOG.info("Image upload with txid " + txid +
                        (150: 5) LOG.info("Uploaded image with txid " + txid + " to namenode at " +
                        (188: 11) LOG.warn("SIMULATING A CORRUPT BYTE IN IMAGE TRANSFER!");
                        (217: 5) LOG.info("Opening connection to " + str);
                        (297: 15) LOG.warn("Overwriting existing file " + f
                        (302: 13) LOG.warn("Unable to download file " + f, ioe);
                        (348: 5) LOG.info(String.format("Transfer took %.2fs at %.2f KB/s",
                org.apache.hadoop.hdfs.server.namenode.ha  (27 usages)
                    BootstrapStandby.java  (5 usages)
                        (145: 7) LOG.fatal("Unable to fetch namespace information from active NN at " +
                        (148: 9) LOG.debug("Full exception trace", ioe);
                        (154: 7) LOG.fatal("Layout version on remote node (" +
                        (243: 9) LOG.fatal(msg, e);
                        (245: 9) LOG.fatal(msg);
                    ConfiguredFailoverProxyProvider.java  (1 usage)
                        (128: 9) LOG.error("Failed to create RPC proxy to NameNode", e);
                    EditLogTailer.java  (13 usages)
                        (117: 7) LOG.info("Will roll logs on active node at " + activeAddr + " every " +
                        (120: 7) LOG.info("Not going to trigger log rolls on active node because " +
                        (127: 5) LOG.debug("logRollPeriodMs=" + logRollPeriodMs +
                        (157: 7) LOG.warn("Edit log tailer thread exited with an exception");
                        (205: 9) LOG.debug("lastTxnId: " + lastTxnId);
                        (214: 9) LOG.warn("Edits tailer failed to find any streams. Will try again " +
                        (219: 9) LOG.debug("edit streams to load from: " + streams.size());
                        (233: 11) LOG.info(String.format("Loaded %d edits starting from txid %d ",
                        (266: 5) LOG.info("Triggering log roll on remote NameNode " + activeAddr);
                        (271: 7) LOG.warn("Unable to trigger a roll of the active NN", ioe);
                        (323: 11) LOG.warn("Error while reading edits from disk. Will try again.", elie);
                        (328: 11) LOG.fatal("Unknown error encountered while tailing edits. " +
                        (336: 11) LOG.warn("Edit log tailer interrupted", e);
                    StandbyCheckpointer.java  (8 usages)
                        (125: 5) LOG.info("Starting standby checkpoint thread...\n" +
                        (138: 7) LOG.warn("Edit log tailer thread exited with an exception");
                        (159: 9) LOG.info("A checkpoint was triggered but the Standby Node has not " +
                        (274: 13) LOG.info("Triggering checkpoint because there have been " +
                        (280: 13) LOG.info("Triggering checkpoint because it has been " +
                        (288: 15) LOG.info("But skipping this checkpoint since we are about to failover!");
                        (301: 11) LOG.info("Checkpoint was cancelled: " + ce.getMessage());
                        (307: 11) LOG.error("Exception in doCheckpoint", t);
                org.apache.hadoop.hdfs.server.namenode.web.resources  (2 usages)
                    NamenodeWebHdfsMethods.java  (2 usages)
                        (151: 7) LOG.trace("HTTP " + op.getValue().getType() + ": " + op + ", " + path
                        (250: 7) LOG.trace("redirectURI=" + uri);
                org.apache.hadoop.hdfs.tools  (18 usages)
                    DelegationTokenFetcher.java  (12 usages)
                        (167: 21) LOG.debug("Cancelled token for " + token.getService());
                        (176: 21) LOG.debug("Renewed token for " + token.getService()
                        (188: 21) LOG.debug("Fetched token via " + webUrl + " for "
                        (199: 21) LOG.debug("Fetched token for " + token.getService()
                        (226: 9) LOG.debug("Retrieving token from: " + url);
                        (280: 7) LOG.info("error in renew over HTTP", ie);
                        (285: 9) LOG.info("rethrowing exception from HTTP request: " +
                        (312: 5) LOG.info("Error response from HTTP request=" + resp +
                        (331: 7) LOG.warn("failed to create object of this class", ee);
                        (337: 5) LOG.info("Exception from HTTP response=" + e.getLocalizedMessage());
                        (368: 7) LOG.info("error in cancel over HTTP", ie);
                        (373: 9) LOG.info("rethrowing exception from HTTP request: " +
                    DFSAdmin.java  (1 usage)
                        (1158: 7) LOG.debug("Exception encountered:", debugException);
                    DFSHAAdmin.java  (1 usage)
                        (71: 7) LOG.debug("Using NN principal: " + nameNodePrincipal);
                    DFSZKFailoverController.java  (3 usages)
                        (140: 5) LOG.info("Failover controller configured for NameNode " +
                        (184: 7) LOG.info("Allowed RPC access from " + ugi + " at " + Server.getRemoteAddress());
                        (189: 5) LOG.warn(msg);
                    GetGroups.java  (1 usage)
                        (76: 7) LOG.debug("Using NN principal: " + nameNodePrincipal);
                org.apache.hadoop.hdfs.tools.offlineEditsViewer  (4 usages)
                    OfflineEditsBinaryLoader.java  (4 usages)
                        (82: 11) LOG.error("Got IOException at position " + inputStream.getPosition());
                        (86: 9) LOG.error("Got IOException while reading stream!  Resyncing.", e);
                        (91: 11) LOG.error("Got RuntimeException at position " + inputStream.getPosition());
                        (95: 9) LOG.error("Got RuntimeException while reading stream!  Resyncing.", e);
                org.apache.hadoop.hdfs.tools.offlineImageViewer  (1 usage)
                    OfflineImageViewer.java  (1 usage)
                        (140: 9) LOG.error("image loading failed at offset " + tracker.getPos());
                org.apache.hadoop.hdfs.util  (6 usages)
                    AtomicFileOutputStream.java  (3 usages)
                        (88: 11) LOG.warn("Unable to delete tmp file " + tmpFile);
                        (103: 7) LOG.warn("Unable to abort file " + tmpFile, ioe);
                        (106: 7) LOG.warn("Unable to delete tmp file during abort " + tmpFile);
                    LightWeightGSet.java  (1 usage)
                        (83: 7) LOG.debug("recommended=" + recommended_length + ", actual=" + actual);
                    LightWeightHashSet.java  (1 usage)
                        (129: 5) LOG.debug("initial capacity=" + initialCapacity + ", max load factor= "
                    MD5FileUtils.java  (1 usage)
                        (146: 5) LOG.debug("Saved MD5 " + digest + " to " + md5File);
                org.apache.hadoop.hdfs.web  (6 usages)
                    WebHdfsFileSystem.java  (6 usages)
                        (220: 9) LOG.debug("Created new DT for " + token.getService());
                        (222: 9) LOG.debug("Found existing DT for " + token.getService());
                        (363: 7) LOG.trace("url=" + url);
                        (398: 7) LOG.trace("url=" + url);
                        (509: 13) LOG.info("Retrying connect to namenode: " + nnAddr
                        (516: 11) LOG.warn("Original exception is ", ioe);
                org.apache.hadoop.hdfs.web.resources  (2 usages)
                    ExceptionHandler.java  (2 usages)
                        (57: 7) LOG.trace("GOT EXCEPTION", e);
                        (92: 7) LOG.warn("INTERNAL_SERVER_ERROR", e);
            hadoop-hdfs-bkjournal  (24 usages)
                org.apache.hadoop.contrib.bkjournal  (24 usages)
                    BookKeeperEditLogOutputStream.java  (1 usage)
                        (177: 9) LOG.warn("Tried to set transmit result to (" + rc + ") \""
                    BookKeeperJournalManager.java  (14 usages)
                        (225: 11) LOG.info("Successfully created bookie available path : "
                        (230: 11) LOG.error("Error : "
                        (268: 15) LOG.warn("Ledger " + l.getLedgerId() + " does not exist;"
                        (291: 7) LOG.error("Error accessing zookeeper to format", ke);
                        (340: 11) LOG.error(err);
                        (424: 7) LOG.error("Error closing ledger", bke);
                        (427: 7) LOG.warn("Interrupted while closing ledger", ie);
                        (463: 13) LOG.error("Error closing current ledger", bke);
                        (601: 15) LOG.error("Unrecoverable corruption has occurred in segment "
                        (609: 13) LOG.warn("Inprogress znode " + child
                        (651: 11) LOG.error("Interrupted while purging " + l, ie);
                        (653: 11) LOG.error("Couldn't delete ledger from bookkeeper", bke);
                        (655: 11) LOG.error("Error deleting ledger entry in zookeeper", ke);
                        (765: 11) LOG.warn("ZNode: " + legderMetadataPath
                    CurrentInprogress.java  (4 usages)
                        (72: 13) LOG.debug(currentInprogressNode + " already created by other process.",
                        (108: 7) LOG.debug("Updated data[" + content + "] to CurrentInprogress");
                        (141: 7) LOG.debug("No data available in CurrentInprogress");
                        (157: 5) LOG.debug("Cleared the data from CurrentInprogress");
                    EditLogLedgerMetadata.java  (4 usages)
                        (122: 9) LOG.debug("Reading " + path + " data: " + new String(data, UTF_8));
                        (177: 9) LOG.trace("Verifying " + this.toString()
                        (182: 7) LOG.error("Couldn't verify data in " + path, e);
                        (185: 7) LOG.error("Couldn't verify data in " + path, ie);
                    MaxTxId.java  (1 usage)
                        (55: 9) LOG.trace("Setting maxTxId to " + maxTxId);
            hadoop-hdfs-httpfs  (41 usages)
                org.apache.hadoop.fs.http.server  (20 usages)
                    HttpFSExceptionProvider.java  (2 usages)
                        (95: 11) AUDIT_LOG.warn("FAILED [{}:{}] response [{}] {}", new Object[]{method, path, status, message});
                        (96: 5) LOG.warn("[{}:{}] response [{}] {}", new Object[]{method, path, status, message}, throwable);
                    HttpFSServer.java  (17 usages)
                        (123: 13) AUDIT_LOG.info("Proxy user [{}] DoAs user [{}]", proxyUserName, doAs);
                        (250: 15) AUDIT_LOG.info("[{}] offset [{}] len [{}]",
                        (261: 15) AUDIT_LOG.info("[{}]", path);
                        (270: 15) AUDIT_LOG.info("[{}] filter [{}]", path,
                        (279: 15) AUDIT_LOG.info("");
                        (301: 15) AUDIT_LOG.info("[{}]", path);
                        (309: 15) AUDIT_LOG.info("[{}]", path);
                        (359: 15) AUDIT_LOG.info("[{}] recursive [{}]", path, recursive);
                        (419: 17) AUDIT_LOG.info("[{}]", path);
                        (431: 15) AUDIT_LOG.info("[{}]", path);
                        (514: 17) AUDIT_LOG.info(
                        (527: 15) AUDIT_LOG.info("[{}] permission [{}]", path, permission);
                        (536: 15) AUDIT_LOG.info("[{}] to [{}]", path, toPath);
                        (546: 15) AUDIT_LOG.info("[{}] to (O/G)[{}]", path, owner + ":" + group);
                        (556: 15) AUDIT_LOG.info("[{}] to [{}]", path, permission);
                        (566: 15) AUDIT_LOG.info("[{}] to [{}]", path, replication);
                        (578: 15) AUDIT_LOG.info("[{}] to (M/A)[{}]", path,
                    HttpFSServerWebApp.java  (1 usage)
                        (102: 5) LOG.info("Connects to Namenode [{}]",
                org.apache.hadoop.lib.service.hadoop  (7 usages)
                    FileSystemAccessService.java  (7 usages)
                        (145: 5) LOG.info("Using FileSystemAccess JARs version [{}]", VersionInfo.getVersion());
                        (167: 7) LOG.info("Using FileSystemAccess Kerberos authentication, principal [{}] keytab [{}]", principal, keytab);
                        (172: 7) LOG.info("Using FileSystemAccess simple/pseudo authentication, principal [{}]", System.getProperty("user.name"));
                        (191: 5) LOG.debug("FileSystemAccess FileSystem configuration:");
                        (193: 7) LOG.debug("  {} = {}", entry.getKey(), entry.getValue());
                        (246: 11) LOG.warn("Error while purging filesystem, " + ex.toString(), ex);
                        (249: 7) LOG.debug("Purged [{}} filesystem instances", count);
                org.apache.hadoop.lib.service.scheduler  (9 usages)
                    SchedulerService.java  (9 usages)
                        (59: 5) LOG.debug("Scheduler started");
                        (68: 9) LOG.debug("Waiting for scheduler to shutdown");
                        (70: 11) LOG.warn("Gave up waiting for scheduler to shutdown");
                        (75: 9) LOG.debug("Scheduler shutdown");
                        (78: 7) LOG.warn(ex.getMessage(), ex);
                        (96: 7) LOG.debug("Scheduling callable [{}], interval [{}] seconds, delay [{}] in [{}]",
                        (104: 13) LOG.debug("Skipping [{}], server status [{}]", callable, getServer().getStatus());
                        (107: 13) LOG.debug("Executing [{}]", callable);
                        (114: 15) LOG.error("Error executing [{}], {}", new Object[]{callable, ex.getMessage(), ex});
                org.apache.hadoop.lib.service.security  (4 usages)
                    ProxyUserService.java  (4 usages)
                        (94: 9) LOG.info("Loading proxyuser settings [{}]=[{}]", key, value);
                        (107: 9) LOG.info("Loading proxyuser settings [{}]=[{}]", key, value);
                        (118: 13) LOG.info("  Hostname, original [{}], normalized [{}]", originalName, hosts[i]);
                        (133: 5) LOG.debug("Authorization check proxyuser [{}] host [{}] doAs [{}]",
                org.apache.hadoop.lib.wsrs  (1 usage)
                    ExceptionProvider.java  (1 usage)
                        (61: 5) LOG.debug("{}", throwable.getMessage(), throwable);            
            hadoop-mapreduce-client-app  (305 usages)
                org.apache.hadoop.mapred  (39 usages)
                    LocalContainerLauncher.java  (14 usages)
                        (89: 7) LOG.error("Local filesystem " + curDir.toURI().toString()
                        (177: 11) LOG.error("Returning, interrupted : " + e);
                        (181: 9) LOG.info("Processing the event " + event.toString());
                        (241: 13) LOG.fatal("oopsie...  this can never happen: "
                        (256: 11) LOG.warn("Ignoring unexpected event " + event.toString());
                        (285: 9) LOG.info(MRConfig.LOCAL_DIR + " for uber task: "
                        (300: 13) LOG.error("CONTAINER_REMOTE_LAUNCH contains a map task ("
                        (326: 13) LOG.error("CONTAINER_REMOTE_LAUNCH contains a reduce task ("
                        (344: 9) LOG.fatal("FSError from child", e);
                        (350: 9) LOG.warn("Exception running local (uberized) 'child' : "
                        (358: 11) LOG.info("Exception cleaning up: "
                        (367: 9) LOG.fatal("Error running local (uberized) 'child' : "
                        (399: 9) LOG.debug("Renaming map output file for task attempt "
                        (435: 13) LOG.warn("Unable to delete unexpected local file/dir "
                    TaskAttemptListenerImpl.java  (14 usages)
                        (173: 5) LOG.info("Commit go/no-go request from " + taskAttemptID.toString());
                        (208: 5) LOG.info("Commit-pending state update from " + taskAttemptID.toString());
                        (224: 5) LOG.info("Done acknowledgement from " + taskAttemptID.toString());
                        (239: 5) LOG.fatal("Task: " + taskAttemptID + " - exited : " + msg);
                        (252: 5) LOG.fatal("Task: " + taskAttemptID + " - failed due to FSError: "
                        (271: 5) LOG.info("MapCompletionEvents request from " + taskAttemptID.toString()
                        (289: 5) LOG.info("Ping from " + taskAttemptID.toString());
                        (297: 5) LOG.info("Diagnostics report from " + taskAttemptID.toString() + ": "
                        (317: 5) LOG.info("Status update from " + taskAttemptID.toString());
                        (326: 5) LOG.info("Progress of TaskAttempt " + taskAttemptID + " is : "
                        (405: 5) LOG.info("JVM with ID : " + jvmId + " asked for a task");
                        (419: 7) LOG.info("JVM with ID: " + jvmId + " is invalid and will be killed.");
                        (424: 9) LOG.info("JVM with ID: " + jvmId
                        (433: 9) LOG.info("JVM with ID: " + jvmId + " given task: " + task.getTaskID());
                    YarnChild.java  (11 usages)
                        (75: 5) LOG.debug("Child starting");
                        (97: 5) LOG.info("Executing with tokens:");
                        (99: 7) LOG.info(token);
                        (119: 5) LOG.debug("PID: " + System.getenv().get("JVM_PID"));
                        (129: 9) LOG.info("Sleeping for " + sleepTimeMilliSecs
                        (163: 7) LOG.fatal("FSError from child", e);
                        (166: 7) LOG.warn("Exception running child : "
                        (185: 9) LOG.info("Exception cleaning up: " + StringUtils.stringifyException(e));
                        (192: 7) LOG.fatal("Error running child : "
                        (220: 5) LOG.info(MRConfig.LOCAL_DIR + " for child: " + job.get(MRConfig.LOCAL_DIR));
                        (263: 5) LOG.debug("APPLICATION_ATTEMPT_ID: " + appAttemptIdEnv);
                org.apache.hadoop.mapreduce.jobhistory  (39 usages)
                    JobHistoryCopyService.java  (3 usages)
                        (89: 7) LOG.warn("error trying to open previous history file. No history data " +
                        (97: 7) LOG.info("Got an error parsing job-history file" +
                        (119: 5) LOG.info("History file is at " + historyFile);
                    JobHistoryEventHandler.java  (36 usages)
                        (133: 7) LOG.error("Failed while getting the configured log directories", e);
                        (145: 7) LOG.error("Failed while checking for/creating  history staging path: ["
                        (160: 9) LOG.info("Creating intermediate history logDir: ["
                        (180: 9) LOG.error(message);
                        (185: 7) LOG.error("Failed checking for the existance of history intermediate " +
                        (197: 7) LOG.error("Error creating user intermediate history done directory: [ "
                        (232: 9) LOG.info("Perms after creating " + fsStatus.getPermission().toShort()
                        (235: 11) LOG.info("Explicitly setting permissions to : " + fsp.toShort()
                        (240: 9) LOG.info("Directory: [" + path + "] already exists.");
                        (256: 13) LOG.info("Size of the JobHistory event queue is "
                        (265: 13) LOG.info("EventQueue take interrupted. Returning");
                        (290: 5) LOG.info("Stopping JobHistoryEventHandler. "
                        (303: 7) LOG.info("Interruped Exception while stopping", ie);
                        (312: 9) LOG.info("Exception while cancelling delayed flush timer. "
                        (321: 7) LOG.info("In stop, writing event " + ev.getType());
                        (333: 11) LOG.warn("Found jobId " + toClose
                        (353: 9) LOG.info("Exception while closing file " + e.getMessage());
                        (356: 5) LOG.info("Stopped JobHistoryEventHandler. super.stop()");
                        (377: 7) LOG.error("Log Directory is null, returning");
                        (402: 9) LOG.info("Event Writer setup for JobId: " + jobId + ", File: "
                        (405: 9) LOG.info("Could not create log file: [" + historyFile + "] + for job "
                        (422: 11) LOG.info("Failed to write the job configuration file", e);
                        (444: 7) LOG.error("Error closing writer for JobID: " + id);
                        (480: 11) LOG.error("Error JobHistoryEventHandler in handleEvent: " + event,
                        (498: 11) LOG.debug("In HistoryEventHandler "
                        (502: 9) LOG.error("Error writing History Event: " + event.getHistoryEvent(),
                        (644: 7) LOG.error("Error closing writer for JobID: " + jobId);
                        (649: 7) LOG.warn("No file for job-history with " + jobId + " found in cache!");
                        (652: 7) LOG.warn("No file for jobconf with " + jobId + " found in cache!");
                        (672: 7) LOG.info("Unable to write out JobSummaryInfo to ["
                        (712: 7) LOG.error("Error closing writer for JobID: " + jobId);
                        (878: 7) LOG.info("Moved tmp to done: " + tmpPath + " to " + path);
                        (887: 7) LOG.info("Copying " + fromPath.toString() + " to " + toPath.toString());
                        (896: 9) LOG.info("Copied to done location: " + toPath);
                        (898: 9) LOG.info("copy failed");
                        (919: 5) LOG.info("JobHistoryEventHandler notified that forceJobCompletion is "
                org.apache.hadoop.mapreduce.v2.app  (30 usages)
                    MRAppMaster.java  (29 usages)
                        (214: 5) LOG.info("Created MRAppMaster for application " + applicationAttemptId);
                        (229: 5) LOG.info("AM Retries: " + numAMRetries +
                        (251: 7) LOG.info("Using mapred newApiCommitter.");
                        (271: 9) LOG.fatal(shutDownMessage);
                        (344: 9) LOG.info("Recovery is enabled. "
                        (352: 9) LOG.info("Not starting RecoveryService: recoveryEnabled: "
                        (433: 5) LOG.info("OutputCommitter set in config "
                        (456: 5) LOG.info("OutputCommitter is " + committer.getClass().getName());
                        (487: 11) LOG.warn("Job Staging directory is null");
                        (491: 9) LOG.info("Deleting staging directory " + FileSystem.getDefaultUri(getConfig()) +
                        (496: 7) LOG.error("Failed to cleanup staging dir " + jobTempDir, io);
                        (516: 9) LOG.info("Job end notification started for jobID : "
                        (522: 9) LOG.warn("Job end notification interrupted for jobID : "
                        (540: 7) LOG.info("Calling stop for all the services");
                        (544: 7) LOG.warn("Graceful stop failed ", t);
                        (549: 5) LOG.info("Exiting MR AppMaster..GoodBye!");
                        (624: 7) LOG.info("jobSubmitDir=" + jobSubmitDir + " jobTokenFile="
                        (665: 7) LOG.error("Can't make a speculator -- check "
                        (669: 7) LOG.error("Can't make a speculator -- check "
                        (673: 7) LOG.error("Can't make a speculator -- check "
                        (677: 7) LOG.error("Can't make a speculator -- check "
                        (879: 11) LOG.info("Skipping cleaning up the staging dir. "
                        (883: 9) LOG.error("Failed to cleanup staging dir: ", io);
                        (1008: 9) LOG.info("MRAppMaster uberizing job " + job.getID()
                        (1016: 9) LOG.info("MRAppMaster launching normal, non-uberized, multi-container "
                        (1063: 7) LOG.warn("Could not parse the old history file. "
                        (1180: 7) LOG.error(msg);
                        (1230: 7) LOG.fatal("Error starting MRAppMaster", t);
                        (1243: 7) LOG.info("MRAppMaster received a signal. Signaling RMCommunicator and "
                    TaskHeartbeatHandler.java  (1 usage)
                        (157: 11) LOG.info("TaskHeartbeatHandler thread interrupted");
                org.apache.hadoop.mapreduce.v2.app.client  (7 usages)
                    MRClientService.java  (7 usages)
                        (147: 5) LOG.info("Instantiated MRClientService at " + this.bindAddress);
                        (152: 7) LOG.error("Webapps failed to start. Ignoring for now:", e);
                        (286: 7) LOG.info(message);
                        (303: 7) LOG.info(message);
                        (318: 7) LOG.info(message);
                        (348: 7) LOG.info(message);
                        (373: 7) LOG.info("Getting task report for " + taskType + "   " + jobId
                org.apache.hadoop.mapreduce.v2.app.commit  (8 usages)
                    CommitterEventHandler.java  (8 usages)
                        (126: 15) LOG.error("Returning, interrupted : " + e);
                        (180: 7) LOG.info("Canceling commit");
                        (206: 7) LOG.info("Processing the event " + event.toString());
                        (233: 9) LOG.warn("Job setup failed", e);
                        (257: 11) LOG.error("could not create failure file.", e2);
                        (259: 9) LOG.error("Could not commit job", e);
                        (275: 9) LOG.warn("Could not abort job", e);
                        (287: 9) LOG.warn("Task cleanup failed for attempt " + event.getAttemptID(), e);
                org.apache.hadoop.mapreduce.v2.app.job.impl  (50 usages)
                    JobImpl.java  (15 usages)
                        (857: 7) LOG.debug("Processing " + event.getJobId() + " of type "
                        (866: 9) LOG.error("Can't handle this event at current state", e);
                        (874: 9) LOG.info(jobId + "Job Transitioned from " + oldState + " to "
                        (939: 5) LOG.info("Calling handler for JobFinishedEvent ");
                        (1103: 7) LOG.info("Uberizing job " + jobId + ": " + numMapTasks + "m+"
                        (1136: 7) LOG.info(msg.toString());
                        (1178: 11) LOG.info(mesg + ". AttemptId:" + id);
                        (1313: 9) LOG.warn("Job init failed", e);
                        (1331: 9) LOG.debug("startJobs: parent=" + path + " child=" + oldJobIDString);
                        (1350: 7) LOG.info("Adding job token for " + oldJobIDString
                        (1378: 7) LOG.info("Input size for job " + job.jobId + " = " + inputLength
                        (1396: 7) LOG.info("Number of reduces for job " + job.jobId + " = "
                        (1703: 11) LOG.info("Too many fetch-failures for output of task attempt: " +
                        (1719: 7) LOG.info("Num completed Tasks: " + job.completedTaskCount);
                        (1744: 9) LOG.info(diagnosticMsg);
                    TaskAttemptImpl.java  (18 usages)
                        (659: 9) LOG.info("The job-jar file on the remote FS is "
                        (664: 9) LOG.info("Job jar is not present. "
                        (681: 7) LOG.info("The job-conf file on the remote FS is "
                        (689: 7) LOG.info("Adding #" + credentials.numberOfTokens()
                        (699: 7) LOG.info("Size of containertokens_dob is "
                        (707: 7) LOG.info("Putting shuffle token in serviceData");
                        (1011: 7) LOG.debug("Processing " + event.getTaskAttemptID() + " of type "
                        (1020: 9) LOG.error("Can't handle this event at current state for "
                        (1029: 11) LOG.info(attemptId + " TaskAttempt Transitioned from "
                        (1297: 7) LOG.warn("Failed to resolve address: " + src
                        (1408: 11) LOG.error("Task final state is not FAILED or KILLED: " + finalState);
                        (1424: 9) LOG.debug("Not generating HistoryFinish event since start event not " +
                        (1462: 7) LOG.info("TaskAttempt: [" + taskAttempt.attemptId
                        (1559: 9) LOG.debug("Not generating HistoryFinish event since start event not " +
                        (1629: 9) LOG.debug("Not generating HistoryFinish event since start event not " +
                        (1653: 9) LOG.info("Ignoring killed event for successful reduce task attempt" +
                        (1697: 9) LOG.debug("Not generating HistoryFinish event since start event not " +
                        (1772: 7) LOG.info("Diagnostics report from " + taskAttempt.attemptId + ": "
                    TaskImpl.java  (17 usages)
                        (315: 7) LOG.info("Task is from previous run " + taskId);
                        (572: 9) LOG.info("Result of canCommit for " + taskAttemptID + ":" + canCommit);
                        (602: 7) LOG.debug("Created attempt " + attempt.getID());
                        (645: 7) LOG.debug("Processing " + event.getTaskID() + " of type "
                        (654: 9) LOG.error("Can't handle this event at current state for "
                        (659: 9) LOG.info(taskId + " Task Transitioned from " + oldState + " to "
                        (669: 5) LOG.error("Invalid event " + type + " on Task " + this.taskId);
                        (776: 7) LOG.info("Scheduling a redundant attempt for task " + task.taskId);
                        (791: 9) LOG.info(attemptID + " given a go for committing the task output.");
                        (795: 9) LOG.info(task.commitAttempt
                        (818: 7) LOG.info("Task succeeded with attempt " + task.successfulAttempt);
                        (832: 11) LOG.info("Issuing kill to other attempt " + attempt.getID());
                        (889: 11) LOG.debug("Not generating HistoryFinish event since start event not" +
                        (962: 11) LOG.debug("Not generating HistoryFinish event since start event not" +
                        (995: 9) LOG.error("Unexpected event for REDUCE task " + event.getType());
                        (1040: 9) LOG.error("Unexpected event for REDUCE task " + event.getType());
                        (1084: 9) LOG.debug("Not generating HistoryFinish event since start event not" +
                org.apache.hadoop.mapreduce.v2.app.launcher  (9 usages)
                    ContainerLauncherImpl.java  (9 usages)
                        (135: 7) LOG.info("Launching " + taskAttemptID);
                        (165: 9) LOG.info("Shuffle port returned by ContainerManager for "
                        (197: 9) LOG.info("KILLING " + taskAttemptID);
                        (218: 11) LOG.warn(message);
                        (251: 5) LOG.info("Upper limit on the thread pool size is " + this.limitOnPoolSize);
                        (279: 15) LOG.error("Returning, interrupted : " + e);
                        (300: 15) LOG.info("Setting ContainerLauncher pool size to " + newPoolSize
                        (384: 7) LOG.info("Processing the event " + event.toString());
                        (410: 5) LOG.error(message);
                org.apache.hadoop.mapreduce.v2.app.local  (3 usages)
                    LocalContainerAllocator.java  (3 usages)
                        (113: 9) LOG.error("Could not contact RM after " + retryInterval + " milliseconds.");
                        (124: 7) LOG.info("Event from RM: shutting down Application Master");
                        (138: 7) LOG.info("Processing the event " + event.toString());
                org.apache.hadoop.mapreduce.v2.app.recover  (24 usages)
                    RecoveryService.java  (24 usages)
                        (135: 7) LOG.warn(e);
                        (136: 7) LOG.warn("Could not parse the old history file. Aborting recovery. "
                        (141: 7) LOG.info("SETTING THE RECOVERY MODE TO TRUE. NO OF COMPLETED TASKS "
                        (143: 7) LOG.info("Job launch time " + jobInfo.getLaunchTime());
                        (189: 7) LOG.info("Got an error parsing job-history file" +
                        (198: 9) LOG.info("Read from history task "
                        (202: 5) LOG.info("Read completed tasks from history "
                        (223: 5) LOG.info("History file is at " + historyFile);
                        (250: 11) LOG.info("Recovered Attempt start time " + attInfo.getStartTime());
                        (258: 11) LOG.info("Recovered Attempt finish time " + attInfo.getFinishTime());
                        (266: 11) LOG.info("Recovered Task attempt " + tEvent.getTaskAttemptID());
                        (276: 13) LOG.info("CompletedTasks() " + completedTasks.size());
                        (280: 15) LOG.info("Setting the recovery mode to false. " +
                        (330: 11) LOG.debug("Adding to pending task events "
                        (340: 9) LOG.debug("CONTAINER_REQ " + aId);
                        (347: 9) LOG.debug("TASK_CLEAN");
                        (382: 15) LOG.info("Recovered output from task attempt " + attInfo.getAttemptId());
                        (384: 15) LOG.info("Will not try to recover output for "
                        (388: 13) LOG.error("Caught an exception while trying to recover task "+aId, e);
                        (397: 11) LOG.info("Sending done event to recovered attempt " + aId);
                        (402: 11) LOG.info("Sending kill event to recovered attempt " + aId);
                        (407: 11) LOG.info("Sending fail event to recovered attempt " + aId);
                        (431: 7) LOG.info("Sending status update event to " + yarnAttemptID);
                        (450: 7) LOG.info("Sending assigned event to " + yarnAttemptID);
                org.apache.hadoop.mapreduce.v2.app.rm  (83 usages)
                    RMCommunicator.java  (13 usages)
                        (166: 7) LOG.info("minContainerCapability: " + minContainerCapability.getMemory());
                        (167: 7) LOG.info("maxContainerCapability: " + maxContainerCapability.getMemory());
                        (169: 7) LOG.error("Exception while registering", are);
                        (191: 7) LOG.info("Setting job diagnostics to " + sb.toString());
                        (195: 7) LOG.info("History url is " + historyUrl);
                        (205: 7) LOG.error("Exception while unregistering ", are);
                        (227: 7) LOG.warn("InterruptedException while stopping", ie);
                        (245: 15) LOG.error("Error communicating with RM: " + e.getMessage() , e);
                        (248: 15) LOG.error("ERROR IN CONTACTING RM. ", e);
                        (257: 15) LOG.warn("Allocated thread interrupted. Returning.");
                        (296: 9) LOG.debug("AppMasterToken is " + token);
                        (331: 5) LOG.info("RMCommunicator notified that shouldUnregistered is: "
                        (337: 5) LOG.info("RMCommunicator notified that iSignalled is: "
                    RMContainerAllocator.java  (55 usages)
                        (189: 15) LOG.error("Returning, interrupted : " + e);
                        (197: 13) LOG.error("Error in handling event type " + event.getType()
                        (264: 7) LOG.info("Size of event-queue in RMContainerAllocator is " + qSize);
                        (268: 7) LOG.warn("Very low remaining capacity in the event-queue "
                        (295: 11) LOG.info("mapResourceReqt:"+mapResourceReqt);
                        (300: 13) LOG.info(diagMsg);
                        (320: 11) LOG.info("reduceResourceReqt:"+reduceResourceReqt);
                        (326: 13) LOG.info(diagMsg);
                        (346: 7) LOG.info("Processing the event " + event.toString());
                        (361: 9) LOG.error("Could not deallocate container for task attemptId " +
                        (396: 9) LOG.info("Ramping down all scheduled reduces:" + scheduledRequests.reduces.size());
                        (412: 9) LOG.info("Going to preempt " + toPreempt);
                        (433: 5) LOG.info("Recalculating schedule, headroom=" + headRoom);
                        (440: 9) LOG.info("Reduce slow start threshold not met. " +
                        (445: 9) LOG.info("Reduce slow start threshold reached. Scheduling reduces.");
                        (453: 7) LOG.info("All maps assigned. " +
                        (494: 5) LOG.info("completedMapPercent " + completedMapPercent +
                        (506: 7) LOG.info("Ramping up " + rampUp);
                        (511: 7) LOG.info("Ramping down " + rampDown);
                        (559: 9) LOG.error("Could not contact RM after " + retryInterval + " milliseconds.");
                        (584: 9) LOG.debug("headroom=" + newHeadRoom);
                        (590: 9) LOG.debug("Received new Container :" + cont);
                        (600: 7) LOG.info("Received completed container " + cont.getContainerId());
                        (603: 9) LOG.error("Container complete event for unknown container id "
                        (646: 13) LOG.info("Killing taskAttempt:" + tid
                        (714: 9) LOG.info("Added "+event.getAttemptID()+" to list of failed maps");
                        (724: 13) LOG.debug("Added attempt req to host " + host);
                        (735: 13) LOG.debug("Added attempt req to rack " + rack);
                        (753: 7) LOG.info("Got allocated containers " + allocatedContainers.size());
                        (758: 11) LOG.debug("Assigning container " + allocated.getId()
                        (773: 13) LOG.info("Cannot assign container " + allocated
                        (784: 13) LOG.info("Cannot assign container " + allocated
                        (792: 11) LOG.warn("Container allocated at unwanted priority: " + priority +
                        (810: 11) LOG.info("Got allocated container on a blacklisted "
                        (819: 13) LOG.info("Placing a new container request for task attempt "
                        (834: 13) LOG.info("Could not map allocated container to a valid request."
                        (851: 9) LOG.info("Releasing unassigned and invalid container "
                        (870: 9) LOG.info("Assigned container (" + allocated + ") "
                        (886: 9) LOG.info("Assigning container " + allocated + " to fast fail map");
                        (890: 11) LOG.debug("Assigning container " + allocated + " to reduce");
                        (913: 7) LOG.info("Finding containerReq for allocated container: " + allocated);
                        (917: 9) LOG.info("Replacing FAST_FAIL_MAP container " + allocated.getId());
                        (922: 9) LOG.info("Found replacement: " + toBeReplaced);
                        (926: 9) LOG.info("Replacing MAP container " + allocated.getId());
                        (945: 7) LOG.info("Found replacement: " + toBeReplaced);
                        (962: 11) LOG.info("Assigned from earlierFailedMaps");
                        (975: 9) LOG.info("Assigned to reduce");
                        (994: 13) LOG.debug("Host matched to the request list " + host);
                        (1007: 15) LOG.debug("Assigned based on host match " + host);
                        (1037: 15) LOG.debug("Assigned based on rack match " + rack);
                        (1059: 11) LOG.debug("Assigned based on * match");
                        (1076: 7) LOG.info("Assigned container " + container.getId().toString() + " to " + tId);
                        (1102: 9) LOG.info("Preempting " + id);
                        (1117: 13) LOG.info("Reduce preemption successful " + tId);
                        (1199: 9) LOG.info(msgPrefix + "PendingReds:" + numPendingReduces +
                    RMContainerRequestor.java  (15 usages)
                        (130: 5) LOG.info("nodeBlacklistingEnabled:" + nodeBlacklistingEnabled);
                        (137: 5) LOG.info("maxTaskFailuresPerNode is " + maxTaskFailuresPerNode);
                        (143: 5) LOG.info("blacklistDisablePercent is " + blacklistDisablePercent);
                        (159: 7) LOG.info("getResources() for " + applicationId + ":" + " ask="
                        (184: 9) LOG.info("KnownNode Count at 0. Not computing ignoreBlacklisting");
                        (190: 11) LOG.info("Ignore blacklisting set to true. Known: " + clusterNmCount
                        (195: 11) LOG.info("Ignore blacklisting set to false. Known: " + clusterNmCount
                        (208: 9) LOG.debug("Host " + hostName + " is already blacklisted.");
                        (215: 5) LOG.info(failures + " failures on node " + hostName);
                        (220: 7) LOG.info("Blacklisted host " + hostName);
                        (303: 9) LOG.debug("Added priority=" + priority);
                        (325: 7) LOG.debug("addResourceRequest:" + " applicationId="
                        (342: 9) LOG.debug("Not decrementing resource as " + resourceName
                        (350: 7) LOG.debug("BEFORE decResourceRequest:" + " applicationId="
                        (373: 7) LOG.info("AFTER decResourceRequest:" + " applicationId="
                org.apache.hadoop.mapreduce.v2.app.speculate  (10 usages)
                    DefaultSpeculator.java  (10 usages)
                        (130: 7) LOG.error("Can't make a speculation runtime extimator", ex);
                        (133: 7) LOG.error("Can't make a speculation runtime extimator", ex);
                        (136: 7) LOG.error("Can't make a speculation runtime extimator", ex);
                        (139: 7) LOG.error("Can't make a speculation runtime extimator", ex);
                        (185: 21) LOG.info("We launched " + speculations
                        (192: 19) LOG.error("Background thread returning, interrupted : " + e);
                        (229: 5) LOG.info("We got asked to run a debug speculation scan.");
                        (275: 9) LOG.info("ATTEMPT_START " + event.getTaskID());
                        (283: 9) LOG.info("JOB_CREATE " + event.getJobID());
                        (420: 5) LOG.info
                org.apache.hadoop.mapreduce.v2.app.webapp  (3 usages)
                    AppController.java  (2 usages)
                        (227: 9) LOG.error("Failed to render tasks page with task type : "
                        (292: 9) LOG.error("Failed to render attempts page with task type : "
                    ConfBlock.java  (1 usage)
                        (116: 7) LOG.error("Error while reading "+confPath, e);
            hadoop-mapreduce-client-common  (35 usages)
                org.apache.hadoop.mapred  (24 usages)
                    LocalDistributedCacheManager.java  (5 usages)
                        (166: 9) LOG.info(String.format("Localized %s as %s", resourcePath, path));
                        (202: 9) LOG.info(String.format("Creating symlink: %s <- %s", target, link));
                        (204: 11) LOG.warn(String.format("Failed to create symlink: %s <- %s", target,
                        (235: 7) LOG.info(urls[i]);
                        (248: 9) LOG.warn("Failed to delete symlink created by the local job runner: " +
                    LocalJobRunner.java  (19 usages)
                        (212: 11) LOG.info("Starting task: " + mapId);
                        (237: 11) LOG.info("Finishing task: " + mapId);
                        (302: 7) LOG.debug("Starting thread pool executor.");
                        (303: 7) LOG.debug("Max local threads: " + maxMapThreads);
                        (304: 7) LOG.debug("Map tasks to process: " + this.numMapTasks);
                        (319: 7) LOG.info("OutputCommitter set in config "
                        (337: 7) LOG.info("OutputCommitter is " + committer.getClass().getName());
                        (350: 9) LOG.info("Failed to createOutputCommitter", e);
                        (385: 11) LOG.info("Waiting for map tasks");
                        (393: 9) LOG.info("Map task executor complete.");
                        (470: 11) LOG.info("Error cleaning up job:" + id);
                        (478: 9) LOG.warn(id, t);
                        (489: 11) LOG.warn("Error cleaning up "+id+": "+e);
                        (500: 7) LOG.info(taskStatus.getStateString());
                        (555: 7) LOG.info("Task " + taskid + " reportedNextRecordRange " + range);
                        (578: 7) LOG.fatal("FSError: "+ message + "from task: " + taskId);
                        (582: 7) LOG.fatal("shuffleError: "+ message + "from task: " + taskId);
                        (587: 7) LOG.fatal("Fatal: "+ msg + "from task: " + taskId);
                        (844: 5) LOG.debug(MRConfig.LOCAL_DIR + " for child : " + childMapredLocalDir);
                org.apache.hadoop.mapreduce.v2.jobhistory  (5 usages)
                    FileNameIndexUtils.java  (5 usages)
                        (125: 9) LOG.warn("Unable to parse submit time from job history file "
                        (139: 9) LOG.warn("Unable to parse finish time from job history file "
                        (147: 9) LOG.warn("Unable to parse num maps from job history file "
                        (155: 9) LOG.warn("Unable to parse num reduces from job history file "
                        (165: 7) LOG.warn("Parsing job history file with partial data encoded into name: "
                org.apache.hadoop.mapreduce.v2.security  (1 usage)
                    MRDelegationTokenRenewer.java  (1 usage)
                        (109: 7) LOG.debug("Connecting to MRHistoryServer at: " + hsAddress);
                org.apache.hadoop.mapreduce.v2.security.client  (2 usages)
                    ClientHSTokenSelector.java  (2 usages)
                        (43: 5) LOG.debug("Looking for a token with service " + service.toString());
                        (46: 9) LOG.debug("Token kind is " + token.getKind().toString()
                org.apache.hadoop.mapreduce.v2.util  (3 usages)
                    MRApps.java  (3 usages)
                        (269: 9) LOG.warn("Not using job classloader since APP_CLASSPATH is not set.");
                        (271: 9) LOG.info("Using job classloader");
                        (273: 11) LOG.debug("APP_CLASSPATH=" + appClasspath);
            hadoop-mapreduce-client-core  (342 usages)
                org.apache.hadoop.mapred  (141 usages)
                    AuditLogger.java  (2 usages)
                        (84: 7) LOG.info(createSuccessLog(user, operation, target));
                        (123: 7) LOG.warn(createFailureLog(user, operation, perm, target, description));
                    BackupStore.java  (14 usages)
                        (109: 5) LOG.info("Created a new BackupStore with a memory of " + maxSize);
                        (163: 7) LOG.debug("Dropping a segment");
                        (172: 5) LOG.debug("Setting the FirsSegmentOffset to " + currentKVOffset);
                        (212: 5) LOG.debug("Reset - First segment offset is " + firstSegmentOffset +
                        (404: 7) LOG.debug("Created a new mem block of " + allocatedSize);
                        (473: 7) LOG.debug("ID: " + segmentList.size() + " WRITE TO MEM");
                        (506: 7) LOG.debug("Added Memory Segment to List. List Size is " +
                        (537: 7) LOG.debug("ID: " + segmentList.size() + " WRITE TO DISK");
                        (557: 7) LOG.debug("Disk Segment added to List. Size is "  + segmentList.size());
                        (567: 7) LOG.info("Created file: " + tmp);
                        (586: 7) LOG.warn("Reserve(int, InputStream) not supported by BackupRamManager");
                        (596: 7) LOG.debug("Reserving: " + reservedSize + " Requested: " + requestedSize);
                        (602: 9) LOG.debug("No Space available. Available: " + availableSize +
                        (612: 7) LOG.debug("Unreserving: " + requestedSize +
                    CleanupQueue.java  (6 usages)
                        (90: 7) LOG.debug("Trying to delete " + context.fullPath);
                        (125: 9) LOG.debug(getName() + " started.");
                        (133: 13) LOG.warn("CleanupThread:Unable to delete path " + context.fullPath);
                        (136: 13) LOG.debug("DELETED " + context.fullPath);
                        (139: 11) LOG.warn("Interrupted deletion of " + context.fullPath);
                        (142: 11) LOG.warn("Error deleting path " + context.fullPath + ": " + e);
                    Counters.java  (1 usage)
                        (441: 7) LOG.warn("Counter name MAP_INPUT_BYTES is deprecated. " +
                    DeprecatedQueueConfigurationParser.java  (3 usages)
                        (70: 9) LOG.warn("Not able to initialize queue " + name);
                        (97: 7) LOG.warn(
                        (115: 13) LOG.warn(
                    FileInputFormat.java  (2 usages)
                        (233: 5) LOG.info("Total input paths to process : " + result.size());
                        (302: 5) LOG.debug("Total # of splits: " + splits.size());
                    IFile.java  (2 usages)
                        (122: 11) LOG.warn("Could not obtain compressor from CodecPool");
                        (349: 11) LOG.warn("Could not obtain decompressor from CodecPool");
                    IFileInputStream.java  (1 usage)
                        (98: 7) LOG.info("Unable to determine FileDescriptor", e);
                    IndexCache.java  (6 usages)
                        (47: 5) LOG.info("IndexCache created with max memory = " + totalMemoryAllowed);
                        (79: 7) LOG.debug("IndexCache HIT: MapId " + mapId + " found");
                        (113: 7) LOG.debug("IndexCache HIT: MapId " + mapId + " found");
                        (116: 5) LOG.debug("IndexCache MISS: MapId " + mapId + " not found") ;
                        (155: 9) LOG.warn("Map ID" + mapId + " not found in queue!!");
                        (158: 7) LOG.info("Map ID " + mapId + " not found in cache");
                    JobACLsManager.java  (1 usage)
                        (110: 7) LOG.debug("checkAccess job acls, jobOwner: " + jobOwner + " jobacl: "
                    JobConf.java  (8 usages)
                        (1876: 5) LOG.warn(
                        (1912: 5) LOG.warn("setMaxVirtualMemoryForTask() is deprecated."+
                        (1932: 5) LOG.warn("The API getMaxPhysicalMemoryForTask() is deprecated."
                        (1943: 5) LOG.warn("The API setMaxPhysicalMemoryForTask() is deprecated."
                        (1954: 7) LOG.warn(JobConf.deprecatedString(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY)
                        (1959: 7) LOG.warn(JobConf.deprecatedString(JobConf.MAPRED_TASK_ULIMIT));
                        (1962: 7) LOG.warn(JobConf.deprecatedString(JobConf.MAPRED_MAP_TASK_ULIMIT));
                        (1965: 7) LOG.warn(JobConf.deprecatedString(JobConf.MAPRED_REDUCE_TASK_ULIMIT));
                    JobEndNotifier.java  (8 usages)
                        (59: 33) LOG.error("Thread has ended unexpectedly", irex);
                        (72: 31) LOG.error("Notification failure [" + notification + "]", ioex);
                        (78: 35) LOG.error("Notification queuing error [" + notification + "]",
                        (84: 31) LOG.error("Notification failure [" + notification + "]", ex);
                        (128: 9) LOG.error("Notification queuing failure [" + notification + "]", iex);
                        (157: 11) LOG.error("Notification error [" + notification.getUri() + "]", ioex);
                        (160: 11) LOG.error("Notification error [" + notification.getUri() + "]", ex);
                        (166: 11) LOG.error("Notification retry error [" + notification + "]", iex);
                    LineRecordReader.java  (1 usage)
                        (217: 7) LOG.info("Skipped line of size " + newSize + " at pos " + (pos - newSize));
                    MapTask.java  (21 usages)
                        (255: 9) LOG.warn("Further records got skipped.");
                        (387: 5) LOG.info("Map output collector class = " + collector.getClass().getName());
                        (415: 5) LOG.info("numReduceTasks: " + numReduceTasks);
                        (456: 5) LOG.info("Processing split: " + inputSplit);
                        (727: 5) LOG.info("Processing split: " + split);
                        (963: 9) LOG.info(JobContext.IO_SORT_MB + ": " + sortmb);
                        (964: 9) LOG.info("soft limit at " + softLimit);
                        (965: 9) LOG.info("bufstart = " + bufstart + "; bufvoid = " + bufvoid);
                        (966: 9) LOG.info("kvstart = " + kvstart + "; length = " + maxRec);
                        (1148: 9) LOG.info("Record too large for in-memory buffer: " + e.getMessage());
                        (1171: 9) LOG.info("(EQUATOR) " + pos + " kvi " + kvindex +
                        (1189: 9) LOG.info("(RESET) equator " + e + " kv " + kvstart + "(" +
                        (1433: 7) LOG.info("Starting flush of map output");
                        (1452: 13) LOG.info("Spilling map output");
                        (1453: 13) LOG.info("bufstart = " + bufstart + "; bufend = " + bufmark +
                        (1455: 13) LOG.info("kvstart = " + kvstart + "(" + (kvstart * 4) +
                        (1543: 9) LOG.info("Spilling map output");
                        (1544: 9) LOG.info("bufstart = " + bufstart + "; bufend = " + bufmark +
                        (1546: 9) LOG.info("kvstart = " + kvstart + "(" + (kvstart * 4) +
                        (1641: 9) LOG.info("Finished spill " + numSpills);
                        (1868: 15) LOG.debug("MapId=" + mapId + " Reducer=" + parts +
                    Merger.java  (4 usages)
                        (438: 9) LOG.debug("MergeQ: adding: " + file);
                        (568: 7) LOG.info("Merging " + segments.size() + " sorted segments");
                        (667: 11) LOG.info("Down to the last merge-pass, with " + numSegments +
                        (672: 11) LOG.info("Merging " + segmentsToMerge.size() +
                    Queue.java  (6 usages)
                        (300: 5) LOG.debug("created jobQInfo " + queueInfo.getQueueName());
                        (338: 7) LOG.info(" current name " + name + " not equal to " + newState.getName());
                        (344: 9) LOG.info( newState + " has added children in refresh ");
                        (351: 9) LOG.fatal("In the current state, queue " + getName() + " has "
                        (358: 9) LOG.fatal("Number of children for queue " + newState.getName()
                        (372: 11) LOG.info(" Queue " + q.getName() + " not equal to " + newq.getName());
                    QueueConfigurationParser.java  (6 usages)
                        (171: 7) LOG.info(
                        (189: 9) LOG.info("Bad conf file: top-level element not <queues>");
                        (196: 9) LOG.warn("Configuring " + ACLS_ENABLED_TAG + " flag in " +
                        (207: 9) LOG.info(" Bad configuration no queues defined ");
                        (219: 11) LOG.info("At root level only \" queue \" tags are allowed ");
                        (235: 7) LOG.info("Error parsing conf file: " + e);
                    QueueManager.java  (8 usages)
                        (206: 5) LOG.info("AllQueues : " + allQueues + "; LeafQueues : " + leafQueues);
                        (242: 7) LOG.info("Queue " + queueName + " is not present");
                        (247: 7) LOG.info("Cannot submit job to parent queue " + q.getName());
                        (256: 7) LOG.debug("Checking access for the acl " + toFullPropertyName(queueName,
                        (353: 7) LOG.warn(MSG_REFRESH_FAILURE_WITH_CHANGE_OF_HIERARCHY);
                        (371: 9) LOG.error(msg.toString());
                        (390: 5) LOG.info("Queue configuration is refreshed successfully.");
                        (592: 9) LOG.warn("Queue " + queueName + " is not present.");
                    ReduceTask.java  (2 usages)
                        (266: 10) LOG.warn("Further groups got skipped.");
                        (366: 7) LOG.info("Using ShuffleConsumerPlugin: " + shuffleConsumerPlugin);
                    SortedRanges.java  (8 usages)
                        (91: 7) LOG.debug("previousRange "+previousRange);
                        (108: 7) LOG.debug("nextRange "+nextRange +"   startIndex:"+startIndex+
                        (144: 7) LOG.debug("previousRange "+previousRange);
                        (150: 11) LOG.debug("removed previousRange "+previousRange);
                        (162: 7) LOG.debug("nextRange "+nextRange +"   startIndex:"+startIndex+
                        (184: 7) LOG.debug("added "+recRange);
                        (348: 7) LOG.debug("currentIndex "+next +"   "+range);
                        (360: 9) LOG.warn("Skipping index " + next +"-" + range.getEndIndex());
                    Task.java  (24 usages)
                        (307: 5) LOG.fatal(logMsg);
                        (315: 7) LOG.fatal("Failed to contact the tasktracker", ioe);
                        (542: 9) LOG.debug("using new api for output committer");
                        (565: 5) LOG.info(" Using ResourceCalculatorProcessTree : " + pTree);
                        (579: 7) LOG.warn("Task status: \"" + status + "\" truncated to max limit ("
                        (726: 13) LOG.warn("Parent died.  Exiting "+taskId);
                        (735: 11) LOG.info("Communication exception: " + StringUtils.stringifyException(t));
                        (739: 13) LOG.warn("Last retry, killing "+taskId);
                        (797: 7) LOG.debug("sending reportNextRecordRange " + range);
                        (979: 5) LOG.info("Task:" + taskId + " is done."
                        (995: 11) LOG.warn("Failure sending commit pending: " +
                        (1042: 11) LOG.warn("Parent died.  Exiting "+taskId);
                        (1050: 9) LOG.warn("Failure sending status update: " +
                        (1088: 9) LOG.warn ("Could not find output size " , e);
                        (1099: 9) LOG.info("Task '" + taskId + "' done.");
                        (1102: 9) LOG.warn("Failure signalling completion: " +
                        (1128: 9) LOG.warn("Failure asking whether task can commit: " +
                        (1140: 7) LOG.info("Task " + taskId + " is allowed to commit now");
                        (1144: 7) LOG.warn("Failure committing: " +
                        (1157: 7) LOG.warn("Failure cleaning up: " +
                        (1175: 5) LOG.info("Runnning cleanup for the task");
                        (1188: 5) LOG.info("Cleaning up job");
                        (1191: 7) LOG.info("Aborting job with runstate : " + jobRunStateForCleanup.name());
                        (1200: 7) LOG.info("Committing job");
                    TaskLog.java  (2 usages)
                        (91: 7) LOG.error("getTaskLogFileDetail threw an exception " + ie);
                        (639: 9) LOG.debug("mkdirs failed. Ignoring.");
                    TaskStatus.java  (5 usages)
                        (126: 7) LOG.info("task-diagnostic-info for task " + taskid + " : " + info);
                        (134: 7) LOG.info("task-diagnostic-info for task " + taskid + " : "
                        (149: 9) LOG.info("state-string for task " + taskid + " : " + stateString);
                        (196: 7) LOG.error("Trying to set finish time for task " + taskid +
                        (274: 7) LOG.error("Trying to set illegal startTime for task : " + taskid +
                org.apache.hadoop.mapred.jobcontrol  (1 usage)
                    Job.java  (1 usage)
                        (99: 7) LOG.info("Exception" + ioe);
                org.apache.hadoop.mapred.lib  (4 usages)
                    FieldSelectionMapReduce.java  (1 usage)
                        (171: 5) LOG.info(specToString());
                    MultithreadedMapRunner.java  (3 usages)
                        (74: 7) LOG.debug("Configuring jobConf " + jobConf.getJobName() +
                        (157: 9) LOG.debug("Finished dispatching all Mappper.map calls, job "
                        (170: 13) LOG.debug("Awaiting all running Mappper.map calls to finish, job "
                org.apache.hadoop.mapred.pipes  (17 usages)
                    Application.java  (3 usages)
                        (151: 5) LOG.debug("Authentication succeeded");
                        (194: 5) LOG.debug("Waiting for authentication response");
                        (214: 5) LOG.info("Aborting because of " + StringUtils.stringifyException(t));
                    BinaryProtocol.java  (9 usages)
                        (126: 11) LOG.debug("Handling uplink command " + cmd);
                        (131: 13) LOG.warn("Message " + cmd + " received before authentication is "
                        (157: 13) LOG.debug("Pipe child done");
                        (166: 11) LOG.error(StringUtils.stringifyException(e));
                        (259: 5) LOG.debug("closing connection");
                        (268: 5) LOG.debug("Sending AUTHENTICATION_REQ, digest=" + digest + ", challenge="
                        (276: 5) LOG.debug("starting downlink");
                        (334: 5) LOG.debug("Sent close command");
                        (339: 5) LOG.debug("Sent abort command");
                    PipesReducer.java  (3 usages)
                        (81: 9) LOG.info("starting application");
                        (117: 7) LOG.info("waiting for finish");
                        (119: 7) LOG.info("got done");
                    Submitter.java  (2 usages)
                        (475: 9) LOG.warn("-jobconf option is deprecated, please use -D instead.");
                        (505: 7) LOG.info("Error : " + pe);
                org.apache.hadoop.mapreduce  (30 usages)
                    Cluster.java  (4 usages)
                        (89: 9) LOG.debug("Trying ClientProtocolProvider : "
                        (102: 13) LOG.debug("Picked " + provider.getClass().getName()
                        (107: 13) LOG.debug("Cannot pick " + provider.getClass().getName()
                        (112: 11) LOG.info("Failed to use " + provider.getClass().getName()
                    Job.java  (13 usages)
                        (1222: 5) LOG.info("The url to track the job: " + getTrackingURL());
                        (1267: 5) LOG.info("Running job: " + jobId);
                        (1288: 9) LOG.info("Job " + jobId + " running in uber mode : " + isUber());
                        (1295: 9) LOG.info(report);
                        (1306: 7) LOG.info("Job " + jobId + " completed successfully");
                        (1308: 7) LOG.info("Job " + jobId + " failed with state " + status.getState() +
                        (1313: 7) LOG.info(counters.toString());
                        (1368: 11) LOG.info(event.toString());
                        (1374: 11) LOG.info(event.toString());
                        (1387: 11) LOG.info(event.toString());
                        (1391: 9) LOG.info(event.toString());
                        (1403: 7) LOG.warn(PROGRESS_MONITOR_POLL_INTERVAL_KEY +
                        (1416: 7) LOG.warn(COMPLETION_POLL_INTERVAL_KEY +
                    JobSubmissionFiles.java  (1 usage)
                        (119: 9) LOG.info("Permissions on staging directory " + stagingArea + " are " +
                    JobSubmitter.java  (12 usages)
                        (140: 7) LOG.warn("Hadoop command-line option parsing not performed. " +
                        (160: 5) LOG.debug("default FileSystem: " + jtFs.getUri());
                        (249: 7) LOG.warn("No job jar file set.  User classes may not be found. "+
                        (354: 7) LOG.debug("Configuring job " + jobId + " with " + submitJobDir
                        (366: 7) LOG.debug("Creating splits at " + jtFs.makeQualified(submitJobDir));
                        (369: 7) LOG.info("number of splits:" + maps);
                        (401: 9) LOG.info("Cleaning up the staging area " + submitJobDir);
                        (439: 5) LOG.info("Submitting tokens for job: " + jobId);
                        (441: 7) LOG.info(token);
                        (542: 7) LOG.info("loading user's secret keys from " + tokensFileName);
                        (562: 9) LOG.warn("couldn't parse Token Cache JSON file with user secret keys");
                        (573: 5) LOG.debug("adding the following namenodes' delegation tokens:" +
                org.apache.hadoop.mapreduce.counters  (1 usage)
                    AbstractCounters.java  (1 usage)
                        (234: 9) LOG.warn("Group " + groupName + " is deprecated. Use " + newGroupName
                org.apache.hadoop.mapreduce.jobhistory  (1 usage)
                    JobHistoryParser.java  (1 usage)
                        (116: 7) LOG.info("Caught exception parsing history file after " + eventCtr +
                org.apache.hadoop.mapreduce.lib.db  (24 usages)
                    BigDecimalSplitter.java  (2 usages)
                        (65: 7) LOG.error("Cannot find a range for NUMERIC or DECIMAL fields with one end NULL.");
                        (132: 7) LOG.warn("Set BigDecimal splitSize to MIN_INCREMENT");
                    DataDrivenDBInputFormat.java  (5 usages)
                        (207: 9) LOG.debug("SQLException closing resultset: " + se.toString());
                        (215: 9) LOG.debug("SQLException closing statement: " + se.toString());
                        (222: 9) LOG.debug("SQLException committing split transaction: " + se.toString());
                        (269: 9) LOG.warn("Could not find " + SUBSTITUTE_TOKEN + " token in query: " + query
                        (285: 5) LOG.debug("Creating db record reader for db product: " + dbProductName);
                    DataDrivenDBRecordReader.java  (2 usages)
                        (124: 9) LOG.error("Could not find the clause substitution token "
                        (133: 5) LOG.debug("Using query: " + query.toString());
                    DateSplitter.java  (1 usage)
                        (149: 7) LOG.warn("Encountered a NULL date in the split column. Splits may be poorly balanced.");
                    DBInputFormat.java  (1 usage)
                        (362: 7) LOG.debug("Exception on close", sqlE);
                    DBOutputFormat.java  (1 usage)
                        (101: 11) LOG.warn(StringUtils.stringifyException(ex));
                    FloatSplitter.java  (4 usages)
                        (49: 5) LOG.warn("Generating splits for a floating-point index column. Due to the");
                        (50: 5) LOG.warn("imprecise representation of floating-point values in Java, this");
                        (51: 5) LOG.warn("may result in an incomplete import.");
                        (52: 5) LOG.warn("You are strongly encouraged to choose an integral split column.");
                    OracleDBRecordReader.java  (5 usages)
                        (117: 7) LOG.error("Could not find method setSessionTimeZone in " + conn.getClass().getName(), ex);
                        (130: 7) LOG.info("Time zone has been set to " + clientTimeZone);
                        (132: 7) LOG.warn("Time zone " + clientTimeZone +
                        (134: 7) LOG.warn("Setting default time zone: GMT");
                        (139: 9) LOG.error("Could not set time zone for oracle connection", ex2);
                    TextSplitter.java  (3 usages)
                        (66: 5) LOG.warn("Generating splits for a textual index column.");
                        (67: 5) LOG.warn("If your database sorts in a case-insensitive order, "
                        (69: 5) LOG.warn("You are strongly encouraged to choose an integral split column.");
                org.apache.hadoop.mapreduce.lib.fieldsel  (2 usages)
                    FieldSelectionMapper.java  (1 usage)
                        (93: 5) LOG.info(FieldSelectionHelper.specToString(fieldSeparator,
                    FieldSelectionReducer.java  (1 usage)
                        (89: 5) LOG.info(FieldSelectionHelper.specToString(fieldSeparator,
                org.apache.hadoop.mapreduce.lib.input  (4 usages)
                    FileInputFormat.java  (2 usages)
                        (245: 5) LOG.info("Total input paths to process : " + result.size());
                        (303: 5) LOG.debug("Total # of splits: " + splits.size());
                    LineRecordReader.java  (1 usage)
                        (178: 7) LOG.info("Skipped line of size " + newSize + " at pos " +
                    SequenceFileInputFilter.java  (1 usage)
                        (263: 9) LOG.warn(e);
                org.apache.hadoop.mapreduce.lib.jobcontrol  (6 usages)
                    ControlledJob.java  (1 usage)
                        (339: 7) LOG.info(getJobName()+" got an error while submitting ",ioe);
                    JobControl.java  (5 usages)
                        (226: 13) LOG.debug("Checking state of job "+j);
                        (264: 7) LOG.error("Error while trying to run jobs.",t);
                        (280: 9) LOG.error("Error while tyring to clean up "+j.getJobName(), e);
                        (282: 9) LOG.error("Error while tyring to clean up "+j.getJobName(), e);
                        (326: 2) LOG.error("Job control has circular dependency for the  job "
                org.apache.hadoop.mapreduce.lib.map  (1 usage)
                    MultithreadedMapper.java  (1 usage)
                        (134: 7) LOG.debug("Configuring multithread runner to use " + numberOfThreads +
                org.apache.hadoop.mapreduce.lib.output  (14 usages)
                    FileOutputCommitter.java  (14 usages)
                        (285: 9) LOG.error("Mkdirs failed to create " + jobAttemptPath);
                        (288: 7) LOG.warn("Output Path is null in setupJob()");
                        (315: 7) LOG.warn("Output Path is null in commitJob()");
                        (330: 6) LOG.debug("Merging data from "+from+" to "+to);
                        (376: 7) LOG.warn("Output Path is null in cleanupJob()");
                        (432: 9) LOG.info("Saved output of task '" + attemptId + "' to " +
                        (435: 9) LOG.warn("No Output found for " + attemptId);
                        (438: 7) LOG.warn("Output Path is null in commitTask()");
                        (460: 9) LOG.warn("Could not delete "+taskAttemptPath);
                        (463: 7) LOG.warn("Output Path is null in abortTask()");
                        (511: 7) LOG.debug("Trying to recover task from " + previousCommittedTaskPath
                        (526: 9) LOG.info("Saved output of " + attemptId + " to " + committedTaskPath);
                        (528: 9) LOG.warn(attemptId+" had no output to recover.");
                        (531: 7) LOG.warn("Output Path is null in recoverTask()");
                org.apache.hadoop.mapreduce.lib.partition  (3 usages)
                    InputSampler.java  (2 usages)
                        (204: 7) LOG.debug("seed: " + seed);
                        (321: 5) LOG.info("Using " + samples.length + " samples");
                    KeyFieldBasedPartitioner.java  (1 usage)
                        (71: 7) LOG.warn("Using deprecated num.key.fields.for.partition. " +
                org.apache.hadoop.mapreduce.security  (2 usages)
                    TokenCache.java  (2 usages)
                        (125: 9) LOG.info("Got dt for " + fs.getUri() + "; "+token);
                        (172: 7) LOG.debug("Task: Loaded jobTokenFile from: "+
                org.apache.hadoop.mapreduce.task.reduce  (55 usages)
                    EventFetcher.java  (9 usages)
                        (64: 5) LOG.info(reduce + " Thread started: " + getName());
                        (72: 13) LOG.info(reduce + ": " + "Got " + numNewMaps + " new map-outputs");
                        (74: 11) LOG.debug("GetMapEventsThread about to sleep for " + SLEEP_TIME);
                        (79: 11) LOG.info("EventFetcher is interrupted.. Returning");
                        (82: 11) LOG.info("Exception in getting events", ie);
                        (107: 7) LOG.warn("Got interrupted while joining " + getName(), ie);
                        (129: 7) LOG.debug("Got " + events.length + " map completion events from " +
                        (168: 11) LOG.info("Ignoring obsolete output of " + event.getTaskStatus() +
                        (173: 11) LOG.info("Ignoring output of failed map TIP: '" +
                    Fetcher.java  (16 usages)
                        (175: 7) LOG.warn("Got interrupt while joining " + getName(), ie);
                        (215: 7) LOG.debug("Fetcher " + id + " going to fetch from " + host + " for: "
                        (256: 7) LOG.debug("url="+msgToEncode+";encHash="+encHash+";replyHash="+replyHash);
                        (259: 7) LOG.info("for url="+msgToEncode+" sent hash and received reply");
                        (263: 7) LOG.warn("Failed to connect to " + host + " with " + remaining.size() +
                        (299: 9) LOG.warn("copyMapOutput failed for tasks "+Arrays.toString(failedTasks));
                        (342: 9) LOG.warn("Invalid map id ", e);
                        (355: 9) LOG.debug("header: " + mapId + ", len: " + compressedLength +
                        (364: 9) LOG.info("fetcher#" + id + " - MergeManager returned status WAIT ...");
                        (370: 7) LOG.info("fetcher#" + id + " about to shuffle output of map " +
                        (388: 9) LOG.info("fetcher#" + id + " failed to read map header" +
                        (398: 7) LOG.warn("Failed to shuffle output of " + mapId +
                        (422: 7) LOG.warn(getName() + " invalid lengths in map output header: id: " +
                        (430: 7) LOG.warn(getName() + " data for the wrong reduce map: " +
                        (439: 7) LOG.warn("Invalid map-output! Received output for " + mapId);
                        (468: 5) LOG.debug("MapOutput URL for " + host + " -> " + url.toString());
                    InMemoryMapOutput.java  (1 usage)
                        (100: 7) LOG.info("Read " + memory.length + " bytes from map-output for " +
                    MergeManagerImpl.java  (20 usages)
                        (193: 5) LOG.info("MergerManager: memoryLimit=" + memoryLimit + ", " +
                        (254: 7) LOG.info(mapId + ": Shuffling to disk since " + requestedSize +
                        (277: 7) LOG.debug(mapId + ": Stalling shuffle since usedMemory (" + usedMemory
                        (284: 5) LOG.debug(mapId + ": Proceeding with shuffle since usedMemory ("
                        (307: 5) LOG.info("closeInMemoryFile -> map-output of size: " + mapOutput.getSize()
                        (315: 7) LOG.info("Starting inMemoryMerger's merge since commitMemory=" +
                        (334: 5) LOG.info("closeInMemoryMergedFile -> size: " + mapOutput.getSize() +
                        (392: 7) LOG.info("Initiating Memory-to-Memory merge with " + noInMemorySegments +
                        (406: 7) LOG.info(reduceId +
                        (461: 9) LOG.info("Initiating in-memory merge with " + noInMemorySegments +
                        (482: 9) LOG.info(reduceId +
                        (512: 9) LOG.info("No ondisk files to merge...");
                        (520: 7) LOG.info("OnDiskMerger: We have  " + inputs.size() +
                        (565: 7) LOG.info(reduceId +
                        (664: 5) LOG.info("finalMerge called with " +
                        (737: 9) LOG.info("Merged " + numMemDiskSegments + " segments, " +
                        (743: 9) LOG.info("Keeping " + numMemDiskSegments + " segments, " +
                        (760: 7) LOG.debug("Disk file: " + file + " Length is " + fileLength);
                        (767: 5) LOG.info("Merging " + onDisk.length + " files, " +
                        (782: 5) LOG.info("Merging " + finalSegments.size() + " segments, " +
                    MergeThread.java  (1 usage)
                        (65: 7) LOG.info(getName() + ": Starting merge with " + toMergeInputs.size() +
                    OnDiskMapOutput.java  (2 usages)
                        (91: 7) LOG.info("Read " + (compressedLength - bytesLeft) +
                        (126: 7) LOG.info("failure to clean up " + tmpOutputPath, ie);
                    ShuffleScheduler.java  (6 usages)
                        (147: 7) LOG.debug("map " + mapId + " done " + status.getStateString());
                        (214: 7) LOG.info("Reporting fetch failure for " + mapId + " to jobtracker.");
                        (260: 7) LOG.fatal("Shuffle failed with too many fetch failures " +
                        (319: 7) LOG.info("Assiging " + host + " with " + host.getNumKnownMapOutputs() +
                        (349: 5) LOG.info("assigned " + includedMaps + " of " + totalSize + " to " +
                        (361: 5) LOG.info(host + " freed by " + Thread.currentThread().getName() + " in " +
                org.apache.hadoop.mapreduce.tools  (1 usage)
                    CLI.java  (1 usage)
                        (152: 9) LOG.info(iae);
                org.apache.hadoop.mapreduce.util  (35 usages)
                    LinuxResourceCalculatorPlugin.java  (9 usages)
                        (190: 7) LOG.warn("Error reading the stream " + io);
                        (198: 11) LOG.warn("Error closing the stream " + in);
                        (201: 9) LOG.warn("Error closing the stream " + fReader);
                        (243: 7) LOG.warn("Error reading the stream " + io);
                        (251: 11) LOG.warn("Error closing the stream " + in);
                        (254: 9) LOG.warn("Error closing the stream " + fReader);
                        (292: 7) LOG.warn("Error reading the stream " + io);
                        (300: 11) LOG.warn("Error closing the stream " + in);
                        (303: 9) LOG.warn("Error closing the stream " + fReader);
                    ProcessTree.java  (8 usages)
                        (61: 7) LOG.warn("setsid is not available on this machine. So not using it.");
                        (64: 7) LOG.info("setsid exited with exit code " + shexec.getExitCode());
                        (133: 7) LOG.warn("Error executing shell command " + ioe);
                        (136: 9) LOG.info("Sending signal to all members of process group " + pid
                        (139: 9) LOG.info("Signaling process " + pid
                        (208: 9) LOG.warn("Thread sleep is interrupted.");
                        (298: 7) LOG.warn("Error executing shell command "
                        (323: 7) LOG.warn("Error executing shell command "
                    ProcfsBasedProcessTree.java  (18 usages)
                        (73: 7) LOG.error(StringUtils.stringifyException(e));
                        (87: 7) LOG.error(StringUtils.stringifyException(e));
                        (157: 9) LOG.info("ProcfsBasedProcessTree currently is supported only on "
                        (162: 7) LOG.warn("Failed to get Operating System name. " + se);
                        (237: 9) LOG.debug(this.toString());
                        (287: 7) LOG.warn("Unexpected: Process with PID " + pidStr +
                        (292: 7) LOG.debug(pidStr + " is a process group leader, as expected.");
                        (329: 5) LOG.debug("Killing ProcfsBasedProcessTree of " + pid);
                        (341: 11) LOG.warn(StringUtils.stringifyException(e));
                        (516: 7) LOG.info("The process " + pinfo.getPid()
                        (533: 9) LOG.warn("Unexpected: procfs stat file is not in the expected format"
                        (538: 7) LOG.warn("Error reading the stream " + io);
                        (547: 11) LOG.warn("Error closing the stream " + in);
                        (550: 9) LOG.warn("Error closing the stream " + fReader);
                        (670: 11) LOG.warn("Sum of stime (" + this.stime + ") and utime (" + this.utime
                        (724: 9) LOG.warn("Error reading the stream " + io);
                        (733: 13) LOG.warn("Error closing the stream " + in);
                        (736: 11) LOG.warn("Error closing the stream " + fReader);
            hadoop-mapreduce-client-hs  (52 usages)
                org.apache.hadoop.mapreduce.v2.hs  (51 usages)
                    CachedHistoryStorage.java  (5 usages)
                        (64: 5) LOG.info("CachedHistoryStorage Init");
                        (89: 9) LOG.debug("Adding " + job.getID() + " to loaded job cache");
                        (105: 7) LOG.debug("Looking for Job " + jobId);
                        (129: 5) LOG.debug("Called getAllPartialJobs()");
                        (139: 7) LOG.warn("Error trying to scan for all FileInfos", e);
                    CompletedJob.java  (4 usages)
                        (94: 5) LOG.info("Loading job: " + jobId + " from file: " + historyFile);
                        (270: 9) LOG.warn("Cannot constuct TACEStatus from TaskAtemptState: ["
                        (323: 5) LOG.info("Loading history file: [" + historyFileAbsolute + "]");
                        (350: 7) LOG.info("TaskInfo loaded");
                    HistoryClientService.java  (1 usage)
                        (143: 5) LOG.info("Instantiated MRClientService at " + this.bindAddress);
                    HistoryFileManager.java  (28 usages)
                        (104: 11) LOG.error("Dropping " + key
                        (148: 9) LOG.debug("Adding " + jobId + " to job list cache with "
                        (173: 21) LOG.error("Error while trying to delete history files" +
                        (177: 19) LOG.warn("Waiting to remove " + key
                        (220: 11) LOG.error("Error while trying to scan the directory " + p, e);
                        (275: 11) LOG.info("No file for job-history with " + jobId + " found in cache!");
                        (281: 11) LOG.info("No file for jobConf with " + jobId + " found in cache!");
                        (287: 11) LOG.info("No summary file for job: " + jobId);
                        (291: 19) SUMMARY_LOG.info(jobSummaryString);
                        (292: 11) LOG.info("Deleting JobSummary file: [" + summaryFile + "]");
                        (318: 9) LOG.error("Error while trying to move a job to done", t);
                        (439: 7) LOG.info("error creating done directory on dfs " + e);
                        (476: 9) LOG.info("Perms after creating " + fsStatus.getPermission().toShort()
                        (479: 11) LOG.info("Explicitly setting permissions to : " + fsp.toShort()
                        (484: 9) LOG.info("Directory: [" + path + "] already exists.");
                        (495: 5) LOG.info("Initializing Existing Jobs...");
                        (511: 7) LOG.warn("Could not find timestamp portion from path: "
                        (516: 7) LOG.warn("Could not find serial portion from path: "
                        (525: 7) LOG.debug("Adding " + serialDirPath + " to serial index");
                        (531: 7) LOG.warn("Could not find timestamp portion from path: " + serialDirPath
                        (536: 7) LOG.warn("Could not find serial portion from path: "
                        (545: 7) LOG.debug("Adding " + path + " to job list cache.");
                        (551: 9) LOG.debug("Adding in history for " + fs.getPath());
                        (655: 13) LOG.warn("Error cleaning up a HistoryFile that is out of date.", e);
                        (664: 17) LOG.info("Failed to process fileInfo for job: " +
                        (763: 5) LOG.info("Moving " + src.toString() + " to " + target.toString());
                        (785: 9) LOG.info("Perms after creating " + fsStatus.getPermission().toShort()
                        (788: 11) LOG.info("Explicitly setting permissions to : " + fsp.toShort()
                    JobHistory.java  (10 usages)
                        (75: 5) LOG.info("JobHistory Init");
                        (135: 5) LOG.info("Stopping JobHistory");
                        (137: 7) LOG.info("Stopping History Cleaner/Move To Done");
                        (150: 9) LOG.warn("HistoryCleanerService/move to done shutdown may not have " +
                        (175: 9) LOG.info("Starting scan to move intermediate done files");
                        (178: 9) LOG.error("Error while scanning intermediate done dir ", e);
                        (185: 7) LOG.info("History Cleaner started");
                        (189: 9) LOG.warn("Error trying to clean up ", e);
                        (191: 7) LOG.info("History Cleaner complete");
                        (210: 7) LOG.debug("Called getAllJobs(AppId): " + appID);
                    JobHistoryServer.java  (2 usages)
                        (118: 7) LOG.error("Error while starting the Secret Manager threads", io);
                        (148: 7) LOG.fatal("Error starting JobHistoryServer", t);
                    PartialJob.java  (1 usage)
                        (85: 7) LOG.warn("Exception while parsing job state. Defaulting to KILLED", e);
                org.apache.hadoop.mapreduce.v2.hs.webapp  (1 usage)
                    HsJobsBlock.java  (1 usage)
                        (70: 5) LOG.info("Getting list of all Jobs.");
            hadoop-mapreduce-client-jobclient  (30 usages)
                org.apache.hadoop.mapred  (30 usages)
                    ClientCache.java  (3 usages)
                        (62: 9) LOG.warn("Could not connect to History server.", e);
                        (88: 5) LOG.debug("Connecting to HistoryServer at: " + serviceAddr);
                        (90: 5) LOG.debug("Connected to HistoryServer at: " + serviceAddr);
                    ClientServiceDelegate.java  (19 usages)
                        (156: 9) LOG.info("Could not get Job info from RM for job " + jobId
                        (162: 11) LOG.debug("AM not assigned to Job. Waiting to get the AM ...");
                        (165: 11) LOG.debug("Application state is " + application.getYarnApplicationState());
                        (170: 13) LOG.info("Job " + jobId + " is running, but the host is unknown."
                        (187: 11) LOG.debug("Connecting to " + serviceAddr);
                        (197: 13) LOG.info("Network ACL closed to AM for job " + jobId
                        (208: 9) LOG.info("Could not connect to " + serviceAddr +
                        (213: 11) LOG.warn("getProxy() call interruped", e1);
                        (218: 11) LOG.info("Could not get Job info from RM for job " + jobId
                        (223: 9) LOG.warn("getProxy() call interruped", e);
                        (256: 7) LOG.info("Application state is completed. FinalApplicationStatus="
                        (267: 7) LOG.warn("Job History Server is not configured.");
                        (275: 5) LOG.trace("Connecting to ApplicationMaster at: " + serviceAddr);
                        (280: 5) LOG.trace("Connected to ApplicationMaster at: " + serviceAddr);
                        (302: 9) LOG.warn("Exception thrown by remote end.", yre);
                        (306: 11) LOG.warn("Error from remote end: " + e
                        (308: 11) LOG.debug("Tracing remote error ", e.getTargetException());
                        (311: 9) LOG.debug("Failed to contact AM/History for job " + jobId +
                        (320: 9) LOG.debug("Failed to contact AM/History for job " + jobId
                    ResourceMgrDelegate.java  (2 usages)
                        (77: 5) LOG.warn("getBlacklistedTrackers - Not implemented yet");
                        (138: 5) LOG.debug("getStagingAreaDir: dir=" + path);
                    YARNRunner.java  (6 usages)
                        (337: 5) LOG.debug("AppMaster capability = " + capability);
                        (349: 5) LOG.debug("Creating setup context, jobSubmitDir url is "
                        (367: 7) LOG.info("Job jar is not present. "
                        (437: 5) LOG.debug("Command to launch container for ApplicationMaster is : "
                        (569: 7) LOG.debug("Error when checking for application status", io);
                        (609: 7) LOG.warn("Usage of -Djava.library.path in " + javaConf + " can cause " +
            hadoop-mapreduce-client-shuffle  (20 usages)
                org.apache.hadoop.mapred  (20 usages)
                    FadvisedChunkedFile.java  (1 usage)
                        (75: 9) LOG.warn("Failed to manage OS cache for " + identifier, t);
                    FadvisedFileRegion.java  (1 usage)
                        (77: 9) LOG.warn("Failed to manage OS cache for " + identifier, t);
                    ShuffleHandler.java  (18 usages)
                        (250: 7) LOG.info("Added token for " + jobId.toString());
                        (253: 7) LOG.error("Error during initApp", e);
                        (303: 5) LOG.info(getName() + " listening on port " + port);
                        (324: 7) LOG.error("Error during getMeta", e);
                        (416: 9) LOG.debug("RECV: " + request.getUri() +
                        (453: 9) LOG.warn("Shuffle failure ", e);
                        (471: 11) LOG.error("Shuffle error ", e);
                        (485: 9) LOG.info("Request for unknown token " + appid);
                        (494: 9) LOG.info("Missing header hash for " + appid);
                        (499: 9) LOG.debug("verifying request. enc_str=" + enc_str + "; hash=..." +
                        (511: 9) LOG.debug("Fetcher request verfied. enc_str=" + enc_str + ";reply=" +
                        (530: 7) LOG.debug("DEBUG0 " + base);
                        (537: 7) LOG.debug("DEBUG1 " + base + " : " + mapOutputFileName + " : " +
                        (551: 9) LOG.info(spillfile + " not found");
                        (607: 11) LOG.debug("Ignoring closed channel error", cause);
                        (612: 11) LOG.debug("Ignoring client socket close", cause);
                        (617: 7) LOG.error("Shuffle error: ", cause);
                        (619: 9) LOG.error("Shuffle error " + e);
            hadoop-mapreduce-examples  (27 usages)
                org.apache.hadoop.examples  (8 usages)
                    BaileyBorweinPlouffe.java  (4 usages)
                        (95: 7) LOG.info("offset=" + offset + ", length=" + length);
                        (128: 7) LOG.info("hex.size() = " + hex.size());
                        (155: 9) LOG.info("Writing text output to " + outfile);
                        (220: 7) LOG.info("Map #" + i + ": workload=" + workload(offset, size)
                    DBCountPageView.java  (4 usages)
                        (118: 7) LOG.warn("Exception occurred while closing connection :"
                        (126: 9) LOG.warn("Exception occurred while shutting down HSQLDB :"
                        (270: 7) LOG.info("totalPageview=" + totalPageview);
                        (271: 7) LOG.info("sumPageview=" + sumPageview);
                org.apache.hadoop.examples.dancing  (2 usages)
                    DancingLinks.java  (2 usages)
                        (222: 5) LOG.debug("cover " + col.head.name);
                        (244: 5) LOG.debug("uncover " + col.head.name);
                org.apache.hadoop.examples.pi  (8 usages)
                    DistSum.java  (6 usages)
                        (162: 7) LOG.info(s = "sigma=" + sigma);
                        (170: 7) LOG.info(s = "result=" + result);
                        (276: 13) //LOG.info("parts[" + i + "] = " + parts[i]);
                        (350: 11) LOG.info("parts[" + i + "] = " + parts[i]);
                        (370: 9) LOG.info("index=" + index);
                        (502: 9) //  LOG.info("line = " + line);
                    Parser.java  (2 usages)
                        (56: 9) //      LOG.info("line = " + line);
                        (111: 7) //LOG.info("m=" + m.toString().replace(", ", ",\n  "));
                org.apache.hadoop.examples.terasort  (9 usages)
                    TeraGen.java  (1 usage)
                        (177: 7) LOG.info("Generating " + totalRows + " using " + numSplits);
                    TeraScheduler.java  (5 usages)
                        (148: 7) LOG.debug("picking " + result);
                        (159: 7) LOG.debug("  examine: " + cur.filename + " " + cur.locations.size());
                        (175: 9) LOG.debug(" best: " + best[i].filename);
                        (236: 5) LOG.info("starting solve");
                        (250: 5) LOG.info("done");
                    TeraSort.java  (3 usages)
                        (281: 5) LOG.info("starting");
                        (305: 9) LOG.error(e.getMessage());
                        (317: 5) LOG.info("done");
            hadoop-rumen  (61 usages)
                org.apache.hadoop.tools.rumen  (61 usages)
                    DeskewedJobTraceReader.java  (3 usages)
                        (159: 7) LOG.error("The current job was submitted earlier than the previous one");
                        (160: 7) LOG.error("Its jobID is " + result.getJobID());
                        (161: 7) LOG.error("Its submit time is " + result.getSubmitTime()
                    Folder.java  (22 usages)
                        (175: 9) LOG.error("You must have an input cycle length.");
                        (194: 9) LOG.warn("This run effectively has a -seed of " + randomSeed);
                        (242: 9) LOG.error("The job trace is empty");
                        (250: 9) LOG.info("starts-after time is specified. Initial job submit time : "
                        (261: 9) LOG.debug("Considering jobs with submit time greater than "
                        (265: 11) LOG.error("No more jobs to process in the trace with 'starts-after'"+
                        (269: 9) LOG.info("The first job has a submit time of " + job.getSubmitTime());
                        (283: 9) LOG.debug("The first job has a submit time of " + firstJobSubmitTime);
                        (311: 19) LOG.debug("The next segment name is " + nextSegment);
                        (336: 15) LOG.debug("Creating " + nextSegment
                        (371: 9) LOG.error("All of your job[s] have the same submit time."
                        (379: 7) LOG.warn("Your input trace spans "
                        (386: 9) LOG.warn("run: submitTimeSpan = " + submitTimeSpan + ", numberJobs = "
                        (391: 9) LOG.warn("You needed a -skew-buffer-length of "
                        (398: 9) LOG.warn("run: timeDilation = " + timeDilation + ", concentration = "
                        (400: 9) LOG.warn("The transcription probability is " + tProbability);
                        (422: 11) LOG.debug("A job with submit time of "
                        (430: 11) LOG.debug("That job's submit time is adjusted to "
                        (444: 11) LOG.debug("The most recent job has an adjusted submit time of "
                        (446: 11) LOG.debug(" Its replacement in the heap will come from input engine "
                        (456: 13) LOG.debug("That input engine is depleted.");
                        (462: 13) LOG.debug("The replacement has an adjusted submit time of "
                    HadoopLogsAnalyzer.java  (15 usages)
                        (484: 7) LOG.info("We dropped " + (inputDirectoryFiles.length - dropPoint)
                        (532: 7) LOG.info("File closed: " + currentFileName);
                        (552: 5) LOG.info("\nOpening file " + currentFileName
                        (952: 7) LOG.warn(
                        (1021: 9) LOG.error("A task status you don't know about is \"" + status + "\".",
                        (1033: 9) LOG.error("A task type you don't know about is \"" + taskType + "\".",
                        (1285: 9) LOG.error("A map attempt status you don't know about is \"" + status
                        (1375: 7) LOG.warn(
                        (1438: 9) LOG.warn("A map attempt status you don't know about is \"" + status
                        (1524: 7) LOG.error(
                        (1703: 9) LOG.debug("" + lineNumber + " " + line.second());
                        (1716: 11) LOG.warn("anomalous line #" + lineNumber + ":" + line, e);
                        (1837: 7) LOG.error("", e);
                        (1841: 7) LOG.error("", e);
                        (1845: 7) LOG.error("", e);
                    HistoryEventEmitter.java  (2 usages)
                        (70: 7) LOG.warn("The counter string, \"" + counters + "\" is badly formatted.");
                        (78: 7) LOG.warn("HistoryEventEmitters: null counter detected:");
                    ParsedJob.java  (1 usage)
                        (162: 5) LOG.info("ParsedJob details:" + obtainTotalCounters() + ";"
                    ParsedTask.java  (2 usages)
                        (119: 5) LOG.info("ParsedTask details:" + obtainCounters()
                        (124: 7) LOG.info(l.getLayers() + ";" + l.toString());
                    ParsedTaskAttempt.java  (1 usage)
                        (113: 5) LOG.info("ParsedTaskAttempt details:" + obtainCounters()
                    TraceBuilder.java  (5 usages)
                        (221: 11) LOG.warn("Unable to bind Path " + p + " .  Skipping...", e);
                        (238: 17) LOG.warn("File skipped: Invalid file name: "
                        (256: 19) LOG.warn("File skipped: Cannot find suitable parser: "
                        (273: 13) LOG.warn("TraceBuilder got an error while processing the [possibly virtual] file "
                        (284: 9) LOG.warn("No job found in traces: ");
                    ZombieJob.java  (10 usages)
                        (157: 11) LOG.warn("TaskType for a MapTask is not Map. task="
                        (168: 15) LOG.warn("Bad location layer format for task "+mapTask.getTaskID());
                        (173: 15) LOG.warn("Bad location layer format for task "+mapTask.getTaskID() + ": " + layers);
                        (183: 11) LOG.warn("InputBytes for task "+mapTask.getTaskID()+" is not defined.");
                        (194: 9) LOG.warn("TotalMaps for job " + job.getJobID()
                        (246: 7) LOG.warn(name +" not defined for "+id);
                        (330: 7) LOG.warn("Task " + task.getTaskID() + " has nulll TaskType");
                        (334: 7) LOG.warn("Task " + task.getTaskID() + " has nulll TaskStatus");
                        (345: 7) LOG.warn("TaskAttempt " + attempt.getResult() + " has nulll Result");
                        (520: 7) LOG.warn("Negative running time for task "+id+": "+time);
            hadoop-streaming  (45 usages)
                org.apache.hadoop.streaming  (42 usages)
                    PipeMapper.java  (1 usage)
                        (119: 9) LOG.info(getContext() , io);
                    PipeMapRed.java  (22 usages)
                        (199: 7) LOG.info("PipeMapRed exec " + Arrays.asList(argvSplit));
                        (221: 7) LOG.error("configuration exception", e);
                        (224: 7) LOG.error("configuration exception", e);
                        (233: 7) LOG.info("JobConf set minRecWrittenToEnableSkip_ ="
                        (273: 9) LOG.info("Skip env entry:" + nv[i]);
                        (282: 7) LOG.debug("Add  env entry:" + name + "=" + value);
                        (323: 11) LOG.info("PipeMapRed.waitOutputThreads(): subprocess exited with " +
                        (390: 13) LOG.info(hline);
                        (395: 9) LOG.warn(th);
                        (403: 11) LOG.info(io);
                        (442: 15) LOG.warn("Cannot parse reporter line: " + lineStr);
                        (460: 11) LOG.info("MRErrorThread done");
                        (464: 9) LOG.warn(th);
                        (474: 11) LOG.info(io);
                        (499: 11) LOG.warn("Cannot parse counter increment '" + columns[2] +
                        (503: 9) LOG.warn("Cannot parse counter line: " + line);
                        (521: 9) LOG.info("mapRedFinished");
                        (529: 11) LOG.warn(io);
                        (535: 9) LOG.warn(io);
                        (538: 7) LOG.info("mapRedFinished");
                        (540: 7) LOG.info("PipeMapRed failed!", e);
                        (548: 7) LOG.info(info);
                    StreamBaseRecordReader.java  (1 usage)
                        (108: 7) LOG.info(status);
                    StreamInputFormat.java  (1 usage)
                        (47: 5) LOG.info("getRecordReader start.....split=" + split);
                    StreamJob.java  (16 usages)
                        (131: 7) LOG.debug("Error in streaming job", ex);
                        (253: 7) LOG.error(oe.getMessage());
                        (290: 9) LOG.warn("-file option is deprecated, please use generic option" +
                        (321: 9) LOG.warn("-dfs option is deprecated, please use -fs instead.");
                        (337: 9) LOG.warn("-cacheArchive option is deprecated, please use -archives instead.");
                        (345: 9) LOG.warn("-cacheFile option is deprecated, please use -files instead.");
                        (353: 9) LOG.warn("-jobconf option is deprecated, please use -D instead.");
                        (731: 7) LOG.warn("-additionalconfspec option is deprecated, please use -conf instead.");
                        (1017: 9) LOG.info("Job is running in background.");
                        (1019: 9) LOG.error("Job not Successful!");
                        (1022: 7) LOG.info("Output directory: " + output_);
                        (1024: 7) LOG.error("Error launching job , bad input path : " + fe.getMessage());
                        (1027: 7) LOG.error("Error launching job , Invalid job conf : " + je.getMessage());
                        (1030: 7) LOG.error("Error launching job , Output path already exists : "
                        (1034: 7) LOG.error("Error Launching job : " + ioe.getMessage());
                        (1037: 7) LOG.error("Error monitoring job : " + ie.getMessage());
                    StreamXmlRecordReader.java  (1 usage)
                        (68: 5) LOG.info("StreamBaseRecordReader.init: " + " start_=" + start_ + " end_=" + end_ + " length_="
                org.apache.hadoop.streaming.mapreduce  (3 usages)
                    StreamBaseRecordReader.java  (1 usage)
                        (113: 7) LOG.info(status);
                    StreamInputFormat.java  (1 usage)
                        (56: 8) // LOG.info("getRecordReader start.....split=" + split);
                    StreamXmlRecordReader.java  (1 usage)
                        (73: 5) LOG.info("StreamBaseRecordReader.init: " + " start_=" + start_ + " end_="
            hadoop-yarn-applications-distributedshell  (61 usages)
                org.apache.hadoop.yarn.applications.distributedshell  (61 usages)
                    ApplicationMaster.java  (35 usages)
                        (215: 7) LOG.info("Initializing ApplicationMaster");
                        (222: 7) LOG.fatal("Error running ApplicationMaster", t);
                        (226: 7) LOG.info("Application Master completed successfully. exiting");
                        (229: 7) LOG.info("Application Master failed. exiting");
                        (239: 5) LOG.info("Dump debug output");
                        (242: 7) LOG.info("System env: key=" + env.getKey() + ", val=" + env.getValue());
                        (258: 9) LOG.info("System CWD content: " + line);
                        (339: 5) LOG.info("Application master for app" + ", appId="
                        (385: 9) LOG.error("Illegal values in env for shell script path" + ", path="
                        (418: 5) LOG.info("Starting ApplicationMaster");
                        (440: 7) LOG.info("Min mem capabililty of resources in this cluster " + minMem);
                        (441: 7) LOG.info("Max mem capabililty of resources in this cluster " + maxMem);
                        (449: 9) LOG.info("Container memory specified below min threshold of cluster."
                        (454: 9) LOG.info("Container memory specified above max threshold of cluster."
                        (483: 9) LOG.info("Current application state: loop=" + loopCounter
                        (496: 11) LOG.info("Sleep interrupted " + e.getMessage());
                        (512: 9) LOG.info("Asking RM for containers" + ", askCount=" + askCount);
                        (517: 9) LOG.info("Got response from RM for container ask, allocatedCnt="
                        (521: 11) LOG.info("Launching shell command on a new container."
                        (546: 9) LOG.info("Current available resources in the cluster "
                        (552: 9) LOG.info("Got response from RM for container ask, completedCnt="
                        (555: 11) LOG.info("Got container status for containerID="
                        (586: 13) LOG.info("Container completed successfully." + ", containerId="
                        (594: 9) LOG.info("Current application state: loop=" + loopCounter
                        (612: 11) LOG.info("Exception thrown in thread join: " + e.getMessage());
                        (619: 7) LOG.info("Application completed. Signalling finish to RM");
                        (663: 7) LOG.debug("Connecting to ContainerManager for containerid="
                        (668: 7) LOG.info("Connecting to ContainerManager at " + cmIpPortStr);
                        (683: 7) LOG.info("Setting up container launch container for containerid="
                        (694: 7) LOG.info("Setting user in ContainerLaunchContext to: " + jobUserName);
                        (714: 11) LOG.error("Error when trying to use shell script path specified"
                        (764: 9) LOG.info("Start container failed for :" + ", containerId="
                        (781: 10) // LOG.info("Container Status"
                        (811: 5) LOG.info("Requested container ask: " + request.toString());
                        (826: 5) LOG.info("Sending request to RM for containers" + ", progress="
                    Client.java  (26 usages)
                        (160: 7) LOG.info("Initializing Client");
                        (173: 7) LOG.fatal("Error running CLient", t);
                        (177: 7) LOG.info("Application completed successfully");
                        (180: 5) LOG.error("Application failed to complete successfully");
                        (316: 5) LOG.info("Running Client");
                        (320: 5) LOG.info("Got Cluster metric info from ASM"
                        (324: 5) LOG.info("Got Cluster node info from ASM");
                        (326: 7) LOG.info("Got node report from ASM for"
                        (335: 5) LOG.info("Queue info"
                        (345: 9) LOG.info("User ACL Info for Queue"
                        (362: 5) LOG.info("Min mem capabililty of resources in this cluster " + minMem);
                        (363: 5) LOG.info("Max mem capabililty of resources in this cluster " + maxMem);
                        (369: 7) LOG.info("AM memory specified below min threshold of cluster. Using min value."
                        (375: 7) LOG.info("AM memory specified above max threshold of cluster. Using max value."
                        (382: 5) LOG.info("Setting up application submission context for ASM");
                        (398: 5) LOG.info("Copy App Master jar from local filesystem and add to local environment");
                        (468: 5) LOG.info("Set the environment for the application master");
                        (507: 5) LOG.info("Setting up app master command");
                        (539: 5) LOG.info("Completed setting up app master command " + command.toString());
                        (572: 5) LOG.info("Submitting application to ASM");
                        (599: 9) LOG.debug("Thread sleep in monitoring loop interrupted");
                        (605: 7) LOG.info("Got application report from ASM for"
                        (622: 11) LOG.info("Application has completed successfully. Breaking monitoring loop");
                        (626: 11) LOG.info("Application did finished unsuccessfully."
                        (634: 9) LOG.info("Application did not finish."
                        (641: 9) LOG.info("Reached client specified timeout for application. Killing application");
            hadoop-yarn-applications-unmanaged-am-launcher  (17 usages)
                org.apache.hadoop.yarn.applications.unmanagedamlauncher  (17 usages)
                    UnmanagedAMLauncher.java  (17 usages)
                        (91: 7) LOG.info("Initializing Client");
                        (98: 7) LOG.fatal("Error running Client", t);
                        (207: 11) LOG.warn("Error reading the error stream", ioe);
                        (221: 11) LOG.warn("Error reading the out stream", ioe);
                        (233: 7) LOG.info("AM process exited with value: " + exitCode);
                        (247: 7) LOG.info("ShellExecutor: Interrupted while reading the error/out stream",
                        (250: 7) LOG.warn("Error while closing the error/out stream", ioe);
                        (256: 5) LOG.info("Starting Client");
                        (266: 7) LOG.info("Setting up application submission context for ASM");
                        (290: 7) LOG.info("Setting unmanaged AM");
                        (293: 7) LOG.info("Submitting application to ASM");
                        (300: 7) LOG.info("Launching application with id: " + attemptId);
                        (312: 7) LOG.info("App ended with state: " + appReport.getYarnApplicationState()
                        (318: 9) LOG.info("Application has completed successfully.");
                        (321: 9) LOG.info("Application did finished unsuccessfully." + " YarnState="
                        (350: 9) LOG.debug("Thread sleep in monitoring loop interrupted");
                        (356: 7) LOG.info("Got application report from ASM for" + ", appId="
            hadoop-yarn-client  (11 usages)
                org.apache.hadoop.yarn.client  (11 usages)
                    AMRMClientImpl.java  (8 usages)
                        (132: 9) LOG.debug("AppMasterToken is " + token);
                        (144: 5) LOG.debug("Connecting to ResourceManager at " + rmAddress);
                        (322: 9) LOG.debug("Added priority=" + priority);
                        (343: 7) LOG.debug("addResourceRequest:" + " applicationId="
                        (357: 9) LOG.debug("Not decrementing resource as priority " + priority
                        (366: 9) LOG.debug("Not decrementing resource as " + resourceName
                        (374: 7) LOG.debug("BEFORE decResourceRequest:" + " applicationId="
                        (403: 7) LOG.info("AFTER decResourceRequest:" + " applicationId="
                    YarnClientImpl.java  (3 usages)
                        (107: 7) LOG.debug("Connecting to ResourceManager at " + rmAddress);
                        (138: 5) LOG.info("Submitted application " + applicationId + " to ResourceManager"
                        (146: 5) LOG.info("Killing application " + applicationId);
            hadoop-yarn-common  (116 usages)
                org.apache.hadoop.yarn  (3 usages)
                    YarnUncaughtExceptionHandler.java  (3 usages)
                        (42: 7) LOG.error("Thread " + t + " threw an Throwable, but we are shutting " +
                        (46: 9) LOG.fatal("Thread " + t + " threw an Error.  Shutting down now...", e);
                        (63: 7) LOG.error("Thread " + t + " threw an Exception.", e);
                org.apache.hadoop.yarn.event  (9 usages)
                    AsyncDispatcher.java  (9 usages)
                        (72: 15) LOG.warn("AsyncDispatcher thread interrupted", ie);
                        (109: 9) LOG.warn("Interrupted Exception while stopping", ie);
                        (121: 7) LOG.debug("Dispatching the event " + event.getClass().getName() + "."
                        (137: 7) LOG.fatal("Error in dispatcher thread", t);
                        (140: 9) LOG.info("Exiting, bbye..");
                        (153: 5) LOG.info("Registering " + eventType + " for " + handler.getClass());
                        (180: 9) LOG.info("Size of event-queue is " + qSize);
                        (184: 9) LOG.warn("Very low remaining capacity in the event-queue: "
                        (191: 11) LOG.warn("AsyncDispatcher thread interrupted", e);
                org.apache.hadoop.yarn.factories.impl.pb  (2 usages)
                    RpcClientFactoryPBImpl.java  (1 usage)
                        (92: 7) LOG.error("Cannot call close method due to Exception. "
                    RpcServerFactoryPBImpl.java  (1 usage)
                        (174: 5) LOG.info("Adding protocol "+pbProtocol.getCanonicalName()+" to the server");
                org.apache.hadoop.yarn.ipc  (3 usages)
                    HadoopYarnProtoRPC.java  (2 usages)
                        (45: 5) LOG.debug("Creating a HadoopYarnProtoRpc proxy for protocol " + protocol);
                        (60: 5) LOG.debug("Creating a HadoopYarnProtoRpc server for protocol " + protocol +
                    YarnRPC.java  (1 usage)
                        (57: 5) LOG.debug("Creating YarnRPC for " +
                org.apache.hadoop.yarn.logaggregation  (8 usages)
                    AggregatedLogDeletionService.java  (6 usages)
                        (58: 7) LOG.info("aggregated log deletion started.");
                        (72: 7) LOG.info("aggregated log deletion finished.");
                        (83: 17) LOG.info("Deleting aggregated logs in "+appDir.getPath());
                        (119: 7) LOG.warn(comment + " " + message);
                        (121: 7) LOG.error(comment, e);
                        (139: 7) LOG.info("Log Aggregation deletion is disabled because retention is" +
                    AggregatedLogFormat.java  (2 usages)
                        (272: 9) LOG.warn("Exception closing writer", e);
                        (277: 9) LOG.warn("Exception closing output-stream", e);
                org.apache.hadoop.yarn.security  (5 usages)
                    AdminACLsManager.java  (1 usage)
                        (71: 7) LOG.warn("Could not add current user to admin:" + e);
                    ApplicationTokenSelector.java  (2 usages)
                        (42: 5) LOG.debug("Looking for a token with service " + service.toString());
                        (44: 7) LOG.debug("Token kind is " + token.getKind().toString()
                    ContainerTokenIdentifier.java  (1 usage)
                        (98: 5) LOG.debug("Writing ContainerTokenIdentifier to RPC layer: " + this);
                    ContainerTokenSelector.java  (1 usage)
                        (45: 9) LOG.info("Looking for service: " + service + ". Current token is "
                org.apache.hadoop.yarn.security.client  (4 usages)
                    ClientTokenSelector.java  (2 usages)
                        (42: 5) LOG.debug("Looking for a token with service " + service.toString());
                        (44: 7) LOG.debug("Token kind is " + token.getKind().toString()
                    RMTokenSelector.java  (2 usages)
                        (42: 5) LOG.debug("Looking for a token with service " + service.toString());
                        (44: 7) LOG.debug("Token kind is " + token.getKind().toString()
                org.apache.hadoop.yarn.server.security  (2 usages)
                    ApplicationACLsManager.java  (2 usages)
                        (94: 7) LOG.debug("Verifying access-type " + applicationAccessType + " for "
                        (108: 9) LOG.debug("ACL not found for access-type " + applicationAccessType
                org.apache.hadoop.yarn.service  (7 usages)
                    AbstractService.java  (3 usages)
                        (81: 5) LOG.info("Service:" + getName() + " is inited.");
                        (94: 5) LOG.info("Service:" + getName() + " is started.");
                        (113: 5) LOG.info("Service:" + getName() + " is stopped.");
                    CompositeService.java  (3 usages)
                        (72: 7) LOG.error("Error starting services " + getName(), e);
                        (101: 9) LOG.info("Error stopping " + service.getName(), t);
                        (124: 9) LOG.info("Error stopping " + compositeService.getName(), t);
                    ServiceOperations.java  (1 usage)
                        (133: 7) LOG.warn("When stopping the service " + service.getName()
                org.apache.hadoop.yarn.util  (38 usages)
                    AbstractLivelinessMonitor.java  (2 usages)
                        (111: 15) LOG.info("Expired:" + entry.getKey().toString() +
                        (119: 11) LOG.info(getName() + " thread interrupted");
                    ApplicationClassLoader.java  (6 usages)
                        (105: 11) LOG.debug("Remove leading / off " + name);
                        (117: 9) LOG.debug("getResource("+name+")=" + url);
                        (134: 7) LOG.debug("Loading class: " + name);
                        (147: 11) LOG.debug("Loaded class: " + name + " ");
                        (151: 11) LOG.debug(e);
                        (160: 9) LOG.debug("Loaded class from parent: " + name + " ");
                    FSDownload.java  (5 usages)
                        (207: 9) LOG.warn("Cannot unpack " + localrsrc);
                        (228: 9) LOG.warn("Treating [" + localrsrc + "] as an archive even though it " +
                        (234: 9) LOG.warn("Treating [" + localrsrc + "] as an archive even though it " +
                        (238: 9) LOG.warn("Cannot unpack " + localrsrc);
                        (333: 5) LOG.debug("Changing permissions for path " + path
                    LinuxResourceCalculatorPlugin.java  (9 usages)
                        (181: 7) LOG.warn("Error reading the stream " + io);
                        (189: 11) LOG.warn("Error closing the stream " + in);
                        (192: 9) LOG.warn("Error closing the stream " + fReader);
                        (233: 7) LOG.warn("Error reading the stream " + io);
                        (241: 11) LOG.warn("Error closing the stream " + in);
                        (244: 9) LOG.warn("Error closing the stream " + fReader);
                        (281: 7) LOG.warn("Error reading the stream " + io);
                        (289: 11) LOG.warn("Error closing the stream " + in);
                        (292: 9) LOG.warn("Error closing the stream " + fReader);
                    ProcfsBasedProcessTree.java  (14 usages)
                        (70: 7) LOG.error(StringUtils.stringifyException(e));
                        (84: 7) LOG.error(StringUtils.stringifyException(e));
                        (131: 9) LOG.info("ProcfsBasedProcessTree currently is supported only on "
                        (136: 7) LOG.warn("Failed to get Operating System name. " + se);
                        (213: 9) LOG.debug(this.toString());
                        (387: 7) LOG.info("The process " + pinfo.getPid()
                        (404: 9) LOG.warn("Unexpected: procfs stat file is not in the expected format"
                        (409: 7) LOG.warn("Error reading the stream " + io);
                        (418: 11) LOG.warn("Error closing the stream " + in);
                        (421: 9) LOG.warn("Error closing the stream " + fReader);
                        (535: 11) LOG.warn("Sum of stime (" + this.stime + ") and utime (" + this.utime
                        (589: 9) LOG.warn("Error reading the stream " + io);
                        (598: 13) LOG.warn("Error closing the stream " + in);
                        (601: 11) LOG.warn("Error closing the stream " + fReader);
                    RackResolver.java  (1 usage)
                        (100: 5) LOG.info("Resolved " + hostName + " to " + rName);
                    YarnVersionInfo.java  (1 usage)
                        (105: 5) LOG.debug("version: "+ getVersion());
                org.apache.hadoop.yarn.webapp  (27 usages)
                    Controller.java  (2 usages)
                        (218: 5) LOG.debug("{}: {}", MimeType.JSON, object);
                        (239: 5) LOG.debug("{}: {}", MimeType.TEXT, s);
                    Dispatcher.java  (5 usages)
                        (85: 7) LOG.info("dev mode restart requested");
                        (159: 7) LOG.error("error handling URI: "+ uri, e);
                        (186: 5) LOG.debug("removing cookie {} on {}", name, path);
                        (206: 5) LOG.debug("parts={}, params={}", parts, dest.pathParams);
                        (234: 9) LOG.info("WebAppp /{} exiting...", webApp.name());
                    GenericExceptionHandler.java  (2 usages)
                        (52: 7) LOG.trace("GOT EXCEPTION", e);
                        (89: 7) LOG.warn("INTERNAL_SERVER_ERROR", e);
                    Router.java  (8 usages)
                        (84: 5) LOG.debug("adding {}({})->{}#{}", new Object[]{path, names, cls, action});
                        (150: 11) LOG.debug("exact match for {}: {}", key, dest.action);
                        (153: 11) LOG.debug("prefix match2 for {}: {}", key, dest.action);
                        (166: 13) LOG.debug("prefix match for {}: {}", lower.getKey(), dest.action);
                        (187: 5) LOG.debug("checking prefix {}{} for path: {}", new Object[]{dest.prefix,
                        (266: 5) LOG.debug("trying: {}", className);
                        (270: 9) LOG.debug("found {}", className);
                        (273: 7) LOG.warn("found a {} but it's not a {}", className, cls.getName());
                    WebApp.java  (1 usage)
                        (105: 7) LOG.info("interrupted", e);
                    WebApps.java  (9 usages)
                        (176: 11) LOG.debug("setting webapp host class to {}", cls);
                        (183: 15) LOG.info("stopping existing webapp instance");
                        (186: 15) LOG.info("no existing webapp instance found: {}", e.toString());
                        (189: 15) LOG.warn("error stopping existing instance: {}", e.toString());
                        (192: 13) LOG.error("dev mode does NOT work with ephemeral port!");
                        (209: 9) LOG.info("Web app /"+ name +" started at "+ server.getPort());
                        (223: 7) LOG.info("Registered webapp guice modules");
                        (228: 9) LOG.info("in dev mode!");
                        (244: 7) LOG.warn("could not infer host class from", t);
                org.apache.hadoop.yarn.webapp.hamlet  (4 usages)
                    HamletGen.java  (4 usages)
                        (79: 5) LOG.info("Generating {} using {} and {}", new Object[]{outputName,
                        (112: 5) LOG.info("Generating {} methods", hamlet);
                        (116: 5) LOG.info("Wrote {} bytes to {}.java", bytes, outputName);
                        (155: 9) LOG.info("Generating class {}<T>", className);
                org.apache.hadoop.yarn.webapp.log  (3 usages)
                    AggregatedLogsBlock.java  (3 usages)
                        (105: 7) LOG.error("Error getting logs for " + logEntity, e);
                        (116: 7) LOG.error("Error getting logs for " + logEntity, e);
                        (161: 7) LOG.error("Error getting logs for " + logEntity, e);
                org.apache.hadoop.yarn.webapp.view  (1 usage)
                    HtmlBlock.java  (1 usage)
                        (63: 5) LOG.debug("Rendering {} @{}", getClass(), nestLevel);
            hadoop-yarn-server-common  (2 usages)
                org.apache.hadoop.yarn.server.security  (2 usages)
                    BaseContainerTokenSecretManager.java  (2 usages)
                        (130: 7) LOG.debug("Creating password for " + identifier.getContainerID()
                        (158: 7) LOG.debug("Retrieving password for " + identifier.getContainerID()
            hadoop-yarn-server-nodemanager  (223 usages)
                org.apache.hadoop.yarn.server.nodemanager  (80 usages)
                    ContainerExecutor.java  (5 usages)
                        (167: 9) LOG.info(str);
                        (249: 7) LOG.error("Got exception reading pid from pid-file " + pidFile, e);
                        (263: 7) LOG.warn("setsid is not available on this machine. So not using it.");
                        (266: 7) LOG.info("setsid exited with exit code " + shexec.getExitCode());
                        (297: 9) LOG.warn("Exception when killing task " + pid, e);
                    DefaultContainerExecutor.java  (18 usages)
                        (99: 5) LOG.info("Copying from " + nmPrivateContainerTokensPath + " to " + tokenDst);
                        (101: 5) LOG.info("CWD set to " + appStorageDir + " = " + lfs.getWorkingDirectory());
                        (158: 7) LOG.info("Container " + containerIdStr
                        (175: 7) LOG.info("launchContainer: " + Arrays.toString(command));
                        (184: 9) LOG.info("Container " + containerIdStr +
                        (193: 7) LOG.warn("Exit code from task is : " + exitCode);
                        (235: 5) LOG.debug("Sending signal " + signal.getValue() + " to pid " + sigpid
                        (273: 7) LOG.info("Deleting absolute path : " + subDir);
                        (276: 9) LOG.warn("delete returned false for path: [" + subDir + "]");
                        (282: 7) LOG.info("Deleting path : " + del);
                        (284: 9) LOG.warn("delete returned false for path: [" + del + "]");
                        (351: 9) LOG.warn("Unable to create the user directory : " + localDir, e);
                        (373: 5) LOG.info("Initializing user " + user);
                        (388: 9) LOG.warn("Unable to create app cache directory : " + appDir, e);
                        (396: 9) LOG.warn("Unable to create file cache directory : " + distDir, e);
                        (429: 9) LOG.warn("Unable to create app directory " + fullAppDir.toString(), e);
                        (453: 9) LOG.warn("Unable to create the app-log directory : " + appLogDir, e);
                        (479: 9) LOG.warn("Unable to create the container-log directory : "
                    DeletionService.java  (7 usages)
                        (133: 11) LOG.debug("NM deleting absolute path : " + subDir);
                        (137: 13) LOG.warn("Failed to delete " + subDir);
                        (143: 11) LOG.debug("NM deleting path : " + del);
                        (147: 13) LOG.warn("Failed to delete " + subDir);
                        (152: 11) LOG.debug("Deleting path: [" + subDir + "] as user: [" + user + "]");
                        (155: 11) LOG.warn("Failed to delete as user " + user, e);
                        (157: 11) LOG.warn("Failed to delete as user " + user, e);
                    DirectoryCollection.java  (2 usages)
                        (87: 9) LOG.warn("Unable to create directory " + dir + " error " +
                        (110: 9) LOG.warn("Directory " + dir + " error " +
                    LinuxContainerExecutor.java  (19 usages)
                        (127: 7) LOG.debug("checkLinuxExecutorSetup: " + Arrays.toString(commandArray));
                        (133: 7) LOG.warn("Exit code from container is : " + exitCode);
                        (178: 5) LOG.info("initApplication: " + Arrays.toString(commandArray));
                        (180: 7) LOG.debug("initApplication: " + Arrays.toString(commandArray));
                        (189: 7) LOG.warn("Exit code from container is : " + exitCode);
                        (229: 9) LOG.info("launchContainer: " + Arrays.toString(commandArray));
                        (235: 9) LOG.info("Container was marked as inactive. Returning terminated error");
                        (245: 7) LOG.warn("Exit code from container is : " + exitCode);
                        (251: 9) LOG.warn("Exception from container-launch : ", e);
                        (266: 7) LOG.debug("Output from LinuxContainerExecutor's launchContainer follows:");
                        (284: 7) LOG.debug("signalContainer: " + Arrays.toString(command));
                        (308: 7) LOG.info("Deleting absolute path : " + dir);
                        (312: 9) LOG.info("Deleting path : " + del);
                        (318: 5) LOG.info(" -- DEBUG -- deleteAsUser: " + Arrays.toString(commandArray));
                        (320: 7) LOG.debug("deleteAsUser: " + Arrays.toString(commandArray));
                        (329: 7) LOG.warn("Exit code from container is : " + exitCode);
                        (331: 9) LOG.error("DeleteAsUser for " + dir.toUri().getPath()
                        (333: 9) LOG.error("Output from LinuxContainerExecutor's deleteAsUser follows:");
                        (349: 9) LOG.debug("mountCgroups: " + Arrays.toString(commandArray));
                    LocalDirsHandlerService.java  (4 usages)
                        (255: 5) LOG.info("Disk(s) failed. " + getDisksHealthReport());
                        (265: 7) LOG.error("Most of the disks failed. " + getDisksHealthReport());
                        (313: 11) LOG.warn(paths[i] + " is not a valid path. Path should be with "
                        (320: 9) LOG.warn(e.getMessage());
                    NMAuditLogger.java  (4 usages)
                        (89: 7) LOG.info(createSuccessLog(user, operation, target, appId, containerId));
                        (106: 7) LOG.info(createSuccessLog(user, operation, target, null, null));
                        (150: 7) LOG.warn(createFailureLog(user, operation, target, description, appId, containerId));
                        (170: 7) LOG.warn(createFailureLog(user, operation, target, description, null, null));
                    NodeHealthScriptRunner.java  (2 usages)
                        (113: 9) LOG.warn("Caught exception : " + e.getMessage());
                        (221: 7) LOG.info("Not starting node health monitor");
                    NodeManager.java  (9 usages)
                        (129: 7) LOG.info("Security is enabled on NodeManager. "
                        (219: 5) LOG.info("Containers still running on shutdown: " + containers.keySet());
                        (226: 5) LOG.info("Waiting for containers to be killed");
                        (234: 9) LOG.warn("Interrupted while sleeping on container kill", ex);
                        (240: 7) LOG.info("All containers in DONE state");
                        (242: 7) LOG.info("Done waiting for containers to be killed. Still alive: " +
                        (304: 5) LOG.info("Rebooting the node manager.");
                        (324: 7) LOG.fatal("Error starting NodeManager", t);
                        (340: 7) LOG.warn("Invalid shutdown event " + event.getType() + ". Ignoring.");
                    NodeStatusUpdaterImpl.java  (10 usages)
                        (136: 5) LOG.info("Initialized nodemanager for " + nodeId + ":" +
                        (193: 5) LOG.info("Connecting to ResourceManager at " + this.rmAddress);
                        (210: 7) LOG.info("Security enabled - updating secret keys now");
                        (219: 5) LOG.info("Registered with ResourceManager as " + this.nodeId
                        (265: 7) LOG.info("Sending out status for container: " + containerStatus);
                        (271: 9) LOG.info("Removed completed container " + containerId);
                        (276: 5) LOG.debug(this.nodeId + " sending out status for "
                        (285: 7) LOG.debug("Node's health-status : " + nodeHealthStatus.getIsNodeHealthy()
                        (365: 15) LOG.info("Node is out of sync with ResourceManager,"
                        (391: 13) LOG.error("Caught exception in status-updater", e);
                org.apache.hadoop.yarn.server.nodemanager.containermanager  (20 usages)
                    AuxServices.java  (7 usages)
                        (59: 5) LOG.info("Adding auxiliary service " +
                        (99: 11) LOG.warn("The Auxilurary Service named '"+sName+"' in the "
                        (109: 9) LOG.fatal("Failed to initialize " + sName, e);
                        (153: 5) LOG.fatal("Service " + service.getName() + " changed state: " +
                        (160: 5) LOG.info("Got event " + event.getType() + " for appId "
                        (164: 7) LOG.info("Got APPLICATION_INIT for service " + event.getServiceID());
                        (167: 9) LOG.info("service is null");
                    ContainerManagerImpl.java  (13 usages)
                        (247: 5) LOG.info("ContainerManager started at " + connectAddress);
                        (277: 7) LOG.warn(msg);
                        (330: 9) LOG.debug("Number of TokenIdentifiers in the UGI from RPC: "
                        (382: 7) LOG.error(msg);
                        (402: 5) LOG.info("Start request for " + containerIDStr + " by user "
                        (417: 13) LOG.debug(tk.getService() + " = " + tk.toString());
                        (445: 7) LOG.info("Creating a new application reference for app "
                        (497: 7) LOG.warn("Trying to stop unknown container " + containerID);
                        (534: 5) LOG.info("Getting container-status for " + containerIDStr);
                        (538: 7) LOG.info("Returning " + containerStatus);
                        (558: 9) LOG.warn("Event " + event + " sent to absent container " +
                        (574: 9) LOG.warn("Event " + event + " sent to absent application "
                        (611: 7) LOG.warn("Invalid event " + event.getType() + ". Ignoring.");
                org.apache.hadoop.yarn.server.nodemanager.containermanager.application  (7 usages)
                    ApplicationImpl.java  (7 usages)
                        (255: 7) LOG.info("Adding " + container.getContainerID()
                        (295: 9) LOG.warn("Removing unknown " + containerEvent.getContainerID() +
                        (298: 9) LOG.info("Removing " + containerEvent.getContainerID() +
                        (352: 7) LOG.info("Removing " + containerFinishEvent.getContainerID()
                        (401: 7) LOG.debug("Processing " + applicationID + " of type " + event.getType());
                        (409: 9) LOG.warn("Can't handle this event at current state", e);
                        (412: 9) LOG.info("Application " + applicationID + " transitioned from "
                org.apache.hadoop.yarn.server.nodemanager.containermanager.container  (7 usages)
                    ContainerImpl.java  (7 usages)
                        (521: 15) LOG.info("Got exception parsing " + rsrc.getKey()
                        (528: 11) LOG.warn("Failed to parse resource-request", e);
                        (573: 9) LOG.warn("Localized unknown resource " + rsrcEvent.getResource() +
                        (742: 9) LOG.warn("Localized unknown resource " + rsrcEvent.getResource() +
                        (823: 7) LOG.debug("Processing " + containerID + " of type " + event.getType());
                        (831: 9) LOG.warn("Can't handle this event at current state: Current: ["
                        (835: 9) LOG.info("Container " + containerID + " transitioned from "
                org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher  (16 usages)
                    ContainerLaunch.java  (15 usages)
                        (236: 9) LOG.info("Container " + containerIdStr + " not launched as "
                        (247: 7) LOG.warn("Failed to launch container.", e);
                        (259: 7) LOG.debug("Container " + containerIdStr + " completed with exit code "
                        (274: 7) LOG.warn("Container exited with a non-zero exit code " + ret);
                        (282: 5) LOG.info("Container " + containerIdStr + " succeeded ");
                        (300: 5) LOG.info("Cleaning up container " + containerIdStr);
                        (305: 7) LOG.info("Container " + containerIdStr + " not launched."
                        (310: 5) LOG.debug("Marking container " + containerIdStr + " as inactive");
                        (316: 7) LOG.debug("Getting pid for container " + containerIdStr + " to kill"
                        (334: 9) LOG.debug("Sending signal to pid " + processId
                        (340: 11) LOG.debug("Sent signal to pid " + processId
                        (349: 7) LOG.warn("Got error when trying to cleanup container " + containerIdStr
                        (371: 5) LOG.debug("Accessing pid for container " + containerIdStr
                        (382: 9) LOG.debug("Got pid " + processId + " for container "
                        (387: 9) LOG.info("Could not get pid for " + containerIdStr
                    ContainersLauncher.java  (1 usage)
                        (146: 11) LOG.warn("Got exception while cleaning container " + containerId
                org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer  (33 usages)
                    ContainerLocalizer.java  (3 usages)
                        (228: 7) LOG.warn("Failed to close filesystems: ", e);
                        (247: 17) LOG.warn("Unknown visibility: " + r.getVisibility()
                        (356: 9) LOG.warn("Localization running as " + uid + " not " + user);
                    LocalizedResource.java  (5 usages)
                        (130: 7) LOG.info("Attempt to release claim on " + this +
                        (184: 7) LOG.debug("Processing " + resourcePath + " of type " + event.getType());
                        (191: 9) LOG.warn("Can't handle this event at current state", e);
                        (194: 9) LOG.info("Resource " + resourcePath + " transitioned from "
                        (230: 7) LOG.warn("Resource " + rsrc + " localized without listening container");
                    LocalResourcesTrackerImpl.java  (5 usages)
                        (73: 9) LOG.info("Resource " + rsrc.getLocalPath()
                        (85: 9) LOG.info("Release unknown rsrc null (discard)");
                        (121: 7) LOG.error("Attempt to remove absent resource: " + rem.getRequest()
                        (128: 7) LOG.error("Attempt to remove resource: " + rsrc
                        (151: 7) LOG.warn("Random directory component did not match. " +
                    ResourceLocalizationService.java  (20 usages)
                        (230: 5) LOG.info("Localizer started on port " + server.getPort());
                        (308: 7) LOG.warn("Initializing application " + app + " already present");
                        (348: 5) LOG.debug("Resource cleanup (public) " + retain);
                        (351: 7) LOG.debug("Resource cleanup " + t.getUser() + ":" + retain);
                        (409: 7) LOG.warn("Removing uninitialized application " + application);
                        (484: 11) LOG.info("Unknown localizer with localizerId " + locId
                        (521: 15) LOG.info("Created localizer for " + locId);
                        (609: 7) LOG.info("Downloading public rsrc:" + key);
                        (623: 13) LOG.error("Local path for public localization is not found. "
                        (644: 17) LOG.error("Localized unkonwn resource to " + completed);
                        (656: 15) LOG.info("Failed to download rsrc " + assoc.getResource(),
                        (666: 19) LOG.error("Missing pending list for " + req);
                        (689: 9) LOG.fatal("Error: Shutting down", t);
                        (691: 9) LOG.info("Public cache exiting");
                        (794: 11) LOG.error("Unknown resource reported: " + req);
                        (821: 13) LOG.info("DEBUG: FAILED " + req, stat.getException());
                        (831: 13) LOG.info("Unknown status: " + stat.getStatus());
                        (873: 9) LOG.info("Localizer failed", e);
                        (895: 9) LOG.info("Writing credentials to the nmPrivate file "
                        (900: 13) LOG.debug(tk.getService() + " : " + tk.encodeToUrlString());
                org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.security  (4 usages)
                    LocalizerSecurityInfo.java  (1 usage)
                        (58: 9) LOG.debug("Using localizerTokenSecurityInfo");
                    LocalizerTokenSelector.java  (3 usages)
                        (41: 5) LOG.debug("Using localizerTokenSelector.");
                        (44: 7) LOG.debug("Token of kind " + token.getKind() + " is found");
                        (49: 5) LOG.debug("Returning null.");
                org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation  (23 usages)
                    AppLogAggregatorImpl.java  (10 usages)
                        (105: 7) LOG.info("Starting aggregate log-file for app " + this.applicationId
                        (115: 9) LOG.error("Cannot create writer for app " + this.applicationId
                        (122: 5) LOG.info("Uploading logs for container " + containerId
                        (130: 7) LOG.error("Couldn't upload logs for " + containerId
                        (141: 9) LOG.warn("Aggregation did not complete for application " + appId);
                        (156: 11) LOG.warn("PendingContainers queue is interrupted");
                        (180: 7) LOG.info("Finished aggregate log-file for app " + this.applicationId);
                        (193: 7) LOG.error("Failed to move temporary log file to final location: ["
                        (244: 7) LOG.info("Considering container " + containerId
                        (252: 5) LOG.info("Application just finished : " + this.applicationId);
                    LogAggregationService.java  (13 usages)
                        (138: 5) LOG.info(this.getName() + " waiting for pending aggregation during exit");
                        (151: 9) LOG.info("Waiting for aggregation to complete for " + appId);
                        (158: 9) LOG.warn("Aggregation stop interrupted!");
                        (163: 7) LOG.warn("Some logs may not have been aggregated for " + appId);
                        (187: 11) LOG.warn("Remote Root Log Dir [" + this.remoteRootLogDir
                        (198: 7) LOG.warn("Remote Root Log Dir [" + this.remoteRootLogDir
                        (240: 13) LOG.error("Failed to get remote FileSystem while processing app "
                        (253: 13) LOG.error("Failed to create user dir [" + userDir
                        (267: 13) LOG.error("Failed to create suffixed user dir [" + suffixDir
                        (281: 13) LOG.error("Failed to  create application log dir [" + appDir
                        (364: 7) LOG.warn("Failed to close filesystems: ", e);
                        (382: 7) LOG.warn("Log aggregation is not initialized for " + containerId
                        (396: 7) LOG.warn("Log aggregation is not initialized for " + appId
                org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler  (1 usage)
                    NonAggregatingLogHandler.java  (1 usage)
                        (118: 9) LOG.info("Scheduling Log Deletion for application: "
                org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor  (20 usages)
                    ContainersMonitorImpl.java  (20 usages)
                        (99: 5) LOG.info(" Using ResourceCalculatorPlugin : "
                        (104: 5) LOG.info(" Using ResourceCalculatorProcessTree : "
                        (112: 9) LOG.warn("NodeManager's totalPmem could not be calculated. "
                        (127: 7) LOG.warn("NodeManager configured with " +
                        (168: 13) LOG.info("ResourceCalculatorPlugin is unavailable on this system. "
                        (173: 9) LOG.info("ResourceCalculatorProcessTree is unavailable on this system. "
                        (178: 7) LOG.info("Neither virutal-memory nor physical-memory monitoring is " +
                        (293: 7) LOG.warn("Process tree for container: " + containerId
                        (298: 7) LOG.warn("Process tree for container: " + containerId
                        (336: 11) LOG.debug("Current ProcessTree list : "
                        (346: 13) LOG.info("Starting resource-monitoring for " + containerId);
                        (356: 13) LOG.info("Stopping resource-monitoring for " + containerId);
                        (381: 17) LOG.debug("Tracking ProcessTree " + pId
                        (396: 13) LOG.debug("Constructing ProcessTree for : PID = " + pId
                        (408: 13) LOG.info(String.format(
                        (444: 15) LOG.warn(msg);
                        (447: 17) LOG.error("Killed container process with PID " + pId
                        (454: 15) LOG.info("Removed ProcessTree with root " + pId);
                        (464: 13) LOG.warn("Uncaught exception in ContainerMemoryManager "
                        (472: 11) LOG.warn(ContainersMonitorImpl.class.getName()
                org.apache.hadoop.yarn.server.nodemanager.security  (2 usages)
                    NMContainerTokenSecretManager.java  (2 usages)
                        (68: 5) LOG.info("Rolling master-key for container-tokens, got key with id "
                        (178: 7) LOG.warn("Not adding key for null containerId");
                org.apache.hadoop.yarn.server.nodemanager.util  (8 usages)
                    CgroupsLCEResourcesHandler.java  (6 usages)
                        (124: 7) LOG.debug("createCgroup: " + path);
                        (139: 7) LOG.debug("updateCgroup: " + path + ": " + param + "=" + value);
                        (153: 11) LOG.warn("Unable to close cgroup file: " +
                        (163: 5) LOG.debug("deleteCgroup: " + path);
                        (166: 7) LOG.warn("Unable to delete cgroup at: " + path);
                        (282: 9) LOG.warn("Error closing the stream: " + MTAB_FILE, e2);
                    ProcessIdFileReader.java  (2 usages)
                        (48: 5) LOG.debug("Accessing pid from pid file " + path);
                        (85: 5) LOG.debug("Got pid "
                org.apache.hadoop.yarn.server.nodemanager.webapp  (2 usages)
                    WebServer.java  (2 usages)
                        (64: 5) LOG.info("Instantiating NMWebApp at " + bindAddress);
                        (74: 7) LOG.error(msg, e);
            hadoop-yarn-server-resourcemanager  (313 usages)
                org.apache.hadoop.yarn.server.resourcemanager  (73 usages)
                    AdminService.java  (5 usages)
                        (141: 7) LOG.warn("Couldn't get current user", ioe);
                        (150: 7) LOG.warn("User " + user.getShortUserName() + " doesn't have permission" +
                        (163: 5) LOG.info("RM Admin: " + method + " invoked by user " +
                        (179: 7) LOG.info("Exception refreshing queues ", ioe);
                        (197: 7) LOG.info("Exception refreshing nodes ", ioe);
                    ApplicationMasterService.java  (9 usages)
                        (147: 7) LOG.warn(msg);
                        (155: 7) LOG.warn(msg);
                        (173: 7) LOG.error(message);
                        (184: 7) LOG.info("AM registration " + applicationAttemptId);
                        (221: 7) LOG.error(message);
                        (255: 7) LOG.error("AppAttemptId doesnt exist in cache " + appAttemptId);
                        (264: 7) LOG.error("Invalid responseid from appAttemptId " + appAttemptId);
                        (329: 9) LOG.error(message);
                        (343: 5) LOG.info("Registering " + attemptId);
                    ClientRMService.java  (9 usages)
                        (200: 5) LOG.info("Allocated new applicationId: " + applicationId.getId());
                        (232: 7) LOG.info("Error getting UGI ", ie);
                        (282: 7) LOG.info("Storing Application with id " + applicationId);
                        (288: 9) LOG.error("Failed to store application:" + applicationId, e);
                        (292: 7) LOG.info("Application with id " + applicationId.getId() +
                        (297: 7) LOG.info("Exception in submitting application", ie);
                        (320: 7) LOG.info("Error getting UGI ", ie);
                        (379: 7) LOG.info("Error getting UGI ", ie);
                        (434: 7) LOG.info("Failed to getQueueInfo for " + request.getQueueName(), ioe);
                    NodesListManager.java  (8 usages)
                        (75: 7) LOG.warn("Failed to init hostsReader, disabling", ioe);
                        (94: 5) LOG.debug("hostsReader: in=" + conf.get(YarnConfiguration.RM_NODES_INCLUDE_FILE_PATH,
                        (99: 7) LOG.debug("include: " + include);
                        (102: 7) LOG.debug("exclude: " + exclude);
                        (148: 7) LOG.debug(eventNode + " reported unusable");
                        (161: 9) LOG.debug(eventNode + " reported usable");
                        (172: 9) LOG.warn(eventNode
                        (177: 7) LOG.error("Ignoring invalid eventtype " + event.getType());
                    ResourceManager.java  (15 usages)
                        (184: 7) LOG.error("Failed to init state store", e);
                        (291: 5) LOG.info("Using Scheduler: " + schedulerClassName);
                        (370: 13) LOG.error("Returning, interrupted : " + e);
                        (381: 15) LOG.warn("Exception during shutdown: ", t);
                        (384: 13) LOG.fatal("Error in handling event type " + event.getType()
                        (388: 15) LOG.info("Exiting, bbye..");
                        (413: 11) LOG.info("Size of scheduler event-queue is " + qSize);
                        (417: 11) LOG.info("Very low remaining capacity on scheduler event queue: "
                        (445: 11) LOG.error("Error in handling event type " + event.getType()
                        (473: 13) LOG.error("Error in handling event type " + event.getType()
                        (499: 11) LOG.error("Error in handling event type " + event.getType()
                        (544: 9) LOG.error("Failed to load/recover state", e);
                        (575: 9)         LOG.info("Interrupted while waiting", ie);
                        (606: 7) LOG.error("Error closing store.", e);
                        (713: 7) LOG.fatal("Error starting ResourceManager", t);
                    ResourceTrackerService.java  (7 usages)
                        (165: 7) LOG.info("Disallowed NodeManager from  " + host
                        (186: 7) LOG.info("Reconnect from the node at: " + host);
                        (194: 5) LOG.info("NodeManager from node " + host + "(cmPort: " + cmPort
                        (223: 7) LOG.info("Node not found rebooting " + remoteNodeStatus.getNodeId());
                        (232: 7) LOG.info("Disallowed NodeManager nodeId: " + nodeId + " hostname: "
                        (246: 7) LOG.info("Received duplicate heartbeat from node "
                        (252: 7) LOG.info("Too far behind rm response id:"
                    RMAppManager.java  (10 usages)
                        (156: 9) LOG.info(createAppSummary(app));
                        (171: 7) LOG.error("RMAppManager received completed appId of null, skipping");
                        (223: 7) LOG.info("Application should be expired, max # apps"
                        (259: 9) LOG.info(message);
                        (279: 9) LOG.info("RMAppManager submit application exception", ie);
                        (309: 5) LOG.info("Recovering " + appStates.size() + " applications");
                        (320: 9) LOG.info("Not recovering unmanaged application " + appState.getAppId());
                        (323: 9) LOG.info("Recovering application " + appState.getAppId());
                        (337: 5) LOG.debug("RMAppManager processing event for "
                        (357: 9) LOG.error("Invalid eventtype " + event.getType() + ". Ignoring!");
                    RMAuditLogger.java  (8 usages)
                        (98: 7) LOG.info(createSuccessLog(user, operation, target, appId, null,
                        (119: 7) LOG.info(createSuccessLog(user, operation, target, appId, attemptId,
                        (140: 7) LOG.info(createSuccessLog(user, operation, target, appId, null, null));
                        (157: 7) LOG.info(createSuccessLog(user, operation, target, null, null, null));
                        (207: 7) LOG.warn(createFailureLog(user, operation, perm, target, description,
                        (231: 7) LOG.warn(createFailureLog(user, operation, perm, target, description,
                        (255: 7) LOG.warn(createFailureLog(user, operation, perm, target, description,
                        (277: 7) LOG.warn(createFailureLog(user, operation, perm, target, description,
                    RMNMInfo.java  (2 usages)
                        (61: 9) LOG.warn("Error registering RMNMInfo MBean", e);
                        (63: 5) LOG.info("Registered RMNMInfo MBean");
                org.apache.hadoop.yarn.server.resourcemanager.amlauncher  (11 usages)
                    AMLauncher.java  (9 usages)
                        (104: 5) LOG.info("Setting up container " + application.getMasterContainer()
                        (112: 5) LOG.info("Done launching container " + application.getMasterContainer()
                        (161: 5) LOG.info("Command to launch container "
                        (226: 7) LOG.debug("Putting appMaster token in env : " + token);
                        (254: 9) LOG.info("Launching master" + application.getAppAttemptId());
                        (261: 9) LOG.info(message);
                        (268: 9) LOG.info("Cleaning master " + application.getAppAttemptId());
                        (271: 9) LOG.info("Error cleaning master ", ie);
                        (275: 7) LOG.warn("Received unknown event-type " + eventType + ". Ignoring.");
                    ApplicationMasterLauncher.java  (2 usages)
                        (77: 7) LOG.info(launcherHandlingThread.getName() + " interrupted during join ",
                        (97: 11) LOG.warn(this.getClass().getName() + " interrupted. Returning.");
                org.apache.hadoop.yarn.server.resourcemanager.recovery  (17 usages)
                    FileSystemRMStateStore.java  (11 usages)
                        (97: 11) LOG.info("Loading application from node: " + childNodeName);
                        (111: 11) LOG.info("Loading application attempt from node: " + childNodeName);
                        (123: 11) LOG.info("Unknown child node with name: " + childNodeName);
                        (137: 11) LOG.info("Application node not found for attempt: "
                        (145: 7) LOG.error("Failed to load state.", e);
                        (156: 5) LOG.info("Storing info for app: " + appId + " at: " + nodeCreatePath);
                        (163: 7) LOG.info("Error storing info for app: " + appId, e);
                        (173: 5) LOG.info("Storing info for attempt: " + attemptId
                        (181: 7) LOG.info("Error storing info for attempt: " + attemptId, e);
                        (191: 5) LOG.info("Removing info for app: " + appId + " at: " + nodeRemovePath);
                        (201: 5) LOG.info("Removing info for attempt: " + attemptId
                    RMStateStore.java  (6 usages)
                        (182: 5) LOG.info("Storing info for app: " + context.getApplicationId());
                        (268: 11) LOG.info("Storing info for attempt: " + attemptState.getAttemptId());
                        (273: 13) LOG.error("Error storing appAttempt: "
                        (288: 11) LOG.info("Removing info for app: " + appId);
                        (292: 13) LOG.error("Error removing app: " + appId, e);
                        (297: 9) LOG.error("Unknown RMStateStoreEvent type: " + event.getType());
                org.apache.hadoop.yarn.server.resourcemanager.resource  (1 usage)
                    ResourceCalculator.java  (1 usage)
                        (40: 7) LOG.info("divideAndCeil called with a=" + a + " b=" + b);
                org.apache.hadoop.yarn.server.resourcemanager.rmapp  (6 usages)
                    RMAppImpl.java  (6 usages)
                        (503: 7) LOG.debug("Processing event for " + appID + " of type "
                        (510: 9) LOG.error("Can't handle this event at current state", e);
                        (515: 9) LOG.info(appID + " State change from " + oldState + " to "
                        (526: 5) LOG.info("Recovering app: " + getApplicationId() + " with " +
                        (557: 5) LOG.debug("Received node update event:" + type + " for node:" + node
                        (683: 9) LOG.info(msg);
                org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt  (8 usages)
                    RMAppAttemptImpl.java  (8 usages)
                        (487: 7) LOG.warn("Could not proxify "+trackingUriWithoutScheme,e);
                        (583: 7) LOG.debug("Processing event for " + appAttemptID + " of type "
                        (590: 9) LOG.error("Can't handle this event at current state", e);
                        (595: 9) LOG.info(appAttemptID + " State change from " + oldState + " to "
                        (650: 5) LOG.info("Recovered attempt: AppId: " + getAppAttemptId().getApplicationId()
                        (824: 11) LOG.error("Cannot get this state!! Error!!");
                        (1184: 7) LOG.error("Failed to store attempt: " + getAppAttemptId(),
                        (1193: 5) LOG.info("Storing attempt: AppId: " +
                org.apache.hadoop.yarn.server.resourcemanager.rmcontainer  (4 usages)
                    RMContainerImpl.java  (4 usages)
                        (208: 5) LOG.debug("Processing " + event.getContainerId() + " of type " + event.getType());
                        (215: 9) LOG.error("Can't handle this event at current state", e);
                        (216: 9) LOG.error("Invalid event " + event.getType() +
                        (220: 9) LOG.info(event.getContainerId() + " Container Transitioned from "
                org.apache.hadoop.yarn.server.resourcemanager.rmnode  (7 usages)
                    RMNodeImpl.java  (7 usages)
                        (312: 5) LOG.debug("Processing " + event.getNodeId() + " of type " + event.getType());
                        (319: 9) LOG.error("Can't handle this event at current state", e);
                        (320: 9) LOG.error("Invalid event " + event.getType() +
                        (324: 9) LOG.info(nodeId + " Node Transitioned from " + oldState + " to "
                        (469: 7) LOG.info("Deactivating Node " + rmNode.nodeId + " as it is now "
                        (516: 11) LOG.info("Container " + containerId + " already scheduled for " +
                        (522: 11) LOG.info("Container " + containerId
                org.apache.hadoop.yarn.server.resourcemanager.scheduler  (7 usages)
                    ActiveUsersManager.java  (2 usages)
                        (68: 7) LOG.debug("User " + user + " added to activeUsers, currently: " +
                        (94: 9) LOG.debug("User " + user + " removed from activeUsers, currently: " +
                    AppSchedulingInfo.java  (5 usages)
                        (107: 5) LOG.info("Application " + applicationId + " requests cleared");
                        (134: 11) LOG.debug("update:" + " application=" + applicationId + " request="
                        (165: 11) LOG.info("checking for deactivate... ");
                        (233: 5) LOG.debug("allocate: user: " + user + ", memory: "
                        (345: 5) LOG.debug("allocate: applicationId=" + applicationId + " container="
                org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity  (74 usages)
                    CapacityScheduler.java  (25 usages)
                        (223: 7) LOG.info("Initialized CapacityScheduler with " +
                        (232: 9) LOG.info("Re-initializing queues...");
                        (258: 5) LOG.info("Initialized root queue " + root);
                        (360: 5) LOG.info("Initialized queue: " + queue);
                        (398: 7) LOG.info("Failed to submit application " + applicationAttemptId +
                        (408: 5) LOG.info("Application Submission: " + applicationAttemptId +
                        (421: 5) LOG.info("Application " + applicationAttemptId + " is done." +
                        (429: 7) LOG.info("Unknown application " + applicationAttemptId + " has completed!");
                        (458: 7) LOG.error("Cannot finish application " + "from non-leaf queue: "
                        (478: 7) LOG.info("Calling allocate on removed " +
                        (509: 11) LOG.debug("allocate: pre-update" +
                        (518: 9) LOG.debug("allocate: post-update");
                        (523: 9) LOG.debug("allocate:" +
                        (569: 7) LOG.debug("nodeUpdate: " + nm + " clusterResources: " + clusterResource);
                        (582: 7) LOG.debug("Container FINISHED: " + containerId);
                        (589: 7) LOG.debug("Node being looked for scheduling " + nm
                        (603: 7) LOG.info("Trying to fulfill reservation for application " +
                        (627: 7) LOG.info("Skipping scheduling since node " + nm +
                        (640: 7) LOG.info("Unknown application: " + applicationAttemptId +
                        (701: 7) LOG.error("Invalid eventtype " + event.getType() + ". Ignoring!");
                        (710: 5) LOG.info("Added node " + nodeManager.getNodeAddress() +
                        (744: 5) LOG.info("Removed node " + nodeInfo.getNodeAddress() +
                        (752: 7) LOG.info("Null container completed...");
                        (762: 7) LOG.info("Container " + container + " of" +
                        (776: 5) LOG.info("Application " + applicationAttemptId +
                    CapacitySchedulerConfiguration.java  (7 usages)
                        (194: 5) LOG.debug("CSConf - getCapacity: queuePrefix=" + getQueuePrefix(queue) +
                        (205: 5) LOG.debug("CSConf - setCapacity: queuePrefix=" + getQueuePrefix(queue) +
                        (223: 5) LOG.debug("CSConf - setMaxCapacity: queuePrefix=" + getQueuePrefix(queue) +
                        (235: 5) LOG.debug("here setUserLimit: queuePrefix=" + getQueuePrefix(queue) +
                        (290: 5) LOG.debug("CSConf - getQueues called for: queuePrefix=" + getQueuePrefix(queue));
                        (292: 5) LOG.debug("CSConf - getQueues: queuePrefix=" + getQueuePrefix(queue) +
                        (299: 5) LOG.debug("CSConf - setQueues: qPrefix=" + getQueuePrefix(queue) +
                    LeafQueue.java  (24 usages)
                        (213: 7) LOG.debug("LeafQueue:" + " name=" + queueName
                        (271: 5) LOG.info("Initializing " + queueName + "\n" +
                        (632: 9) LOG.info(msg);
                        (642: 9) LOG.info(msg);
                        (654: 9) LOG.info(msg);
                        (669: 7) LOG.info("Failed to submit application to parent-queue: " +
                        (692: 9) LOG.info("Application " + application.getApplicationId() +
                        (708: 5) LOG.info("Application added -" +
                        (750: 5) LOG.info("Application removed -" +
                        (774: 7) LOG.debug("assignContainers: node=" + node.getHostName()
                        (792: 9) LOG.debug("pre-assignContainers for application "
                        (863: 9) LOG.debug("post-assignContainers for application "
                        (901: 7) LOG.info(getQueueName() +
                        (944: 7) LOG.debug("Headroom calculation for user " + user + ": " +
                        (1011: 7) LOG.debug("User limit computation for " + userName +
                        (1039: 9) LOG.debug("User " + userName + " in queue " + getQueueName() +
                        (1070: 9) LOG.debug("needsContainers:" +
                        (1246: 7) LOG.debug("assignContainers: node=" + node.getHostName()
                        (1264: 7) LOG.warn("Couldn't get container for allocation!");
                        (1303: 7) LOG.info("assignedContainer" +
                        (1318: 7) LOG.info("Reserved container " +
                        (1385: 9) LOG.info("completedContainer" +
                        (1417: 7) LOG.info(getQueueName() +
                        (1441: 5) LOG.info(getQueueName() +
                    ParentQueue.java  (18 usages)
                        (151: 5) LOG.info("Initialized parent-queue " + queueName +
                        (189: 5) LOG.info(queueName +
                        (218: 7) LOG.debug("setChildQueues: " + getChildQueuesToPrint());
                        (405: 9) LOG.info(getQueueName() + ": re-configured queue: " + childQueue);
                        (415: 9) LOG.info(getQueueName() + ": added new child queue: " + newChildQueue);
                        (472: 9) LOG.info("Failed to submit application to parent-queue: " +
                        (485: 5) LOG.info("Application added -" +
                        (510: 5) LOG.info("Application removed -" +
                        (549: 9) LOG.debug("Trying to assign containers to child-queue of "
                        (573: 9) LOG.info("assignedContainer" +
                        (585: 9) LOG.debug("ParentQ=" + getQueueName()
                        (596: 13) LOG.debug("Not assigning more than one off-switch container," +
                        (615: 7) LOG.info(getQueueName() +
                        (642: 9) LOG.debug("Trying to assign to queue: " + childQueue.getQueuePath()
                        (647: 9) LOG.debug("Assigned to queue: " + childQueue.getQueuePath() +
                        (658: 9) LOG.info("Re-sorting queues since queue: " + childQueue.getQueuePath() +
                        (680: 7) LOG.debug("printChildQueues - queue: " + getQueuePath()
                        (696: 9) LOG.info("completedContainer" +
                org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica  (13 usages)
                    FiCaSchedulerApp.java  (6 usages)
                        (219: 5) LOG.info("Completed container: " + rmContainer.getContainerId() +
                        (264: 7) LOG.debug("allocate: applicationAttemptId="
                        (297: 11) LOG.debug("showRequests:" + " application=" + getApplicationId() +
                        (301: 13) LOG.debug("showRequests:" + " application=" + getApplicationId()
                        (388: 5) LOG.info("Application " + getApplicationId()
                        (411: 5) LOG.info("Application " + getApplicationId() + " unreserved " + " on node "
                    FiCaSchedulerNode.java  (7 usages)
                        (108: 5) LOG.info("Assigned container " + container.getId() +
                        (142: 7) LOG.error("Invalid container released " + container);
                        (150: 5) LOG.info("Released container " + container.getId() +
                        (160: 7) LOG.error("Invalid resource addition of null resource for "
                        (170: 7) LOG.error("Invalid deduction of null resource for "
                        (219: 7) LOG.info("Updated reserved container " +
                        (223: 7) LOG.info("Reserved container " + reservedContainer.getContainer().getId() +
                org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair  (51 usages)
                    AppSchedulable.java  (2 usages)
                        (186: 5) LOG.info("Making reservation: node=" + node.getHostName() +
                        (285: 5) LOG.info("Node offered to app: " + getName() + " reserved: " + reserved);
                    FairScheduler.java  (22 usages)
                        (196: 11) LOG.error("Exception in fair scheduler UpdateThread", e);
                        (336: 9) LOG.info("Preempting container (prio=" + container.getContainer().getPriority() +
                        (386: 7) LOG.info(message);
                        (508: 7) LOG.info(msg);
                        (519: 5) LOG.info("Application Submission: " + applicationAttemptId +
                        (531: 5) LOG.info("Application " + applicationAttemptId + " is done." +
                        (537: 7) LOG.info("Unknown application " + applicationAttemptId + " has completed!");
                        (577: 7) LOG.info("Null container completed...");
                        (587: 7) LOG.info("Container " + container + " of" +
                        (604: 5) LOG.info("Application " + applicationAttemptId +
                        (614: 5) LOG.info("Added node " + node.getNodeAddress() +
                        (643: 5) LOG.info("Removed node " + rmNode.getNodeAddress() +
                        (681: 7) LOG.info("Calling allocate on removed " +
                        (709: 11) LOG.debug("allocate: pre-update" +
                        (718: 9) LOG.debug("allocate: post-update");
                        (723: 9) LOG.debug("allocate:" +
                        (741: 7) LOG.info("Unknown application: " + applicationAttemptId +
                        (757: 7) LOG.debug("nodeUpdate: " + nm + " cluster capacity: " + clusterCapacity);
                        (770: 7) LOG.debug("Container FINISHED: " + containerId);
                        (787: 7) LOG.info("Trying to fulfill reservation for application " +
                        (834: 7) LOG.error("Request for appInfo of unknown attempt" + appAttemptId);
                        (909: 7) LOG.error("Unknown event arrived at FairScheduler: " + event.toString());
                    FairSchedulerEventLog.java  (4 usages)
                        (94: 7) LOG.info("Initialized fair scheduler event log, logging to " + logFile);
                        (96: 7) LOG.error(
                        (123: 7) LOG.error("Failed to append to fair scheduler event log", e);
                        (136: 7) LOG.error("Failed to close fair scheduler event log", e);
                    FSLeafQueue.java  (3 usages)
                        (128: 9) LOG.debug("Counting resource from " + sched.getName() + " " + toAdd
                        (139: 7) LOG.debug("The updated demand for " + getName() + " is " + demand
                        (146: 5) LOG.debug("Node offered to queue: " + getName() + " reserved: " + reserved);
                    FSParentQueue.java  (2 usages)
                        (85: 9) LOG.debug("Counting resource from " + childQueue.getName() + " " +
                        (96: 7) LOG.debug("The updated demand for " + getName() + " is " + demand +
                    FSSchedulerApp.java  (7 usages)
                        (220: 5) LOG.info("Completed container: " + rmContainer.getContainerId() +
                        (257: 11) LOG.debug("showRequests:" + " application=" + getApplicationId() +
                        (261: 13) LOG.debug("showRequests:" + " application=" + getApplicationId()
                        (344: 5) LOG.info("Application " + getApplicationId()
                        (367: 5) LOG.info("Application " + getApplicationId() + " unreserved " + " on node "
                        (552: 7) LOG.debug("allocate: applicationAttemptId="
                        (572: 5) LOG.info("Raising locality level from " + old + " to " + level + " at " +
                    FSSchedulerNode.java  (7 usages)
                        (106: 5) LOG.info("Assigned container " + container.getId() +
                        (141: 7) LOG.error("Invalid container released " + container);
                        (149: 5) LOG.info("Released container " + container.getId() +
                        (159: 7) LOG.error("Invalid resource addition of null resource for "
                        (169: 7) LOG.error("Invalid deduction of null resource for "
                        (218: 7) LOG.info("Updated reserved container " +
                        (222: 7) LOG.info("Reserved container " + reservedContainer.getContainer().getId() +
                    QueueManager.java  (4 usages)
                        (113: 9) LOG.error("The fair scheduler allocation file fair-scheduler.xml was "
                        (278: 11) LOG.error("Failed to reload fair scheduler config file - " +
                        (378: 9) LOG.warn("Bad element in allocations file: " + element.getTagName());
                        (474: 7) LOG.warn(String.format("Queue %s has max resources %d less than min resources %d",
                org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo  (19 usages)
                    FifoScheduler.java  (19 usages)
                        (227: 7) LOG.error("Calling allocate on removed " +
                        (255: 9) LOG.debug("allocate: pre-update" +
                        (263: 9) LOG.debug("allocate: post-update" +
                        (268: 9) LOG.debug("allocate:" +
                        (303: 5) LOG.info("Application Submission: " + appAttemptId.getApplicationId() +
                        (348: 5) LOG.debug("assignContainers:" +
                        (356: 7) LOG.debug("pre-assignContainers");
                        (375: 7) LOG.debug("post-assignContainers");
                        (440: 5) LOG.debug("assignContainersOnNode:" +
                        (518: 5) LOG.debug("assignContainers:" +
                        (592: 7) LOG.debug("Container FINISHED: " + containerId);
                        (599: 7) LOG.debug("Node heartbeat " + rmNode.getNodeID() +
                        (604: 7) LOG.debug("Node after allocation " + rmNode.getNodeID() + " resource = "
                        (650: 9) LOG.error("Unable to remove application "
                        (668: 7) LOG.error("Invalid eventtype " + event.getType() + ". Ignoring!");
                        (677: 7) LOG.info("Unknown application: " + applicationAttemptId +
                        (694: 7) LOG.info("Null container completed...");
                        (707: 7) LOG.info("Unknown application: " + applicationAttemptId +
                        (723: 5) LOG.info("Application " + applicationAttemptId +
                org.apache.hadoop.yarn.server.resourcemanager.security  (21 usages)
                    ApplicationTokenSecretManager.java  (4 usages)
                        (81: 7) LOG.debug("Application finished, removing password for " + appAttemptId);
                        (105: 5) LOG.info("Rolling master-key for application-tokens");
                        (119: 7) LOG.debug("Creating password for " + applicationAttemptId);
                        (136: 7) LOG.debug("Trying to retrieve password for " + applicationAttemptId);
                    DelegationTokenRenewer.java  (13 usages)
                        (131: 9) LOG.info("Interrupted while joining on delayed removal thread.", e);
                        (209: 9) LOG.warn("Unable to add token " + token + " for cancellation. " +
                        (226: 13) LOG.debug("Canceling token " + tokenWithConf.token.getService());
                        (239: 11) LOG.warn("Failed to cancel token " + tokenWithConf.token + " " +
                        (244: 11) LOG.warn("Got exception " + StringUtils.stringifyException(t) +
                        (272: 7) LOG.debug("Registering tokens for renewal for:" +
                        (296: 9) LOG.debug("Registering token for renewal for:" +
                        (325: 11) LOG.debug("Renewing delegation-token for:" + token.getService() +
                        (331: 9) LOG.error("Exception renewing token" + token + ". Not rescheduled", e);
                        (383: 7) LOG.info("Did not cancel "+t);
                        (394: 7) LOG.debug("removing failed delegation token for appid=" + applicationId +
                        (439: 13) LOG.debug("Removing delegation token for appId=" + applicationId +
                        (493: 13) LOG.info("Delayed Deletion Thread Interrupted. Shutting it down");
                    RMContainerTokenSecretManager.java  (4 usages)
                        (67: 5) LOG.info("ContainerTokenKeyRollingInterval: " + this.rollingInterval
                        (95: 7) LOG.info("Rolling master-key for container-tokens");
                        (100: 9) LOG.info("Going to activate master-key with key-id "
                        (131: 7) LOG.info("Activating next master key with id: "
                org.apache.hadoop.yarn.server.resourcemanager.webapp  (1 usage)
                    RMWebServices.java  (1 usage)
                        (193: 9) LOG.info("heatlh state is : " + healthState);
            hadoop-yarn-server-web-proxy  (17 usages)
                org.apache.hadoop.yarn.server.webproxy  (14 usages)
                    AppReportFetcher.java  (2 usages)
                        (57: 5) LOG.info("Connecting to ResourceManager at " + rmAddress);
                        (61: 5) LOG.info("Connected to ResourceManager at " + rmAddress);
                    WebAppProxy.java  (4 usages)
                        (61: 7) LOG.warn("Unrecongized attribute value for " +
                        (75: 5) LOG.info("Instantiating Proxy at " + bindAddress);
                        (99: 7) LOG.fatal("Could not start proxy web server",e);
                        (111: 9) LOG.fatal("Error stopping proxy web server", e);
                    WebAppProxyServer.java  (1 usage)
                        (97: 7) LOG.fatal("Error starting Proxy server", t);
                    WebAppProxyServlet.java  (7 usages)
                        (158: 7) LOG.debug("local InetAddress for proxy host: " + localAddress.toString());
                        (169: 9) LOG.debug("REQ HEADER: "+name+" : "+value);
                        (240: 9) LOG.warn(remoteUser+" Gave an invalid proxy path "+pathInfo);
                        (249: 9) LOG.warn(req.getRemoteUser()+" Attempting to access "+appId+
                        (273: 9) LOG.warn(req.getRemoteUser()+" Attempting to access "+id+
                        (318: 9) LOG.info("Asking "+remoteUser+" if they want to connect to the " +
                        (330: 7) LOG.info(req.getRemoteUser()+" is accessing unchecked "+toFetch+
                org.apache.hadoop.yarn.server.webproxy.amfilter  (3 usages)
                    AmIpFilter.java  (3 usages)
                        (68: 15) LOG.debug("proxy address is: " + add.getHostAddress());
                        (96: 7) LOG.debug("Remote address for request is: " + httpReq.getRemoteAddr());
                        (116: 7) LOG.warn("Could not find "+WebAppProxyServlet.PROXY_USER_COOKIE_NAME
            hadoop-yarn-site  (7 usages)
                /home/michalxo/IdeaProjects/hadoop-common/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/apt  (7 usages)
                    WritingYarnApplications.apt.vm  (7 usages)
                        (102: 5)     LOG.info("Connecting to ResourceManager at " + rmAddress);
                        (119: 5)     LOG.info("Got new ApplicationId=" + response.getApplicationId());
                        (363: 5)     LOG.info("Connecting to ResourceManager at " + rmAddress);
                        (536: 7)       LOG.info("Launching shell command on a new container."
                        (566: 7)       LOG.info("Got container status for containerID= "
                        (633: 7)       LOG.info(
                        (695: 5)     LOG.info("Container Status"

END OF PRODUCTION logs
